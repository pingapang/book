\chapter{Categorical predictor variables}\label{chap:categorical}



\section{Dummy coding}
As we have seen in Chapter 1, there are largely two different types of variables: numeric variables and categorical variables. Numeric variables say something about \textit{how much} of an attribute is in an object: for instance height (measured by inches) or heat (measured in degrees Kelvin). Categorical variables say something about the quality of an attribute: for instance colour (red, green, yellow) or seating (aisle seat, window seat). We have also seen a third type of variable: ordinal variables. They are somewhat in the middle between numeric variables and categorical variables: they are about quantitative differences between objects (e.g., size) but the values are sharp disjoint categories (small, medium, large).

In the chapters on simple and multiple regression we have seen that both the dependent and the independent variables were all numeric. The linear model used in regression analysis always involves a numeric dependent variable. However, in such analyses it is possible to use categorical independent variables. In this chapter we explain how to do that and how to interpret the results. 

The basic trick that we need is \textit{dummy coding}. Dummy coding involves making one or more new variables, that reflect the different categories of a categorical variable. First we focus on categorical variables with only two categories (dichotomous variables). Later in this chapter, we will explain what to do with categorical variables with more than two categories (nominal variables). 

Imagine we study bus companies and there are two different seatings in buses: aisle seats and window seats. Suppose we ask 5 people, who have travelled from Amsterdam to Paris by bus during the last 12 months, whether they had an aisle seat or a window seat, and how much they payed for the trip. Suppose we have the variables, person, seat and price. Table \ref{tab:dummy_1} shows the anonymized data.

<<dummy_1, fig.height=4, echo=FALSE, fig.align='center', message=F, results="asis", warning=F>>=
set.seed(123)
person <- c("001","002", "003", "004", "005")
seat <- c('aisle', 'aisle', 'window', 'window', 'aisle')
price <- rnorm(5, 60, 5) %>% round(0)
data.bus <- data.frame(person, seat, price )
data.bus %>%
        head() %>%
        xtable(caption="Bus trips to Paris.", label="tab:dummy_1") %>%
        print(include.rownames=F, caption.placement = "top")
@


With dummy coding, we make a new variable that only has values 0 and 1, and that conveys the same information as the \textbf{seat} variable. The resulting variable is called a \textit{dummy variable}. Let's call this dummy variable \textbf{window} and give it the value 1 for all persons that travelled in a window seat. We give the value 0 for all persons that travelled in an aisle seat. We can also call the new variable \textbf{window} a \textit{boolean variable} with TRUE and FALSE, since in computer science, TRUE is coded by a 1 and FALSE by a 0. Another name that is sometimes used is an \textit{indicator variable}. Whatever you want to call it, the data matrix including the new variable is displayed in Table \ref{tab:dummy_2}.

<<dummy_2, fig.height=4, echo=FALSE, fig.align='center', message=F, results="asis", warning=F>>=
window <- c(0,0,1,1,0)
data.bus <- data.frame(person, seat, window, price )
data.bus %>%
        head() %>%
        xtable(caption="Bus trips to Paris.", label="tab:dummy_2") %>%
        print(include.rownames=F, caption.placement = "top")
out.bus <- lm(price ~ window)
# source('/Users/stephanievandenberg/Dropbox/Statistiek_Onderwijs/Data Analysis/writeForeignCode.R')
# write.foreign(data.train,
#               '/Users/stephanievandenberg/Dropbox/Statistiek_Onderwijs/Data Analysis/spss examples logistic/train.sav',
#               '/Users/stephanievandenberg/Dropbox/Statistiek_Onderwijs/Data Analysis/spss examples logistic/train.sps',
#               package = c("SPSS"))
@

What we have done now is coding the old categorical variable \textbf{seat} into a variable \textbf{window} with values 0 and 1 that looks numeric. Let's see what happens if we use a linear model for the variables price (dependent variable) and window (independent variable). The linear model is:

\begin{eqnarray}
price &=& b_0 + b_1 window + e \\
e &\sim& N(0,\sigma^2_e)
\end{eqnarray}

Let's use the bus trip data and determine the least squares regression line. We find the following linear equation:


\begin{equation}
\widehat{price} = \Sexpr{out.bus$coef[1]} + \Sexpr{out.bus$coef[2]} \times window
\end{equation}

If the variable \textbf{window} has the value 1, then the expected or predicted price of the bus ticket is, according to this equation, $\Sexpr{out.bus$coef[1]} + \Sexpr{out.bus$coef[2]}\times  1= \Sexpr{out.bus$coef[1]+out.bus$coef[2]}$. What does this mean? Well, all prices paid by persons with a window seat were coded as a 1 on the \textbf{window} variable. Therefore the expected price of a window seat equals \Sexpr{out.bus$coef[1]+out.bus$coef[2]}. By the same token, the expected price of an aisle seat (\textbf{window} = 0) is $\Sexpr{out.bus$coef[1]} + \Sexpr{out.bus$coef[2]}\times 0= \Sexpr{out.bus$coef[1]}$.

You see that by coding a categorical variable into a numeric dummy variable, we can describe the 'linear' relationship between the type of seat and the price of the ticket. Figure \ref{fig:dummy_3} shows the relationship between the numeric variable \textbf{window} and the numeric variable \textbf{price}. 

<<dummy_3,fig.height=4, echo=FALSE, fig.align='center', fig.cap='Relation between dummy variable window and price.'>>=
data.bus %>% ggplot(aes(x=window, y=price))+
        geom_point() +
        geom_smooth(se=F, method='lm')
@


Note that the blue regression line goes straight through the mean of the prices for window seats (\textbf{window}=1) and the mean of the prices for aisle seats (\textbf{window}=0). In other words, the dummy variable actually models the \textit{group means} of window and aisle seats.

Figure \ref{fig:dummy_4} shows the same regression line but now for the original variable \textbf{seat}. Although the analysis was based on the dummy variable \textbf{window}, it is more readable for others to show the original categorical variable \textbf{seat}.

<<dummy_4,fig.height=4, echo=FALSE, fig.align='center', fig.cap='Relation between type of seat and price.'>>=
data.bus %>% ggplot(aes(x=seat, y=price))+
        geom_point() +
        geom_abline(intercept=64-10, slope=5, col=4)+
        scale_y_continuous(breaks=seq(50,90))
@



\section{Using regression to describe group means}

In the previous section we saw that if we replace a categorical variable with a numeric dummy variable with values 0 and 1, we can use a linear model to describe the relationship between a categorical independent variable and a numeric dependent variable. We also saw that if we take the least squares regression line, this line goes straight through the averages, the group means. The line goes straight through the group means because then the sum of the squared residuals is then at its smallest value (the least squares principle).  Let's look at the bus trip data again and compute the residuals and the squared residuals, see Table \ref{tab:dummy_5}.

<<dummy_5, fig.height=4, echo=FALSE, fig.align='center', message=F, results="asis", warning=F>>=
data.bus <- data.frame(person, seat, window, price, e=price-predict(out.bus), e_squared=( price-predict(out.bus))^2)
data.bus %>%
        head() %>%
        xtable(caption="Bus trips to Paris.", label="tab:dummy_5") %>%
        print(include.rownames=F, caption.placement = "top")
@



<<dummy_6,fig.height=4, echo=FALSE, fig.align='center', fig.cap='Relation between type of seat and price, with the regression line being not quite the least squares .'>>=
data.bus %>% ggplot(aes(x=seat, y=price))+
        geom_point() +
        geom_abline(intercept=64.1-9.6, slope=4.8, col=4) +
        scale_y_continuous(breaks=seq(50,90))
@


<<dummy_7, fig.height=4, echo=FALSE, fig.align='center', message=F, results="asis", warning=F>>=
wrongpredict<- 59.1 + 4.8*window
data.bus <- data.frame(person, seat, window, price, wrongpredict, e=price-wrongpredict, e_squared=( price-wrongpredict)^2)
data.bus %>%
        head() %>%
        xtable(caption="Bus trips to Paris.", label="tab:dummy_7") %>%
        print(include.rownames=F, caption.placement = "top")
@

If we take the sum of the squared residuals we obtain \Sexpr{sum(( price-predict(out.bus))^2)}. Now if we use a slightly different slope, so that we no longer go straight through the average prices for aisle and window seats (see Figure \ref{fig:dummy_6}) and we compute the predicted values, the residuals and the squared residuals (see Table \ref{tab:dummy_7}), we obtain a higher sum: \Sexpr{sum(( price-wrongpredict)^2)}. 

Only the least squares regression line goes through the average seat prices of aisle and window seats. Thus, we can use the least squares regression equation to describe group means for categorical variables. 

Conversely, when you know the group means, it is very easy to draw the regression line: the intercept is then the mean for the category coded as 0, and the slope is equal to the mean of the category coded as 1 minus the mean of the category coded as 0 (i.e. the intercept). Check Figure \ref{fig:dummy_3} to verify this yourself. But we can also show this for a new data set.


We look at results from an experiment to compare yields (as measured by dried weight of plants) obtained under a control and two different treatment conditions. Let's plot the data first, where we only compare the two experimental conditions (see Figure \ref{fig:dummy_8}).

<<dummy_8,fig.height=4, echo=FALSE, fig.align='center', fig.cap='Data on yield under two experimental conditions: treatment 1 and treatment 2.'>>=
data("PlantGrowth")
data <- PlantGrowth %>% filter(group!="ctrl")
PlantGrowth %>% filter(group!="ctrl") %>% 
        ggplot(aes(x=group, y=weight))+
        geom_point() 
@

With treatment 1, the average yield turns out to be \Sexpr{mean(data$weight[data$group=="trt1"])}, and with treatment 2, the average yield is \Sexpr{mean(data$weight[data$group=="trt2"])}. Suppose we make a new dummy variable \textbf{treatment2} that is 0 for treatment 1, and 1 for treatment 2. Then we have the linear equation:

\begin{equation}
\widehat{weight} = b_0 + b_1 \times treatment2
\end{equation}

If we fill in the dummy variable and the expected weights (the means!), then we have the linear equations:


\begin{eqnarray}
\Sexpr{mean(data$weight[data$group=="trt1"])} &=& b_0 + b_1 \times 0 = b_0 \\
\Sexpr{mean(data$weight[data$group=="trt2"])} &=& b_0 + b_1 \times 1 = b_0 + b_1
\end{eqnarray}

So from this, we know that intercept $b_0 = \Sexpr{mean(data$weight[data$group=="trt1"])}$, and if we fill that in for the second equation above, we get the slope: 

\begin{equation}
b_1 = \Sexpr{mean(data$weight[data$group=="trt2"])}-b_0= \Sexpr{mean(data$weight[data$group=="trt2"])} -\Sexpr{mean(data$weight[data$group=="trt1"])}= \Sexpr{mean(data$weight[data$group=="trt2"])-mean(data$weight[data$group=="trt1"])}.
\end{equation}

Thus, we get the linear equation 

\begin{equation}
\label{weight}
\widehat{weight} = \Sexpr{mean(data$weight[data$group=="trt1"])} + \Sexpr{mean(data$weight[data$group=="trt2"])-mean(data$weight[data$group=="trt1"])}\times treatment
\end{equation}

Since this regression line goes straight through the average yield for each treatment, we know that this is the least square regression equation. We could have obtained the exact same result with a regression analysis using SPSS. But this was not necessary: because we knew the group means, we could find the intercept and the slope ourselves by doing the math.  

The interesting thing about a dummy variable is that the slope of the regression line is exactly equal to the differences between the two averages. If we look at Equation \ref{weight}, we see that the slope coefficient is \Sexpr{mean(data$weight[data$group=="trt2"])-mean(data$weight[data$group=="trt1"])} and this is exactly equal to the difference in mean weight for treatment 1 and treatment 2. Thus, the slope coefficient for a dummy variable indicates how much the average of the treatment that is coded as 1 differs from the treatment that is coded as 0. Here the slope is positive so that we know that the treatment coded as 1 (trt2), leads to a higher average yield than the treatment coded as 0 (trt1). This makes it possible to test null-hypotheses about differences in group means.

\section{Testing hypotheses about differences in group means}

In the previous section we saw that the slope in a dummy regression is equal to the difference in group means. Suppose researchers are interested in the effects of different treatments on yield. They'd like to know what the difference is in yield between treatments 1 and 2, using a sample of 30 data points. Based on this sample, they'd like to generalize to the population of all yields based on treatments 1 and 2. They adopt a type I error rate of $\alpha=0.05$.


<<dummy_9, fig.height=4, echo=FALSE, fig.align='center', message=F, results="asis", warning=F>>=
PlantGrowth %>% 
        filter(group!="ctrl") %>% 
        lm(weight ~ group, .) %>% 
        xtable(caption="Yield by treatment.", label="tab:dummy_9") %>%
        print(include.rownames=T, caption.placement = "top")
source('/Users/stephanievandenberg/Dropbox/Statistiek_Onderwijs/Data Analysis/writeForeignCode.R')
write.foreign(PlantGrowth %>% 
                filter(group!="ctrl"),
              '/Users/stephanievandenberg/Dropbox/Statistiek_Onderwijs/Data Analysis/spss examples dummy/yield1.sav',
              '/Users/stephanievandenberg/Dropbox/Statistiek_Onderwijs/Data Analysis/spss examples dummy/yield1.sps',
              package = c("SPSS"))
@

The researchers analyze the data and they find the results as displayed in Table \ref{tab:dummy_9}. The 95\% confidence interval for the slope is from \Sexpr{round(confint(lm(weight ~ group, data))[2,1],2)} to \Sexpr{round(confint(lm(weight ~ group, data))[2,2],2)}. This means that reasonable values for the \textit{population} difference between the two treatments on yield lie within this interval. All these values are positive, so we reasonably believe that treatment 2 leads to a higher yield. We know that it is treatment 2 that leads to a higher yield, because the slope in the regression equation has been coded as 'grouptr2'. Thus, a dummy variable has been computed, \textbf{grouptrt2}, where trt2 has been coded as 1 (and trt1 consequently coded as 0). In the next section, we will see how to do that in SPSS.

The 95\% confidence interval for the slope does not contain 0, so we can therefore reject the null-hypothesis that there is no difference in group means at an $\alpha$ of 5\%. The exact $p$-value can be read from Table \ref{tab:dummy_9} and is equal to \Sexpr{round(summary(lm(weight ~ group, data))$coef[2,4],4)}.

Thus, based on this regression analysis the researchers can write in a report that there is a significant difference between the yield after treatment 1 and the yield after treatment 2, $p=\Sexpr{round(summary(lm(weight ~ group, data))$coef[2,4],2)}$. Treatment 2 leads to a yield of about \Sexpr{round(summary(lm(weight ~ group, data))$coef[2,1],2)} (SE=\Sexpr{round(summary(lm(weight ~ group, data))$coef[2,2],2)}) more than treatment 1 (95\% CI: \Sexpr{round(confint(lm(weight ~ group, data))[2,1],2)}, \Sexpr{round(confint(lm(weight ~ group, data))[2,2],2)}).

\section{Regression analysis using a dummy variable in SPSS}

In SPSS there are two ways in which you can use a linear model with a categorical independent variable. The first and easiest way is to tell SPSS that your variable, for example your variable \textbf{group}, is to be treated plainly as a categorical variable, and you do that by the keyword BY. For instance, for the data set on treatment 1 and 2 and yield, you get the following syntax:

\begin{verbatim}
UNIANOVA weight BY group 
/DESIGN group
/PRINT parameter.
\end{verbatim}

All variables after the BY keyword are automatically turned into dummy variables. Actually, SPSS creates \textit{two} dummy variables, one that codes 1 for all yields that come from treatment 1 and 0 from all other yields (dummy variable \textbf{[group=trt1}]), and another dummy variable that codes 1 for all yields that come from treatment 2 and 0 for all other yields (dummy variable \textbf{[group=trt2}]). We see that in Figure \ref{fig:dummy_10}. Now, as we already saw from the bus company example, in order to model two group means, one dummy variable is already sufficient. We don't need two dummy variables for that. Therefore, one of the two is redundant and SPSS automatically chooses the second dummy variable to be redundant. That leaves us with one dummy variable, the first one called \textbf{[group=trt1]}. 

The output then looks like as displayed in Figure \ref{fig:dummy_10}. Here, we see an intercept of 5.526 , a slope of $-0.865$ for dummy variable \textbf{[group=trt1]} and a slope of 0 for dummy variable \textbf{[group=trt2]}. We have to read the SPSS output table like this: If group = trt1, then the expected weight equals the intercept (5.526) + 1 times the slope for being in the first treatment group, plus 0 times the slope for being in the second treatment group:

\begin{equation}
\widehat{weight}= 5.536 + 1 \times -0.865 + 0 \times 0  = 5.536 - 0.865 = \Sexpr{5.536 - 0.865}
\end{equation}

If group = trt2, then the expected weight equals the intercept (5.526) + 0 times the slope for being in the first treatment group, plus 1 times the slope for being in the second treatment group:

\begin{equation}
\widehat{weight}= 5.536 + 0 \times -0.865 + 1 \times 0  = 5.536 
\end{equation}

Because the slope for the second dummy variable is automatically fixed to 0, the trt2 group is automatically chosen as the \textit{reference category}. The reference category is the category that is used for comparison. In the linear model, the intercept equals 5.536 and that is the expected yield for the reference category (treatment group 2). The slope parameter for the \textbf{[group=trt2]} dummy variable equals -0.865 and this is the difference in yield for the treatment 1 group \textit{compared to the reference group}. Therefore, the expected yield is the treatment 1 group is 0.865 \textit{less} than in the treatment 2 group (the slope parameter is negative). 

\begin{figure}[h]
    \begin{center}
       \includegraphics[scale=0.7, trim={0cm 25cm 0cm 0cm}]{/Users/stephanievandenberg/Dropbox/Statistiek_Onderwijs/Data" "Analysis/spss" "examples" "dummy/yield1.pdf}
    \end{center}
 \caption{SPSS output of a regression analysis of weight on treatment.}
 \label{fig:dummy_10}
\end{figure}

Sometimes, the automatic choice of the reference category by software is something you don't want. Suppose that you'd like to compare two treatments: the old treatment 1, and a new treatment 2 that avoids all the insecticides that are so bad for bees and bumblebees. Here, the most interesting question is how the new treatment differs from the old one. So you'd like to use the \textit{old} treatment as the reference category (coded as 0) to which you want to compare the yield of the new treatment.

In order to choose your own way of dummy coding, and thereby choosing your own reference category, you can use the syntax below to create a new variable, the dummy variable \textbf{treatment2}. All yields associated with trt2 are coded as a 1, and all others are coded as a 0.

\begin{verbatim}
RECODE group ('trt2'=1) (ELSE=0) INTO treatment2.
EXECUTE.
\end{verbatim}

Then you use a slightly altered syntax. First, you now use the new variable \textbf{treatment2}. Second, you use the keyword WITH instead of BY, to indicate that you want to treat the variable as any \textit{numeric} variable.

\begin{verbatim}
UNIANOVA weight WITH treatment2 
/DESIGN treatment2
/PRINT parameter.
\end{verbatim}

With the keyword BY, SPSS treats the variable as a categorical variable and subsequently chooses its own reference category. With the keyword WITH, you treat the new variable treatment2 as a numerical variable and hence SPSS sees no need to make dummy variables. The result is the output in Figure \ref{fig:dummy_11}.


\begin{figure}[h]
    \begin{center}
       \includegraphics[scale=0.7, trim={0cm 26cm 0cm 0cm}]{/Users/stephanievandenberg/Dropbox/Statistiek_Onderwijs/Data" "Analysis/spss" "examples" "dummy/yield2.pdf}
    \end{center}
 \caption{SPSS output of a regression analysis of weight on treatment.}
 \label{fig:dummy_11}
\end{figure}

Because you now use your own dummy variable and use it as any numerical variable, the output looks a lot simpler (compare it with Figure \ref{fig:dummy_10}). You see an intercept of 4.661 and a slope of 0.865 for the \textbf{treatment2} variable. Thus, if this \textbf{treatment2} variable has value 0, the expected weight equals the intercept 4.661, and if this \textbf{treatment2} variable has value 1, the expected weight equals the sum of the intercept and the slope, $4.661 + 0.865 = \Sexpr{4.661 + 0.865}$. Notice that by using a different reference category (now trt1), \textit{the sign of the slope has changed}. The intercept has also changed, as the intercept is now the expected weight for the other treatment group. 

In general, we advise to use the BY keyword to indicate you'd like to have an automatically coded dummy variable. If however the output is very hard to interpret, think of the best way to code your own dummy variable. For experimental designs, it makes sense to code control conditions as 0, and experimental conditions as 1. For surveys, if you want to compare how a social minority scores relative to a social majority, it makes sense to code the minority group as 1 and the social majority as 0. In eduational studies, it makes sense to code an old teaching method as 0 and a new method as 1.  

\subsection{Exercise}

\begin{enumerate}

\item Look at the output in Figure \ref{fig:dummy_12}. It describes the comparison between average height in males and females. The variable \textbf{sex} was used. In the syntax, the BY keyword was used. Based on this output: what is the average height in females? And what is the average height in males?

\begin{figure}[h]
    \begin{center}
       \includegraphics[scale=0.7, trim={0cm 25cm 0cm 0cm}]{/Users/stephanievandenberg/Dropbox/Statistiek_Onderwijs/Data" "Analysis/spss" "examples" "dummy/height1.pdf}
    \end{center}
 \caption{SPSS output of a regression analysis of height on sex.}
 \label{fig:dummy_12}
\end{figure}

\item Look at the output in Figure \ref{fig:dummy_13}. It describes the comparison between average height in Ethiopeans and Japanese people. A variable ethnicity was used that equals 1 for Ethiopeans and 0 for Japanese people. The analysis was run using the BY keyword, treating the \textbf{ethnicity} variable as a categorical variable. Based on this output: what is the average height in the Japanese in this data set? And what is the average height in the Ethiopeans?

\begin{figure}[h]
    \begin{center}
       \includegraphics[scale=0.7, trim={0cm 25cm 0cm 0cm}]{/Users/stephanievandenberg/Dropbox/Statistiek_Onderwijs/Data" "Analysis/spss" "examples" "dummy/height2.pdf}
    \end{center}
 \caption{SPSS output of a regression analysis of height on ethnicity.}
 \label{fig:dummy_13}
\end{figure}

\item A study looks into the effects of drinking milk during childhood on adult height. A number of adults are categorized into those that that have been drinking less than 1 liter of milk per month during childhood (coded as milk=0) and into those that have been drinking at least 1 liter of milk per month during childhood (coded as milk=1). A regression analysis treating the \textbf{milk} variable as numeric (using the WITH keyword) yields the output in Figure \ref{fig:dummy_14}.

\begin{figure}[h]
    \begin{center}
       \includegraphics[scale=0.7, trim={0cm 26cm 0cm 0cm}]{/Users/stephanievandenberg/Dropbox/Statistiek_Onderwijs/Data" "Analysis/spss" "examples" "dummy/height3.pdf}
    \end{center}
 \caption{SPSS output of a regression analysis of height on milk.}
 \label{fig:dummy_14}
\end{figure}

What is the average height in adults who drank less than 1 liter of milk per month during childhood? And what is the average height in people who drank at 1 liter of milk or more per month?


\item A study looks into the effect of vitamin B2 (riboflavin) on the frequency of migraine attacks. It compares 100 patients who take a pill containing 50 mg of riboflavin per day for a month and 100 patients who take a pill containing 0 mg of riboflavin per day for a month. Suppose you want to code a dummy variable called \textbf{riboflavin} in order to perform a regression analysis. Which patients would you code as 1, and which patients as 0? Motivate your answer. 

\end{enumerate}

Answers:

\begin{enumerate}
\item The intercept equals 182.4. For sex=female, there is an extra effect of -12.067. For sex=male, the extra effect is fixed to 0. Therefore, the average height in females in this data set is 182.4-12.067=\Sexpr{182.4-12.067} and the average height in males equals 182.4. 

\item 
The intercept is 167.167. If ethnicity=0, then there is an extra height of 15.233. Since the 
Japanese are coded as 0, the average height in the Japanese people in the data set equals $167.167+15.233= \Sexpr{167.167+15.233}$. The average height in the Ethiopeans in this data set equals 167.167. 

\item The intercept equals 169.278 and the slope of milk is 10.482. That means that people who score 0 on milk, have an average height of $169.278 + 10.482 \times 0 = 169.278$. The ones that score 0 on the milk variable drank less than 1 liter of milk per month. The ones that score 1 on the milk variable drank at least 1 liter of milk per month, and their average is $169.278 + 10.482 \times 1 = \Sexpr{169.278+10.482}$.

\item If you're interested in the effect of riboflavin, you'd like to compare the people who took 50 mg to those who took 0 mg. How much more or less frequent are the migraine attacks in people who took riboflavin relative to those that did not take extra riboflavin? Then the natural reference cateogory is the group with 0 mg riboflavin. If you then analyze the output, the effect of \textbf{riboflavin} is then the increase or decrease in migraine frequency in people who took riboflavin. Thus, you code the people with 0 mg as 0 and those with 50 mg as 1.

\end{enumerate}



\section{Dummy coding for more than two groups}

In the previous sections we saw how to code a categorical variable with 2 categories (a dichotomous variable) into 1 dummy variable. In this section, we see how to code a categorical variable with 3 categories into 2 dummy variables, and to code a categorical variable with 4 categories into 3 dummy variables, etcetera. That is, how to code nominal variables into sets of dummy variables.

Take for instance the variable \textbf{Country}, where in your data set, there are three different values for this variable, for instance, Norway, Sweden and Finland, or Zimbabwe, Congo and South-Africa. Let's call these countries A, B and C. Table \ref{tab:countryheight} shows a data example.
 
 \begin{table}
 \caption{Height across three different countries.}
 \begin{tabular}{llr}
 ID & Country &  height\\ \hline
  001 &A & 120\\
  002 &A & 160\\
  003 &B & 121\\
  004 &B & 125\\
  005 &C & 140\\
  \dots & \dots & \dots\\
 \end{tabular} 
 \label{tab:countryheight}
 \end{table}


We can code this \textbf{Country} variable with three categories into two dummy variables in the following way. First, we create a variable \textbf{countryA}. This is a dummy variable, or indicator variable, that indicates whether a person comes from country A or not. Those that do are coded 1, and those that do not are coded 0. Next, we create a dummy variable \textbf{countryB} that indicates whether or not people come from country B. Again, those that do are coded 1 and those that do not are coded 0. The resulting variables are displayed in Table \ref{tab:dummy}
 
 \begin{table}
 \caption{Height across three different countries with dummy variables.}
 \begin{tabular}{llrrr}
 ID & Country &  height & countryA & countryB \\ \hline
  001 &A & 120 & 1 & 0\\
  002 &A & 160 & 1 & 0\\
  003 &B & 121 & 0 & 1\\
  004 &B & 125 & 0 & 1\\
  005 &C & 140 & 0 & 0\\
  \dots & \dots & \dots& \dots & \dots\\
  \label{tab:dummy}
 \end{tabular}
 \end{table}

Note that we have now for every value of \textbf{Country} (A, B, or C) a unique combination of the variables \textbf{countryA} and \textbf{countryB}. All those from country A have a 1 for \textbf{countryA} and a 0 for \textbf{countryB}; all those from country B have a 0 for \textbf{countryA} and a 1 for \textbf{countryB}, and all those from country C have a 0 for \textbf{countryA} and a 0 for \textbf{countryB}. Therefore a third dummy variable \textbf{countryC} is not necessary (i.e., is redundant). 

Remember that with two categories, you only need one dummy variable, where one category gets 1s and another category gets 0s. In this way both categories are uniquely identified. Here with three categories we also have unique codes for every category. Similarly, if you have 4 categories, you can code this with 3 dummy variables. In general, when you have a variable with $K$ categories, you can code them with $K-1$ dummy variables.


\subsection{Exercise}

Table \ref{tab:colours} shows data on the favourite colours named by 10 children. The only colours mentioned are blue, pink, purple, red and green. 

 \begin{table}
 \caption{Favourite colours named by ten children.}
 \begin{tabular}{llrrrrrr}
 ID & Colour &   &&&&&\\ \hline
  001 &purple & &&&&&\\
  002 &green &  &&&&&\\
  003 &red &  &&&&&\\
  004 &blue &  &&&&&\\
  005 &red &  &&&&&\\
  006 &pink &  &&&&&\\
  007 &pink &  &&&&&\\
  008 &green &  &&&&&\\
  009 &blue &  &&&&&\\
  010 &red &  &&&&&\\
 \end{tabular}
 \label{tab:colours}
 \end{table}

How many dummy variables do you need in order to uniquely identify each colour? Construct the dummy variables by hand and add them to the table. Don't forget the variable names. You can start with any colour you'd like.


\subsection{Answer}
You have 5 different colours, so you need $5-1=4$ dummy variables. Table \ref{tab:coloursexample} shows one possible solution. 

\begin{table}
\caption{Favourite colours named by ten children, with dummy variable coding.}
 \begin{tabular}{llrrrrrr}
 ID & Colour &  purple &green&red&blue&&\\ \hline
  001 &purple & 1&0&0&0&\\
  002 &green &  0&1&0&0&\\
  003 &red & 0&0&1&0&\\
  004 &blue &  0&0&0&1&\\
  005 &red & 0&0&1&0&\\
  006 &pink &  0&0&0&0&\\
  007 &pink &  0&0&0&0&\\
  008 &green &  0&1&0&0&\\
  009 &blue &  0&0&0&1&\\
  010 &red &  0&0&1&0&\\
 \end{tabular}
 \label{tab:coloursexample}
 \end{table}





\section{Analyzing categorical predictor variables in SPSS}

Suppose we have data on height based on a sample of thirty people ($N=30$) that come from three different countries. We want to know whether the average height is different for each country, or whether the average height is the same across countries (null-hypothesis). Since we know that applying a linear model to a categorical independent variable is the same as modelling group means, we can test the null-hypothesis that all group means are equal in the population. Let $\mu_A$ be the mean height in the population of country A, $\mu_B$ be the mean height in the population of country B, and $\mu_C$ be the mean height in the population of country C. Then we can specify the null-hypothsis using symbols in the following way:

\begin{equation}
H_0: \mu_A= \mu_B=\mu_C
\end{equation}

If all group means are equal in the population, then all population slopes would be 0. We want to test this null-hypothesis with a linear model in SPSS. Now there are two ways of doing this. The first option is that you can use dummy coding first, and then treat these dummy variables just as any numeric variables. The second option is that you let SPSS do the dummy coding for you, by indicating that you want to treat the original variable as a categorical variable. Let's start with the first option and then discuss the second option. Afterwards we will compare these two options.

\subsection{Treating dummy variables as numeric}


First we create two new dummy variables, and then perform a linear model analysis using these. Note that we actually perform a multiple regression with two dummy variables. We use the keyword WITH to indicate that we want to treat the dummy variables as numeric variables.


\begin{verbatim}
RECODE Country ('A'=1) ('B'=0) ('C'=0) INTO CountryA.
RECODE Country ('A'=0) ('B'=1) ('C'=0) INTO CountryB.
EXECUTE.

UNIANOVA height WITH CountryA CountryB
/ design = CountryA CountryB
/ print = parameter.
\end{verbatim}


\begin{figure}[h]
    \begin{center}
       \includegraphics[scale=0.5]{/Users/stephanievandenberg/Dropbox/Statistiek_Onderwijs/Data" "Analysis/spss" "examples" "mixed" "linear" "model/oneway/onewayquant.png}
    \end{center}
    \caption{Output of a multiple regression analysis on two dummy variables, using the keyword WITH.}
    \label{fig:dummy_21}
\end{figure}


In the Parameter Estimates table in Figure \ref{fig:dummy_21}, we see the effects (the 'slopes') of the two dummy variables. All observations with a 1 for variable \textbf{CountryA} get an extra predicted height of -2.4, and all observations with a 1 for variable \textbf{CountryB} get an extra predicted height of 10.1. So the expected height in country A equals $172.4 - 2.4 = 174.8$, and the expected height in country B equals $172.4+10.1=182.5 $. Observations in country C have a 0 for both variables \textbf{CountryA} and \textbf{CountryB}, so the expected height in country C equals the intercept 172.4.\\

In the Tests of Between-Subjects Effects table, we see other stuff going on. This is not regression output, but output based on a so-called Analysis Of VAriance, or ANOVA for short. This table is usually called an ANOVA table. First note that the significance levels (the $p$-values) for the two effects are exactly the same as those from the regression table. Second, note that the reported values of $F$ are the square of the $t$ values in the regression table: $-.799^2=.619$ and $3.364^2=11.317$. \\


ANOVA is a particular way of presenting a linear model. The $F$-statistic is constructed on the basis of Sums of Squares (SS, see Chapter \ref{chap:intro}). For instance, take a look at the row for the effect of \textbf{CountryA}. The sum of squares is equal to 28.80. This sum of squares is equal to the squared difference in average height in country A from the total average height, times the number of observations in country A \footnote{Suppose the average height across the three different countries equals 170 cms, and the average height in country A equals 171.7, then the squared difference in mean height equals $(171.1-170)^2=1.7^2=2.88$. If there are 10 people in the group from country A, then the Sum of Squares equals $2.88\times 10=28.80$}. If you divide this sum by the degrees of freedom for this dummy effect (which is 1), your get the Mean Square (MS): $28.8/1=28.80$. 

\begin{equation}
MS_{CountryA}=\frac{SS_{CountryA}}{df_{CountryA}}=\frac{28.80}{1}=28.80 \label{eq:MSgroup}
\end{equation}

Now look at the row for Error. The sum of squares equals 1216.90. This is exactly the sum of the squared residuals that is minimized using the Least Squares principle (see Chapter \ref{chap:simple}). Dividing this sum by the corresponding residual degrees of freedom you get the Mean Squared Error (MSE): $1216.90/27=45.07$. 

\begin{equation}
MS_{error}=\frac{SS_{error}}{df_{error}}=\frac{1216.90}{27}=45.07  \label{eq:MSerror}
\end{equation}


You obtain the $F$-statistic by dividing the \textbf{CountryA} Mean Square by the Mean Squared Error: 


\begin{equation}
F=\frac{MS_{CountryA}}{MS_{error}}=\frac{28.80}{45.07}=0.64 \label{eq:F}
\end{equation}

It is not a coincidence that this $F$-value is exactly equal to the square of the corresponding $t$-value (save some rounding errors): $F=t^2$. Remember that the $t$-value is equal to the $B$ parameter divided by the standard error: $t=-2.40/3.00=-.80=\sqrt{0.64}$. To obtain the regression coefficient we minimize the sum of squared residuals. So both the $F$-statistic and the $t$-statistic come from computing sums of squares and are thus based on the same general logic of the linear model.\\

Since ANOVA is a special way of presenting the linear model, we believe that it is not necessary to understand ANOVA fully: if you understand the linear model, that is good enough. Just remember that sometimes you see ANOVAs reported in the literature. Be aware that what the researchers are actually doing is running a linear model, or more specifically, their software \textit{runs} a linear model and then \textit{presents} the results as an analysis of variance table, similar to what SPSS is doing.

Returning back to our null-hypothesis that all group means are equal in the population: if the group means are equal in the population, then the slope parameters for \textbf{countryA} and \textbf{countryB} should consequently be 0 in the population. Looking at the 95\% confidence intervals, we see that 0 is a reasonable value for the difference between country C (the reference category) and country A, but 0 is \textit{not} a reasonable value for the difference between country C and country B. But how can we rigourously test the null-hypothesis that all three group means are the same? Now we have two $p$-values, one for the difference between country A and country C ($p=0.431$) and one of the difference between country B and country C ($p=0.002$), but we actually need one $p$-value for the null-hypothesis of three equal means. Let's see if we can get one $p$-value if we try the second way to perform this analysis.




\subsection{Treating the original variable as a categorical variable}

In the second approach, we let SPSS do the dummy variable coding automatically. In that case we use the original variable \textbf{Country} with its three categories directly, and change the WITH keyword into BY in the following way:

\begin{verbatim}
UNIANOVA height BY Country
/ design = Country
/ print = parameter.
\end{verbatim}

All variables named after BY are treated as categorical variables and automatically coded into dummy variables. The output is given in Figure \ref{fig:dummy_22}. The Parameter Estimates table now looks slightly different: the intercept is the same as in Table \ref{fig:dummy_21}, but the dummy effects are presented in a slightly different way, and there is an extra row for country C where a regression coefficient $B$ of 0 is reported, with no standard error, no $T$-statistic, and no $p$-value. The values for the other effects are exactly the same in with the previous analysis. This means we can interpret these [country=A] and [country=B] effects as the effects of dummy variables: all observations start from an intercept of 172.40 and depending on whether the observations are from country A or country B, you get an extra predicted height of -2.4 or 10.1, respectively. Observations from country C get an extra height of 0, so in effect nothing extra. (SPSS creates an extra dummmy variable for country C, but because this is not necessary, the effect is fixed to 0).

\begin{figure}[h]
    \begin{center}
       \includegraphics[scale=0.5]{/Users/stephanievandenberg/Dropbox/Statistiek_Onderwijs/Data" "Analysis/spss" "examples" "mixed" "linear" "model/oneway/onewayqual.png}
    \end{center}
    \caption{Output of a regression analysis on the original variable, using the keyword BY.}
    \label{fig:dummy_22}
\end{figure}



Also the Tests of Between-Subjects Effects table looks slightly different: instead of two separate effects for two dummy variables, we now see one row for the original variable Country. And in the column df (degrees of freedom): instead of 1 degree of freedom for a specific dichotomous country variable, we see 2 degrees of freedom for the nominal \textbf{Country} variable. So this suggests that \textit{the effects of the two dummy variables are now combined into one effect}, with one particular $F$-value, and one $p$-value that is also different from those of the two separate dummy variables. This is actually the $p$-value test for the null-hypothesis that all 3 means are equal: 

\begin{equation}
H_0: \mu_A= \mu_B=\mu_C
\end{equation}

This hypothesis test is very different from the $t$-tests in the Parameter Estimates table. The $t$-test for the [country=A] effect specifically tests whether the average height in country A is different from the average height in country C (the reference country). The $t$-test for the [country=B] effect specifically tests whether the average height in country B is different from the average height in country C (the reference country). Since these hypotheses do not refer to our original research question regarding \textit{overall} differences across all three countries, we do not report these $t$-tests, but we report the overall $F$-test from the Tests of Between-Subjects Effects table.

In general, the rule is that if you have a specific research question that addresses a particular null-hypothesis, you only report the statistical results regarding that null-hypothesis. All other $p$-values that your software happens to show in its output should be ignored. We will come back to this issue in Chapter \ref{chap:advanced}.






\section{$F$-test for comparing multiple group means}

Here we slightly elaborate on the $F$-test for testing null-hypotheses about group means. Remember that the $T$-statistic was based on the slope divided by its standard error. Above we saw that the $F$-statistic is based on the ratio of mean squared errors, that are in turn based on sums of squares. 

If we go back to Chapter \ref{chap:confidence} on the inference about population slopes, we remember that given that the population slope is 0, and if one draws many random samples, the distribution of $T$-statistics shows a $t$-distribution with a certain degrees of freedom that depends on sample size. Similarly for inference about population group means, given a null-hypothesis that $K$ group means are equal in the population, and if one draws many random samples from this population, the $F$-statistic shows an $F$-distribution with a model degrees of freedom of $K-2$ and an error degrees of freedom that depends on the sample size, $N-2$.

Figure \ref{fig:dummy_23} shows the $F$-distribution with 2 model degrees of freedom and 156 residual degrees of freedom. As an be seen, $F$-values are always positive. This is so because they are based on sums of squares, and squares are always positives. The larger the $F$-value, the less likely it is to be the result of sampling error. Thus, if the $F$-value is very large, it is not likely that the population means are equal. 

When is an $F$-value large enough to think that the null-hypothesis is not true? Similar to $T$-statistics, we can choose our own level of significance, say $\alpha=0.05$, and reject the null-hypothesis when the $F$-value is beyond the critical value for the $\alpha$-level. For this particular $F$-distribution, the critical $F$-value for $\alpha=0.05$ is \Sexpr{round(qf(0.95,2,156),2)}. This number can be looked up in tables or is available in software packages like SPSS. Thus, if we find an $F$-value equal to or larger than \Sexpr{round(qf(0.95,2,156),2)}, we reject the null-hypothesis. If the $F$-value is less than \Sexpr{round(qf(0.95,2,156),2)}, we do not reject the null-hypothesis. 

<<dummy_23,fig.height=4, echo=FALSE, fig.align='center', fig.cap='The F-distribution with 2 model degrees of freedom and 156 error degrees of freedom. The shaded area is the upper 5 percent of the distribution. The critical F-value for alpha=0.05 is depicted by the vertical line (3.05).'>>=
df = 156; ncp = 0; limits = c(0,8)
lb=-20; ub=qf(0.95, df1=2,df2=df, ncp=ncp)
    x <- seq(limits[1], limits[2], length.out = 100)
    xmin <- max(lb, limits[1])
    xmax <- min(ub, limits[2])
    areax <- seq(xmax,8, length.out = 100)
    area <- data.frame(x = areax, ymin = 0, ymax = df(areax, df1=2, df2=df, ncp=ncp))
    (ggplot()
     + geom_line(data.frame(x = x, y = df(x, df1=2,df2=df, ncp=ncp)),
                 mapping = aes(x = x, y = y))
     + geom_area(data = area, mapping = aes(x = x,  y = ymax))
     + scale_x_continuous(limits = limits, breaks=seq(-8,8,1))
            + geom_vline(xintercept =qf(0.95, df1=2,df2=df, ncp=ncp) )  + xlab("F")
            + ylab("density"))
@




\section{Reporting ANOVA}
In all cases where you have a categorical predictor variable with more than two categories, and where the null-hypothesis is about the equality of all group means, you have to use the BY syntax in SPSS and using the original nominal variable. You then always report the corresponding $F$-statistic from the Tests of Between-Subjects Effects table. 

For this particular example, you report the results of the analysis of variance in the following way:

\begin{quote}
``The null-hypothesis that all 3 population means are equal was tested with a linear model (analysis of variance). The results showed that the null-hypothesis can be rejected: the means in the population are not equal, $F(2, 27) = 9.76, MSE = 45.07 , p = 0.001$.''
\end{quote}

Always check the degrees of freedom for your $F$-statistic carefully. The first number refers to the number of dummy variables that are tested at once: this is the number of categories minus 1 ($K-1$). This is also called the \textit{model degrees of freedom}. The second number refers to the error degrees of freedom: this is the number of observations minus the number of effects in your model. In this model you have 30 data points and you have three effects (parameters): one intercept, one effect for [Country=A], and one effect for [Country=B]. The effect for [Country=C] is not really an effect because it is redundant and therefore fixed to 0. So your residual (error) degrees of freedom is $30-3=27$. Note that this residual degrees of freedom is equal to that of the $t$-statistic for multiple regression.





\section{Relationship between $F$- and $T$-distributions}

We've also stated that the $t$-distribution and the $F$-distribution have much in common. Here we will illustrate this. Suppose that we test the null-hypothesis that a certain population slope is 0. We perform a regression analysis and obtain a $T$-statistic of -2.40. Suppose our sample size was 42, so that our residual degrees of freedom equals $42-2=40$. Figure \ref{fig:dummy_24} shows the theoretical $t$-distribution with 40 degrees of freedom. It also shows our value of -2.40. The shaded area represents the values for $T$ that would be significant at an $\alpha=0.05$.   


<<dummy_24,fig.height=4, echo=FALSE, fig.align='center', fig.cap='The vertical line represents a T-value of -2.40. The shaded area represents the extreme 5 percent of the possible T-values'>>=

df = 38; ncp = 0; limits = c(-5,5)
lb=-20; ub=qt(0.025,df=40)
    x <- seq(limits[1], limits[2], length.out = 100)
    xmin <- max(lb, limits[1])
    xmax <- min(ub, limits[2])
    areax <- seq(xmin, xmax, length.out = 100)
    area <- data.frame(x = areax, ymin = 0, ymax = dt(areax, df=df, ncp=ncp))
    (ggplot()
     + geom_line(data.frame(x = x, y = dt(x, df=df, ncp=ncp)),
                 mapping = aes(x = x, y = y))
     + geom_area(data = area, mapping = aes(x = x,  y = ymax), alpha=0.5)
     + scale_x_continuous(limits = limits, breaks=seq(-5,5,1))
     + geom_area(data = area, mapping = aes(x = seq(qt(0.975,df=40), 5, length.out = 100),  y = dt(seq(qt(0.975,df=40), 5, length.out = 100), df=df, ncp=ncp)), alpha=0.5)
            + geom_vline(xintercept = -2.40)  + xlab("T")  +ylab("density"))
@


Now look closely at Figure \ref{fig:dummy_24}. The density says something about the probability of drawing certain values. Imagine that you randomly pick numbers from this $T$-distribution. The density plot tells you that values around zero are more probable than values around 2 or -2, and that values around 2 or -2 are more probable than values around 3 or -3. Imagine that you pick a million values for $T$, randomly from this $T$-distribution. Then imagine that you take the square of each value (thus, suppose as the first 3 randomly drawn $T$-values you get -3.12, 0.14, and -1.6, you then square these numbers to get the numbers 9.73, 0.02, and 2.79). If you then make a density plot of these one million squared numbers, you get the density plot in Figure \ref{fig:dummy_25}. It turns out that this density is an $F$-distribution with 1 model degrees of freedom and 40 residuals degrees of freedom. 


<<dummy_25,fig.height=4, echo=FALSE, fig.align='center', fig.cap='The F-distribution with 1 model degrees of freedom and 40 error degrees of freedom. The shaded area is the upper 5 percent of the distribution. The vertical line represents the the square of -2.40: 5.76'>>=
df = 40; ncp = 0; limits = c(0.15,8)
lb=0.15; ub=qf(0.95, df1=2,df2=df, ncp=ncp)
    x <- seq(limits[1], limits[2], length.out = 100)
    xmin <- max(lb, limits[1])
    xmax <- min(ub, limits[2])
    areax <- seq(xmax,8, length.out = 100)
    area <- data.frame(x = areax, ymin = 0, ymax = df(areax, df1=1, df2=df, ncp=ncp))
    (ggplot()
     + geom_line(data.frame(x = x, y = df(x, df1=1,df2=df, ncp=ncp)),
                 mapping = aes(x = x, y = y))
     + geom_area(data = area, mapping = aes(x = x,  y = ymax), alpha=0.5)
     + scale_x_continuous(limits = limits, breaks=seq(-8,8,1))
            + geom_vline(xintercept =5.76 )  + xlab("F")
            + ylab("density"))
@


If we also square the observed test statistic $T$-value of -2.40, we obtain an $F$-value of 5.76. From online tables, we know that, with 1 model degrees of freedom and 40 residual degrees of freedom, the proportion of $F$-values larger than 5.76 equals \Sexpr{round(1-pf(5.76,1,40),2)}. The proportion of $T$-values, with 40 (residual) degrees of freedom, larger than 2.40 or smaller than -2.40 is also \Sexpr{round(2*pt(-2.40,40),2)}. Thus, the two-sided $p$-value associated with a certain $T$-value, is equal to the $p$-value associated with an $F$-value that is the square of the $T$-value. 

This means that if you see a $T$-statistic of say -2.40 reported with a residuals degrees of freedom of 40, $t(40)=-2.40$, you can equally report this as an $F(1,40)=5.76$. Similarly, if you see a reported $F$-value of $F(1,67)=49$, you could without problems turn this into a $t(67)=7$. Note however that this only the case if the \textit{model} degrees of freedom of the $F$-statistic is equal to 1. This means you cannot do this if you are comparing more than two groups means. Next time you look at UNIANOVA output, watch the $t$-statistics and $F$-statistics carefully and check whether the $F$-statistic is the square of the $T$-statistic. Check for instance Figure \ref{fig:dummy_21} again, and see whether the $F$-values associated with the intercept, the \textbf{countryA} effect and the \textbf{countryB} effect reported in the top table are the indeed the square of the respective $T$-values reported in the bottom table. 


\subsection{Exercises}


\begin{enumerate}



\item In Table \ref{tab:anova_1} you see an Analysis of Variance table. It reports the results of a test of the null-hypothesis that the average yield based on three different treatments are the same: the control condition, treatment 1 and treament 2. See if you can plug in the missing values, based on Equations \ref{eq:MSgroup},\ref{eq:MSerror} and \ref{eq:F}.

<<anova_1,fig.height=4, echo=FALSE, fig.align='center', fig.cap='The F-distribution with 2 model degrees of freedom and 156 error degrees of freedom. The shaded area is the upper 5 percent of the distribution. The critical F-value for alpha=0.05 is depicted by the vertical line (3.05).', results='asis'>>=
plant.df = PlantGrowth
plant.df$group = factor(plant.df$group,
  labels = c("Control", "Treatment 1", "Treatment 2"))
plant.mod1 = lm(weight ~ group, data = plant.df)
t <-anova(plant.mod1)
t<- data.frame(t)
t[,3] <- rep("?",2)
t[,4] <- c("?","--")
t[,5] <- c("0.0159","--")
names(t) <- c("df", "SS", "MS", "F", "p")
t%>% 
        xtable(caption="Analysis of Variance table.", label="tab:anova_1") %>%
        print(include.rownames=T, caption.placement = "top")
@

\item Choose as your type I error rate an $\alpha$ of 0.01. Can you reject the null-hypothesis? 

\item Write down your result of your hypothesis testing. 

\item The $F$-distribution and the $t$-distribution are closely related. Is there a way in which you could write down the same result in terms of $T$-statistics? If so, please do. If not, explain why this is not possible. 

\end{enumerate}

\subsection{Answers}

\begin{enumerate}

\item 

Table \ref{tab:anova_1} shows the missing Mean Squares for Group and Error (Residual), as well as the F-statistic which is the ratio of the two the Mean Squares. 

<<anova_2,fig.height=4, echo=FALSE, fig.align='center', fig.cap='The F-distribution with 2 model degrees of freedom and 156 error degrees of freedom. The shaded area is the upper 5 percent of the distribution. The critical F-value for alpha=0.05 is depicted by the vertical line (3.05).', results='asis'>>=
anova(plant.mod1) %>% 
        xtable(caption="Analysis of Variance table.", label="tab:anova_2") %>%
        print(include.rownames=T, caption.placement = "top")
@

\item The $p$-value associated with the $F$-value for the Group effect is larger than $\alpha$. Therefore, we cannot reject our null-hypothesis of equal group means. 


\item 

\begin{quote}
``The null-hypothesis that all 3 population means are equal was tested with a linear model (analysis of variance) at an $\alpha$ of 0.01. The results showed that the null-hypothesis cannot be rejected, $F(2, 27) = 4.85, MSE = 10.49 , p = 0.02$.''
\end{quote}


\item Yes, the two distributions are related, but the $F$-statistic is only the square of the $T$-statistic if the model degrees of freedom equals 1. Here we have 3 groups and therefore 2 model degrees of freedom. Therefore, we can only use the $F$-statistic to describe our results.



\end{enumerate}



