


\chapter{Generalised linear models for count data: Poisson regression}\label{chap:poisson}


\section{Poisson regression}


Count data are inherently discrete, and often when using linear models, we see non-normal distributions of residuals. In Chapter \ref{chap:logistic} we discussed a data set on the scores that a group of students got for an assignment. There were four criteria, and the score consisted of the number of criteria that were met for each student's assignment. Figure \ref{fig:gen_3} showed that after an ordinary linear model analysis, the residuals did not look normal at all.

Table \ref{tab:gen_14} shows part of the data that were analysed. The dependent variable was \texttt{score}. Similar to logistic regression, perhaps we can find a distribution other than the normal distribution that is more suitable for this kind of dependent variable? For dichotomous data (1/0) we found the Bernoulli distribution very useful. For count data like this variable \texttt{score}, the traditional distribution is the Poisson distribution.


<<gen_14, fig.height=4, echo=FALSE, fig.align='center', message=F, results='asis'>>=
detach("package:lmerTest", unload = TRUE)
detach("package:lme4", unload = TRUE)
rm(model3)
set.seed(1234)
ID <- seq(1:100)
score <- rnorm(100, 1, 3) %>% round(0)
score [score > 4] <- 4
score [score < 0] <- 0
previous = rnorm(100, 0, 1)
data <- data.frame(ID, score, previous)
data %>%
  head() %>%
  xtable(caption="Scores on an assignment.", 
         label="tab:gen_14", 
         digits = c(0, 0, 0, 2)) %>%
  print(include.rownames = F, caption.placement = "top")
outcount <-  glm(score ~ previous, data = data , family = "poisson" )
# density <- dnorm(residual, mean(residual), sd(residual))
# data <- data.frame(residual, density)
# data %>% ggplot(aes(x=residual, y=density)) + geom_histogram(aes(y=..density..))  + geom_line(aes(y=density))
@

The normal distribution has two parameters, the mean and the variance. The Bernoulli distribution has only 1 parameter (the probability), and the Poisson distribution has also only 1 parameter, $\lambda$ (Greek letter 'lambda'. $\lambda$ is a parameter that indicates tendency. Figure \ref{fig:gen_15} shows a Poisson distribution with a tendency of 4.

<<gen_15, fig.height=3.5, echo=FALSE, fig.align='center', message=F, fig.cap='Count data example where the normal distribution is not a good approximation of the distribution of the residuals.'>>=
set.seed(1234)
probability4 <- dpois(seq(0, 12), 4)
data.frame(probability4) %>% 
  ggplot(aes(x = seq(0, 12), y = probability4)) +
  geom_bar(stat = "identity") + 
  xlim(c(0, 12)) + 
  scale_x_continuous(breaks = seq(0, 12)) +
  xlab("count") + 
  ylab("probability")
@

What we see is that many values centre around the tendency parameter value of 4 (therefore we call it a tendency parameter). We see only discrete values, and no values below 0. We see a few values higher than 10. If we take the mean of the distribution, we will find a value of 4. If we would compute the variance of the distribution we would also find 4. In general, if we have a Poisson distribution with a tendency parameter $\lambda$, we know that both the mean and the variance will be equal to $\lambda$.

A Poisson model could be suitable for our data: a linear equation could predict the parameter $\lambda$ and then the actual data show a Poisson distribution.


\begin{eqnarray}
\lambda = b_0 + b_1 X \\
Y \sim Poisson(\lambda)
\end{eqnarray}

However, because of the additivity assumption, the equation $b_0 + b_1 X$ can lead to negative values. A negative value for $\lambda$ is not logical, because we then have a tendency to observe data like -2 and -4 in our data, which is contrary to having count data, which consists of non-negative integers. A Poisson distribution always shows integers of at least 0, so one way or another we have to make sure that we always have a $\lambda$ of at least 0.

Remember that we saw the reverse problem with logistic regression: there we wanted to have negative values for our dependent variable logodds ratio, so therefore we used the logarithm. Here we want to have positive values for our dependent variable, so we can use the inverse of the logarithm function: the exponential. Then we have the following model:


\begin{eqnarray}
\lambda = \textrm{exp}(b_0 + b_1 X)= e^{b_0+b_1X} \\
Y \sim Poisson(\lambda)
\end{eqnarray}


This is a generalised linear model, now with a Poisson distribution and an exponential link function. The exponential function makes any value positive, for instance $\textrm{exp}(0)=1$ and $\textrm{exp}(-100)=\Sexpr{round(exp(-100),4)}$.

Let's analyse the assignment data with this generalised linear model. Our dependent variable is the number of criteria met for the assignment (a number between 0 and 4), and the independent variable is \texttt{previous}, which is a standardised mean of a number of previous assignments. That means that a student with \texttt{previous = 0}, that student had a mean grade for previous assignment equal to the mean grade for all students. We expect that a high mean score on previous assignments is associated with a higher score on the present assignment. When we run the analysis, the result is as follows:


\begin{eqnarray}
\lambda = \textrm{exp}(\Sexpr{outcount$coef[1]} \Sexpr{outcount$coef[2]} \times \texttt{previous}) \\
\texttt{score} \sim Poisson(\lambda)
\end{eqnarray}

What does it mean? Well, similar to logistic regression, we can understand such equations by making some predictions for interesting values of the independent variable. For instance, a value of 0 for \texttt{previous} means an average grade on previous assignments that is equal to the mean of all students. So if we choose \texttt{previous = 0}, then we have the prediction for an average student. If we fill in that value, we get the equation $\lambda=\textrm{exp}(\Sexpr{outcount$coef[1]} \Sexpr{outcount$coef[2]} \times 0)= \textrm{exp} (\Sexpr{outcount$coef[1]})= \Sexpr{round(exp(outcount$coef[1]),2)}$. Thus, for an average student, we expect to see a score of \Sexpr{round(exp(outcount$coef[1]),2)}. A Poisson distribution with $\lambda=\Sexpr{round(exp(outcount$coef[1]),2)}$ is depicted in Figure \ref{fig:gen_16}.


<<gen_16, fig.height=3.5, echo=FALSE, fig.align='center', message=F, fig.cap='Poisson distribution with lambda=1.17.'>>=
# set.seed(1234)
# number <- rpois(100000, round(exp(outcount$coef[1]),2))
# data.frame(number) %>% ggplot(aes(x=number) )+geom_histogram(binwidth = 0.5) +xlim(c(0,11))

set.seed(1234)
probability2 <- dpois(seq(0, 10), round(exp(outcount$coef[1]), 2))
tibble(probability2) %>% 
  ggplot(aes(x = seq(0, 10), y = probability2)) + 
  geom_bar(stat = "identity") + 
  xlim(c(0, 12)) + 
  scale_x_continuous(breaks = seq(0, 12)) +
  xlab("count") + 
  ylab("probability")

@

Another interesting value of \texttt{previous} might be -2. That represents a student with generally very low grades. Because the average grades were standardised, only about 2.5\% of the students has a lower average grade than -2. If we fill in that value, we get: $\lambda=\textrm{exp}(\Sexpr{outcount$coef[1]}  \Sexpr{outcount$coef[2]} \times -2)= \Sexpr{round(exp(outcount$coef[1]-2*outcount$coef[2]),2)}$. A Poisson distribution with $\lambda=\Sexpr{round(exp(outcount$coef[1]-2*outcount$coef[2]),2)}$ is depicted in Figure \ref{fig:gen_17}.

<<gen_17, fig.height=3.5, echo=FALSE, fig.align='center', message=F, fig.cap='Poisson distribution with lambda = 1.31.'>>=
# set.seed(1234)
# number <- rpois(100000, round(exp(outcount$coef[1]-2*outcount$coef[1]),2))
# data.frame(number) %>% ggplot(aes(x=number) )+geom_histogram(binwidth = 0.5) +xlim(c(0,11))

set.seed(1234)
probability1 <- dpois(seq(0, 10), round(exp(outcount$coef[1] - 2*outcount$coef[2]), 2))
tibble(probability1) %>% 
  ggplot(aes(x = seq(0, 10), y = probability1)) +
  geom_bar(stat = "identity") + 
  xlim(c(0, 12)) + 
  scale_x_continuous(breaks = seq(0, 12)) +
  xlab("count") + 
  ylab("probability")


@

The last value of \texttt{previous} for which we calculate $\lambda$ is +2, representing a high-performing student. We then get $\lambda=\textrm{exp}(\Sexpr{outcount$coef[1]}  \Sexpr{outcount$coef[2]} \times 2)= \Sexpr{round(exp(outcount$coef[1]+2*outcount$coef[2]),2)}$. A Poisson distribution with $\lambda=\Sexpr{round(exp(outcount$coef[1]+2*outcount$coef[2]),2)}$ is depicted in Figure \ref{fig:gen_18}.

<<gen_18, fig.height=3.5, echo=FALSE, fig.align='center', message=F, fig.cap='Poisson distribution with lambda = 1.05.'>>=
# set.seed(1234)
# number <- rpois(100000, round(exp(outcount$coef[1]+2*outcount$coef[1]),2))
# data.frame(number) %>% ggplot(aes(x=number) )+geom_histogram(binwidth = 0.5) +xlim(c(0,11))

set.seed(1234)
probability3 <- dpois(seq(0, 10) , round(exp(outcount$coef[1]+2*outcount$coef[2]), 2))
tibble(probability3) %>% 
  ggplot(aes(x = seq(0, 10), y = probability3)) +
  geom_bar(stat = "identity")+ 
  xlim(c(0, 12)) + 
  scale_x_continuous(breaks = seq(0, 12)) +
  xlab("count") + 
  ylab("probability") 


@


If we superimpose these three figures, we obtain Figure \ref{fig:gen_19}, where we see that the higher the average score on previous assignments, the lower the expected score on the present assignment.

<<gen_19, fig.height=3.5, echo=FALSE, fig.align='center', message=F, warning=F, fig.cap='Three different Poisson distributions with lambdas 0.85, 1.31, and 1.05, for three different kinds of students.'>>=

average <- probability2

low <- number <- probability1

high <- probability3
scores <- c(low, average, high)

previous <- factor(rep(c("low", "average", "high"), each = 11))
previous <- factor(previous,levels(previous)[c(3, 1, 2)])
data.frame(scores, previous) %>%
  ggplot(aes(x = rep(seq(0, 10), 3), y = scores))+
  geom_bar(aes(fill = previous), col = "black", stat = "identity", position = "dodge") + 
  scale_x_continuous(breaks = seq(0, 10)) +
  xlab("count") +
  ylab("probability") +
  scale_fill_brewer(palette = "Blues")

set.seed(1234)
ID <- seq(1:100)
score <- rnorm(100, 1, 3) %>%  round(0)
score [score>4] <- 4
score [score<0] <- 0
previous = rnorm(100, 0, 1)
degree <- rep(c("Bachelor","Master", "PhD"), 50)
data <- data.frame(ID, score, previous, degree = degree[1:100])
# source('/Users/stephanievandenberg/Dropbox/Statistiek_Onderwijs/Data Analysis/writeForeignCode.R')
# write.foreign(data,
#               '/Users/stephanievandenberg/Dropbox/Statistiek_Onderwijs/Data Analysis/spss examples poisson/assignment.sav',
#               '/Users/stephanievandenberg/Dropbox/Statistiek_Onderwijs/Data Analysis/spss examples poisson/assignment.sps',
#               package = c("SPSS"))
@



We found that in this data set, previous high marks for assignments predicted a higher mark for the present assignment. In the next section we will see how to perform the analysis in R, and how to check whether there is also a relationship in the population of students.

\section{Poisson regression in R}

Poisson regression is a form of a generalised linear model analysis, similar to logistic regression. However, instead of using a Bernoulli distribution we use a Poisson distribution. For a numeric predictor like the variable \texttt{previous}, the syntax is as follows.



<<>>=
model <- data %>% 
  glm(score ~ previous, family = poisson, data = .)
model %>% tidy()
confint(model)
@



We see the same values for the intercept and the effect of \texttt{previous} as in the previous section. We now also see 95\% confidence intervals for these parameter values. For both, the value 0 is included in the confidence intervals, therefore we know that we cannot reject the null-hypotheses that these values are 0 in the population of students. This is also reflected by the test statistics. Remember that the $z$-statistic is computed by $b/SE$. For large enough samples, the $z$-statistic follows a standard normal distribution. From that distribution we know that a value of -0.610 is not significant at the 5\% level. It has an associated $p$-value of 0.542.

We can write:

\begin{quotation}
"Scores for the assignment (1-4) for 100 students were analysed using a generalised linear model with a Poisson distribution (Poisson regression), with independent variable previous. The scores were not predicted by the average score of previous assignments, $b = -0.06, z = -0.61, p = .54$. Therefore we found no evidence of a relationship between the average of previous assignments and the score on the present assignment in the population of students."
\end{quotation}



Suppose we also have a categorical predictor, for example the degree that the students are working for. Some do the assignment for a bachelor's degree (\texttt{degree = "Bachelor"}), some for a master's degree (\texttt{degree= "Master"}), and some for a PhD degree (\texttt{degree = "PhD"}). The code would then look like:



<<>>=
model2 <- data %>% 
glm(score ~ degree, family = poisson, data = .) 
model2 %>% 
  tidy()
confint(model2)
@



Note that only the independent variable has changed. 


We see that the \texttt{degree = "Bachelor"} category is used as the reference category. If we make a prediction for the group of students that is studying for a PhD degree, we have $\lambda = \textrm{exp}(-0.231 + 0.584) = \textrm{exp}(0.353)=\Sexpr{round(exp(0.353),1)}$. For the students studying for a Master's degree we have $\lambda = \textrm{exp}(-0.231 + 0.495) =\Sexpr{round(exp(0.264),1)}$ and for students studying for their Bachelor's degree we have $\lambda = \textrm{exp}(-0.231) =\Sexpr{round(exp(-0.231),1)}$. These $\lambda$-values correspond to the expected count in a Poisson distribution, so for Bachelor students we expect a score of $\Sexpr{round(exp(-0.231),1)}$, for Master students we expect a score of $\Sexpr{round(exp(0.264),1)}$ and for Phd students a score of $\Sexpr{round(exp(0.353),1)}$. Are these differences in scores also present in the population? We see that the effect for \texttt{degree = "Master"} is significant, $z=2.02, p=0.04$, so there is a difference in score between students studying for a Bachelor's degree and students studying for a Master's. The effect for \texttt{degree = PhD} is also significant, $z=0.584, p=0.016$, so there is a difference in assignment scores between Bachelor students and PhD students.
\\
\\
Remember that for the linear model, when we wanted to compare more than two groups at the same time, we used an $F$-test to test for an overall difference in group means. Also for the generalised linear model, we might be interested in whether there is an overall difference in scores between Bachelor, Master and PhD students. For that we need to perform an analysis of variance.


<<>>=
model2 %>% 
  anova(test = "Chisq")
@



However, what we see here is not an analysis of variance, but a model comparison (an Analysis of Deviance Table is plotted). A model with the predictor \texttt{degree} is compared to a baseline model with only an intercept, the so-called null model. The model with only an intercept has 99 residual degrees of freedom. The model with an intercept and the effect of \texttt{degree} has only 97 residual degrees of freedom. The difference of 2 degrees of freedom comes from the fact that the model with \texttt{degree} requires estimating the effects of two dummy variables. 

For both models the \textit{deviance} is calculated. This deviance is based on the method that is used for estimating the parameters. Where in linear models, usually OLS is used (least squares principle), in generalised methods usually the method of Maximum Likelihood is used. The difference in the likelihoods of the two models says something about how significant the effect of the predictor, in this case \texttt{degree}, is. It so happens that minus twice the difference in the logarithm of the likelihood has a distribution close to the chi-square distribution. Minus twice the log-likelihood of a model is called the deviance. The difference between two deviances is also called a deviance. Here we see a deviance of 6.8186 for the difference between the two models. When we look up what the proportion of the chi-square distribution with 2 degrees of freedom is larger than 6.8186, we know the $p$-value. This is exactly what R does, and the output shows a $p$-value of 0.03306. This $p$-value tells us whether the null-hypothesis that the expected scores in the three groups of students are the same can be rejected. We report:


\begin{quotation}
"The null-hypothesis that the expected scores in the three groups were equal was tested with a Poisson regression with degree as the independent variable. Results showed that the null-hypothesis could be rejected, $\chi^2(2) = 6.82, p = .03$."
\end{quotation}





\section{Interaction effects in Poisson models}

In the previous section we looked at a count variable, the number of criteria fulfilled, and we wanted to predict it from the degree that students were studying for. Let's look at an example where we want to predict a count variable using two categorical predictors.

In 1912, the ship Titanic sank after the collision with an iceberg. There were \Sexpr{sum(Titanic)} people on board that ship. Some of these were male, others were female. Some were passengers, others were crew, and some survived, and some did not. For the passengers there were three groups: those travelling first class, second class and third class. There were also children on board. If we focus on only the adults, suppose we want to know whether there is a relationship between the sex and the counts of people that survived the disaster. Table \ref{tab:gen_20} gives the counts of survivors for males and females separately.


<<gen_20, fig.height=4, echo=FALSE, fig.align='center', message=F, warning=F, results="asis">>=

## Higher survival rates in females?
data <- as.matrix(apply(Titanic, c(2, 3, 4), sum)[ , 2, 2])
colnames(data) = "count"
data %>%
  xtable(caption = "Counts of adult survivors on the Titanic.", 
         label = "tab:gen_20",
         digits = c(0)) %>%
  print(include.rownames = T, caption.placement = "top")

# data.spss <- data.frame(sex = c("Male", "Female"), data)

# source('/Users/stephanievandenberg/Dropbox/Statistiek_Onderwijs/Data Analysis/writeForeignCode.R')
# write.foreign(data.spss,
#               '/Users/stephanievandenberg/Dropbox/Statistiek_Onderwijs/Data Analysis/spss examples poisson/titanic1.sav',
#               '/Users/stephanievandenberg/Dropbox/Statistiek_Onderwijs/Data Analysis/spss examples poisson/titanic1.sps',
#               package = c("SPSS"))

# outtitanic1 <- glm(count~ sex, data=data.spss, family="poisson")
# exp(outtitanic1$coef)
@


Let's analyse this data set with R. First we load the \texttt{Titanic} dataset and restructure the data to get counts for the numbers of females and males. 

<<>>=
data("Titanic")
# count the totals of females and males that survived
data <- as.matrix(apply(Titanic, c(2, 3, 4), sum)[ , 2, 2])
# give the variable the name 'count'
colnames(data) = "count"
# there are two cases and only 1 variable
dim(data)
# sex labels are only row labels. Introduce a variable sex
data <- data.frame(sex = c("Male", "Female"), data)
dim(data)
data
@


Next, our dependent variable is \texttt{count}, and the independent variable is \texttt{sex}. Let's do a Poisson regression.

<<>>=

model <- data %>% 
  glm(count ~ sex, family = poisson, data = .)
model %>% tidy()
@



From the output we see that the expected count for females (the reference group) is $\textrm{exp}(5.76)=\Sexpr{round(exp(5.76),1)}$ and the expected count for males is $\textrm{exp}(5.76 + 0.0673)=\Sexpr{round(exp(5.8273),1)}$. These expected counts are close to the observed counts of males and females. The only reason that they differ from the observed is because of rounding errors. From the $z$-statistic, we see that the difference in counts between males and females is not significant, $z=0.86, p=.39$\footnote{Note that a hypothesis test is a bit odd here: there is no clear population that we want to generalise the results to: there was only one Titanic disaster. Also, here we have data on the entire population of those people on board the Titanic, there is no random sample here.}.

The difference in these counts is very small. But does this tell us that women were as likely to survive as men? Note that we have only looked at those who survived. How about the people that perished: were there more men that died than women? Table \ref{tab:gen_21} shows the counts of male survivors, female survivors, male non-survivors and female non-survivors. Then we see a different story: on the whole there were many more men than women, and a relatively small proportion of the men survived. Of the men, most of them perished: 1329 perished and only 338 survived, a survival rate of \Sexpr{round(100*338/(338+1329),1)}\%. Of the women, most of them survived: 109 perished and 316 survived, yielding a survival rate of \Sexpr{round(100*316/(316+109),0)}\%. Does this tell us that women are much more likely than men to survive collisions with icebergs?


<<gen_21, fig.height=4, echo=FALSE, fig.align='center', message=F, warning=F, results="asis">>=

## Higher survival rates in females?
data <- as.matrix(apply(Titanic, c(2, 3,4), sum)[ , 2, ])

data <- as.vector(data)

data <- matrix(data, 4, 1)


sex <- rep(c("Male", "Female"), 2)
survived <- rep(c(0, 1), each = 2)
data <- data.frame(sex, survived, count = data)

data %>%
  xtable(caption = "Counts of adults on the Titanic.", 
         label = "tab:gen_21",
         digits = c(0)) %>%
  print(include.rownames = F, caption.placement = "top")


# source('/Users/stephanievandenberg/Dropbox/Statistiek_Onderwijs/Data Analysis/writeForeignCode.R')
# write.foreign(data,
#               '/Users/stephanievandenberg/Dropbox/Statistiek_Onderwijs/Data Analysis/spss examples poisson/titanic2.sav',
#               '/Users/stephanievandenberg/Dropbox/Statistiek_Onderwijs/Data Analysis/spss examples poisson/titanic2.sps',
#               package = c("SPSS"))
# 
# # outtitanic2 <- glm(count~ sex + survived, data=data, family="poisson") %>% summary()

@

Let's first run a multiple Poisson regression analysis including the effects of both sex and survival. We first need to restructure the data a bit, so that we have both variables \texttt{sex} and \texttt{survived}.

<<>>=
data <- as.matrix(apply(Titanic, c(2, 3,4), sum)[ , 2, ])
data <- as.vector(data)
data <- matrix(data, 4, 1)
sex <- rep(c("Male", "Female"), 2)
survived <- rep(c(0, 1), each = 2)
data <- data.frame(sex, survived, count = data)
data
@

Next, we run the multiple Poissson regression.

<<>>=
model2 <- data %>% 
  glm(count ~ sex + survived, family = poisson, data = .)
model2 %>% tidy()
@


where \texttt{sex} is treated as a factor, since it is a string variable, and survival as a numeric variable (since \texttt{survived} is coded as a dummy).




From the parameter values in the output, we can calculate the predicted numbers of male (\texttt{sex = Male}) and female (\texttt{sex = Female}) that survived and perished. The reference group consists of individuals that perished (\texttt{survived = 0}) and are female. For perished females we have $\textrm{exp}(5.68)=\Sexpr{round(exp(5.68),2)}$, for female survivors we have $\textrm{exp}(5.68 - 0.788)=\Sexpr{round(exp(5.68 - 0.788),2)}$, for male survivors we have $\textrm{exp}(5.68 + 1.37 - 0.788)=\Sexpr{round(exp(5.68 + 1.37 - 0.788),2)}$ and for male non-survivors we have $\textrm{exp}(5.68 + 1.37)=\Sexpr{round(exp(5.68 + 1.37),2)}$.


These predicted numbers are displayed in Figure \ref{fig:gen_22}. It also shows the observed counts. The pattern that is observed is clearly different from the one that is predicted from the generalised linear model. The linear model predicts that there are fewer survivors than non-survivors, irrespective of sex, but we observe that in females, there are more survivors than non-survivors. It seems that \texttt{sex} is a moderator of the effect of survival on counts.

<<gen_22, fig.height=3.5, echo=FALSE, fig.align='center', message=F, warning=F, results="asis", fig.cap="Difference between observed and predicted numbers of passengers.">>=
data$survived <- as.factor(data$survived)
outtitanic2 <- glm(count ~ sex + as.factor(survived),
  data = data,
  family = "poisson"
)
data <- rbind(data, data)
data$predicted <- as.factor(rep(c("predicted", "observed"), each = 4))
data$count[1:4] <- predict(outtitanic2, data[1:4, 1:2], type = "response")
data %>%
  ggplot(aes(x = sex, y = count, fill = survived)) +
  geom_bar(stat = "identity", position = "dodge") +
  facet_wrap(~predicted) +
  scale_fill_brewer(palette = "Set1")
@


In order to test this moderation effect, we run a new generalised linear model for counts including an interaction effect of \texttt{sex} by \texttt{survived}. This is done in R by adding \texttt{sex:survived}:


% \begin{lstlisting}
% model3 <- data %>% 
%   glm(count ~ sex + survived + sex:survived, family = poisson, data = .)
% model3 %>% tidy()
% \end{lstlisting}

<<eval = F>>=
model3 <- data %>% 
  glm(count ~ sex + survived + sex:survived, family = poisson, data = .)
model3 %>% tidy()
@



<<echo = F>>=
model3 <- data[5:8, ] %>% 
  glm(count ~ sex + survived + sex:survived, family = poisson, data = .)
model3 %>% tidy()
@


% <<warning = F, echo = F, eval = T>>=
% model3 <- data[5:8, ] %>% 
%   glm(count ~ sex + survived + sex:survived, family = poisson, data = .)
% model3 
% @

When we plot the predicted counts from this new model with an interaction effect, we see that they are exactly equal to the counts that are actually observed in the data, see Figure \ref{fig:gen_23}.




<<gen_23, fig.height=3.5, echo=FALSE, fig.align='center', message=F, warning=F, results="asis", fig.cap="Difference between observed and predicted numbers of passengers. Predictions based on a model with an interaction effect.">>=
outtitanic3 <- glm(count ~ sex + survived + sex:survived,
                   data = data[5:8, ],
                   family = "poisson"
)
data$count[1:4] <- predict(outtitanic3, data[1:4, 1:2], type = "response")
data %>% 
  ggplot(aes(x = sex, y = count, fill = survived)) +
  geom_bar(stat = "identity", position = "dodge") +
  facet_wrap(~predicted) +
  scale_fill_brewer(palette = "Set1")
@

From the output we see that the interaction effect is significant, $z=\Sexpr{round(summary(outtitanic3)$coef[4,3],1)}, p < .001$. If we regard this data set as a random sample of all ships that sink after collision with an iceberg, we may conclude that in such situations, sex is a significant moderator of the difference in the numbers of survivors and non-survivors. One could also say: the proportion of people that survive a disaster like this is different in females than it is in males. 


\section{Cross-tabulation and the Pearson chi-square statistic}

The data on male and female survivors and non-survivors are often tabulated in a cross-table like in Table \ref{tab:gen_24}


<<gen_24, fig.height=4, echo=FALSE, fig.align='center', message=F, warning=F, results="asis">>=
data <- as.matrix(apply(Titanic, c(2, 3, 4), sum)[, 2, ])

row.names(data) <- c("Male", "Female")
data %>%
  xtable(
    caption = "Counts of adult survivors and non-survivors on the Titanic.",
    label = "tab:gen_24",
    digits = c(0)
  ) %>%
  print(include.rownames = T, caption.placement = "top")
@



In the previous section these counts were analysed using a generalised linear model with a Poisson distribution and an exponential link function. We wanted to know whether there was a significant difference in the proportion of survivors for men and women. In this section we discuss an alternative method of analysing count data. We describe an alternative statistic for the moderation effect of one variable of the effect of another variable: the Pearson chi-square.

First, let's have a look at the overall survival rate. In total there were \Sexpr{sum(as.matrix(apply(Titanic, c(2, 3,4), sum)[,2,])[,2])} people that survived and \Sexpr{sum(as.matrix(apply(Titanic, c(2, 3,4), sum)[,2,])[,1])} people that did not survive. Table \ref{tab:gen_25} shows these column totals.

<<gen_25, fig.height=4, echo=FALSE, fig.align='center', message=F, warning=F, results="asis">>=

A = apply(Titanic, c(2, 3,4), sum)[,2,]
B= data.frame(No = sum(as.matrix(apply(Titanic, c(2, 3, 4), sum)[ , 2, ])[, 1]), 
              Yes = sum(as.matrix(apply(Titanic, c(2, 3, 4), sum)[ , 2, ])[, 2]))

data<- rbind(A, B)

row.names(data) <- c("Male", "Female", "Total")

data %>% 
  xtable(caption = "Counts of adult survivors and non-survivors on the Titanic.", 
         label = "tab:gen_25",
         digits = c(0)) %>%
  print(include.rownames = T, caption.placement = "top", hline.after = c(0, 2, 3))
@

Looking at these total numbers of survivors and non-survivors, we can calculate the proportion of survivors overall (the survival rate) as $\Sexpr{sum(as.matrix(apply(Titanic, c(2, 3,4), sum)[,2,])[,2])}/(\Sexpr{sum(as.matrix(apply(Titanic, c(2, 3,4), sum)[,2,])[,2])}+\Sexpr{sum(as.matrix(apply(Titanic, c(2, 3,4), sum)[,2,])[,1])})= \Sexpr{round(sum(as.matrix(apply(Titanic, c(2, 3,4), sum)[,2,])[,2])/(sum(as.matrix(apply(Titanic, c(2, 3,4), sum)[,2,])[,2])+sum(as.matrix(apply(Titanic, c(2, 3,4), sum)[,2,])[,1])),2)}$. Table \ref{tab:gen_26} shows the totals for men and women, as well as the overall total number of adults.



<<gen_26, fig.height=4, echo=FALSE, fig.align='center', message=F, warning=F, results="asis">>=

data <- data %>%
  mutate(Total = c(
    sum(apply(Titanic, c(2, 3, 4), sum)[1, 2, ]),
    sum(apply(Titanic, c(2, 3, 4), sum)[2, 2, ]),
    sum(Titanic[, , 2, ])
  ))

row.names(data) <- c("Male", "Female", "Total")


data %>%
  xtable(
    caption = "Counts of adult survivors and non-survivors on the Titanic.",
    label = "tab:gen_26",
    digits = c(0)
  ) %>%
  print(
    include.rownames = T,
    caption.placement = "top",
    hline.after = c(0, 2, 3)
  )
@


Suppose we only know that of the \Sexpr{sum(Titanic[,,2,])} people, 1667 were men, and of all people, 654 survived. Then suppose we pick a random person from these \Sexpr{sum(Titanic[,,2,])} people. What is the probability that we get a male person that survived, \textit{given that sex and survival have nothing to do with each other}?

Well, from probability theory we know that if two events $A$ and $B$ are independent (unrelated), the probability of observing $A$ and $B$ at the same time, is equal to the product of the probability of event $A$ and the probability of event $B$.

\begin{equation}
Prob(A \& B) = Prob(A) \times Prob(B)
\end{equation}

If sex and survival are independent from each other, then the probability of observing a male survivor is equal to the probability of seeing a male times the probability of seeing a survivor. The probability for survival is 0.31, as we saw earlier, and the probability of seeing a male is equal to the proportion of males in the data, which is $1667/\Sexpr{sum(Titanic[,,2,])} =\Sexpr{round(1667/sum(Titanic[,,2,]),2) }$. Therefore, the probability of seeing a male survivor is $\Sexpr{round(1667/sum(Titanic[,,2,]),2)} \times 0.31 =\Sexpr{round(0.79*0.31,2)} $. The expected number of male survivors is then that probability times the total number of people, $\Sexpr{round(0.79*0.31,2)} \times \Sexpr{sum(Titanic[,,2,])}= \Sexpr{sum(Titanic[,,2,])*round(0.79*0.31,2)}$. Similarly we can calculate the expected number of non-surviving males, the number of surviving females, and the number of non-surviving females.


These numbers, after rounding, are displayed in Table \ref{tab:gen_27}.


<<gen_27, fig.height=4, echo=FALSE, fig.align='center', message=F, warning=F, results="asis">>=

expected <- data.frame(
  No = c((1 - 0.31) * 0.8 * 2092, (1 - 0.31) * 0.2 * 2092),
  Yes = c(0.31 * 0.8 * 2092, 0.31 * 0.2 * 2092)
)

row.names(expected) <- c("Male", "Female")

expected %>%
  xtable(
    caption = "Expected numbers of adult survivors and non-survivors on the Titanic.",
    label = "tab:gen_27",
    digits = c(0)
  ) %>%
  print(include.rownames = T, caption.placement = "top")
@


The expected numbers in Table \ref{tab:gen_27} are quite different from the observed numbers in Table \ref{tab:gen_24}. Are the differences large enough to think that the two events of being male and being a survivor are NOT independent? If the expected numbers on the assumption of independence are different enough from the observed numbers, then we can reject the null-hypothesis that being male and being a survivor have nothing to do with each other. To measure the difference between expected and observed counts, we need a test statistic. Here we use Pearson's chi-square statistic. It involves calculating the difference between the observed and expected numbers in the respective cells, and standardise them by the expected number. Here's how it works:

For each cell, we take the predicted count and subtract it from the observed count. For instance, for the male survivors, we expected 519 but observed 338. The difference is therefore $338-519= \Sexpr{-(519-338)}$. Then we take the square of this difference, $ 181^2=32761$. Then we divide this number by the expected number, and then we get $32761/519=\Sexpr{32761/519}$. We do exactly the same thing for the male non-survivors, the female survivors and the female non-survivors. Then we add these 4 numbers, and then we have the Pearson chi-square statistic. In formula form:

\begin{equation}
X^2 = \Sigma_i    \frac{(O_i-E_i)^2}{E_i}
\end{equation}


So for male survivors we get


\begin{equation}
 \frac{(338-519)^2}{519} =\Sexpr{(338-519)^2/519}
\end{equation}




For male non-survivors we get


\begin{equation}
 \frac{(1329-1155)^2}{1155} =\Sexpr{(1329-1155)^2/1155}
\end{equation}

For female survivors we get


\begin{equation}
 \frac{(316-130)^2}{130} =\Sexpr{(316-130)^2/130}
\end{equation}

and for female non-survivors we get

\begin{equation}
 \frac{(109-289)^2}{289} =\Sexpr{(109-289)^2/289}
\end{equation}




If we add these 4 numbers we have the chi-square statistic: $X^2= \Sexpr{round(sum((338-519)^2/519+(1329-1155)^2/1155+(316-130)^2/130+ (109-289)^2/289),2)}$. Note that we only use the rounded expected numbers. Better would be to use the non-rounded numbers. Had we used the non-rounded expected numbers, we would have gotten $X^2 = \Sexpr{ round(chisq.test(  as.matrix(apply(Titanic, c(2, 3,4), sum)[,2,]), correct=F )$statistic,2) }$.

Remember that in the Poissson regression earlier, the $z$-statistic for the sex by survived interaction effect was \Sexpr{summary(outtitanic3)$coef[4,3]}, see the earlier output. It tests exactly the same null-hypothesis as the Pearson chi-square: that of independence, or in other words, that the numbers can be explained by only two main effects, sex and survival.

If the data set is large enough and the numbers are not too close to 0, the same conclusions will be drawn, whether from a $z$-statistic for an interaction effect in a Poisson regression model, or from a cross-tabulation and computing a Pearson chi-square. The advantage of the Poisson regression approach is that you can do much more with them, for instance more than two predictors, and that you make it more explicit that when computing the statistic, you take into account the main effects of the variables. You do that also for the Pearson chi-square, but it is less obvious: we did that by first calculating the probability of survival and second calculating the proportion of males.



We can report the results from the cross-tabulation as follows:


\begin{quotation}
"We tested the null-hypothesis that the variables sex and survival are not related with a Pearson chi-square test. The results showed that the null-hypothesis could be rejected, $\chi^2(1) = \Sexpr{ round(chisq.test(  as.matrix(apply(Titanic, c(2, 3,4), sum)[,2,]), correct=F )$statistic,2) }, p < .001$."
\end{quotation}

The degrees of freedom for Pearson's chi-square are determined by the number of categories for variable 1, $K_1$, and the number of categories for variable 2, $K_2$: $\textrm{df} = (K_1 - 1)(K_2 - 1)$. The $p$-value can be found by looking up how much of the chi-square distribution with 1 degree of freedom is more than 460.87:

<<>>=
pchisq(460.87, df = 1, lower.tail = F)
@


\section{Poisson regression or logistic regression?}


In the previous section we analysed the relationship between the variable \texttt{sex} of the person on board the Titanic, and the variable \texttt{survived}: whether or not a person survived the shipwreck. We found a relationship between these two variables by studying the cross-tabulation of the counts, and testing that relationship using a Pearson chi-square statistic. In the section before that, we saw that this relationship could also be tested by applying a Poisson regression model and looking at the sex by survived interaction effect. These methods are equivalent.

There is yet a third way to analyse the sex and survived variables. Remember that in the previous chapter we discussed logistic regression. In logistic regression, a dichotomous variable (a variable with only two values, say 0 and 1) is the dependent variable, with one or more numeric or categorical independent variables. Both \texttt{sex} and \texttt{survived} are dichotomous variables: male and female, and survived yes or survived no. In principle therefore, we could do a logistic regression: for example predicting whether a person is a male or female, on the basis of whether they survived or not, or the other way around, predicting whether people survive or not, on the basis of whether a person is a woman or a man.


What variable is used here as your dependent variable, depends on your research question. If your question is whether females are more likely to survive than men, perhaps because of their body fat composition, or perhaps because of male chivalry, then the most logical choice is to take survival as the dependent variable and sex as the independent variable.

The code for a logistic regression then looks like



<<echo = T, eval = F>>=
model <- data %>% 
 glm(survived ~ sex, family = binomial, data = .)
@





Note however that the data can be in the wrong format. For the Poisson regression, the data were in the form of what we see in Table \ref{tab:gen_21}. However, for a logistic regression, we need the data in long format, like in Table \ref{tab:gen_28}. For every person on-board the ship, we have to know their sex and their survival status.

<<gen_28, fig.height=4, echo=F, fig.align='center', message=F, warning=F, results="asis">>=
data <- as.matrix(apply(Titanic, c(2, 3, 4), sum)[, 2, ])

data <- as.vector(data)

data <- matrix(data, 4, 1)


sex <- rep(c("Male", "Female"), 2)
survived <- rep(c(0, 1), each = 2)
data <- data.frame(sex, survived, count = data)

data.new <- c(
  rep(c("Male", "0"), data[1, 3]),
  rep(c("Female", "0"), data[2, 3]),
  rep(c("Male", "1"), data[3, 3]),
  rep(c("Female", "1"), data[4, 3])
)

data.new <- t(matrix(data.new, 2, sum(data[, 3])))

ID <- 1:sum(data[, 3])

data.logistic <- data.frame(ID,
                            sex = data.new[, 1],
                            survived = data.new[, 2])

set.seed(234)
data.logistic[sample(1000:2000, size = 300, replace = F), ] %>%
  head(5) %>%
  arrange(ID) %>%
  xtable(caption = "Titanic data in long format, appropriate for logistic regression.",
         label = "tab:gen_28",
         digits = c(0, 0, 0, 0)) %>%
  print(include.rownames = F, caption.placement = "top")
@

<<gen_29, fig.height=4, echo=F, eval = T, fig.align='center', message=F, warning=F>>=
# showing the data
# data.logistic %>% glimpse()
# turn survive from character to numeric:
data.logistic <- data.logistic %>%
  mutate(survived = as.numeric(survived))

model <- data.logistic %>%
 glm(survived ~ sex, family = binomial, data = .)
model %>% tidy()
@




When we run the logistic regression, we see in the output that sex is a significant predictor of the survival status, $b = -2.43, z = -19.2, p<.001$. The logodds for a male surviving the shipwreck is $1.06 - 2.43 = -1.37$, and the logodds ratio for a female surviving the shipwreck is $1.06$. These logodds correspond to probabilities of $0.20$ and $0.74$, respectively. Thus, women were much more likely to survive than men.

However, suppose you are the relative of a passenger on board a ship that shipwrecks. After two days, there is news that a person was found. The only thing known about the person is that he or she is alive. Your relative is your niece, so you'd like to know on the basis of the information that the person that was found is alive, what is the probability that that person is a woman, because then it could be your beloved niece! You could therefore run a logistic regression on the Titanic data to see to what extent the survival of a person predicts the sex of that person. First we have to turn the dependent variable into a dummy variable. Since we would be happy if a survivor is a female, we code Female as 1 and Male as 0. Next, we do the logistic regression where we switch the dependent and independent variables.

<<echo = F, eval = T>>=
data.logistic %>% glimpse()

# turn sex variable from character to numeric:
data.logistic <- data.logistic %>% 
  mutate(sex = as.integer((sex == "Female")))
# data.logistic %>% glimpse()
@



<<echo = F,eval = T>>=
model2 <- data.logistic %>% 
  glm(sex ~ survived, family = binomial, data = .)
model2 %>% tidy()
@





From this output we conclude that survival is a significant predictor of sex, $b=2.43, z=19.2, p<.001$. The logodds ratio for a surviving person to be a woman is $-2.50 + 2.43= -0.07$, and the logodds ratio for a non-surviving person to be a woman is $-2.50$. These logodds ratios correspond to probabilities of $0.48$ and $0.08$, respectively. Thus, if you know that there is a person that survived the Titanic, it is about a 50-50\% chance that it is a woman. If you think this is counter-intuitive, remember that even though a large proportion of the women survived the Titanic, there were many more men on-board than women.
\\
\\
In summary, if you have count data, and both of the variables are dichotomous, you have the choice whether to use a Poisson regression model for counts or a logistic regression with one of the dichotomous variables as dependent variables. The choice depends on the research question: if your question involves \textit{prediction} of a dichotomous variable, logistic regression is the logical choice. If you have a theory that one or more independent variables \textit{explain} one other variable, logistic regression is the logical choice. If however your theory does not involve a natural direction or prediction of one variable, and you are simply interested in associations among variables, then Poisson regression of counts is the obvious choice.




