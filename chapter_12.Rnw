

\chapter{Generalized linear models part I: logistic regression}

\section{Introduction}
In previous chapters we were introduced to the linear model, with its basic form


\begin{eqnarray}
y = b_0 + b_1 X_1 + \dots + b_n X_n + e \\
e \sim N(0, \sigma_e^2)
\end{eqnarray}

Two basic assumptions of this model are the linearity in the parameters, and the normally distributed residual $e$. Linearity in the parameters means that the effects of intercept and the independent variables $X_1, X_2, \dots X_n$ are additive: the assumption is that you can sum these effects to come to a predicted value for $y$. So that is also true when we include interaction effects to account for moderation effects,

\begin{eqnarray}
y = b_0 + b_1 X_1 +  b_2 X_2 + b_3 X_1 X_2 + e \\
e \sim N(0, \sigma_e^2)
\end{eqnarray}


or when we use a quadratic term to account for other types of nonlinearity in the data:


\begin{eqnarray}
y = b_0 + b_1 X_1 +  b_2 X_1 X_1 + e \\
e \sim N(0, \sigma_e^2)
\end{eqnarray}

In all these models, the assumption is that the effects of the parameters can be added to one another.

The other major assumption of linear (mixed) models is the normal distribution of the residuals. As we have seen in for instance the previous chapter, sometimes the residuals are not normally distributed. Remember that with a normal distribution $N(0,\sigma^2)$, in principle all values between $-\infty$ and $+\infty$ are possible, but they tend to concentrate around the value of 0, in the shape of the bell-curve. Figure \ref{fig:gen_1} shows the normal distribution $N(0,\sigma^2=4)$: it is centered around 0 and has variance 4. Note that the inflection point, that is the point where the decrease in density tends to decelerate, is exactly at the values -2 and +2. These are equal to the square root of the variance, which is the standard deviation, $+\sigma$ and $-\sigma$.


<<gen_1, fig.height=4, echo=FALSE, fig.align='center', fig.cap='Density function of the normal distribution, with mean 0 and variance 4 (standard deviation 2). Inflection points are positioned at residual values of minus 1 standard deviation and plus 1 standard deviation.'>>=
residual <- seq (-6,6,0.1)
density <- dnorm(residual, 0, 2)
data <- data.frame(residual, density)
data %>% ggplot(aes(x=residual, y=density)) + geom_line() +
        geom_vline(xintercept = -2, col='blue') +geom_vline(xintercept = 2, col='blue') +
        geom_label(  aes(x=5, y=dnorm(2,0,2)), col='black', label='Inflection point', show.legend = F) +
        geom_segment(aes(x=3.5, xend=2.1, y=dnorm(2,0,2), yend=dnorm(2,0,2)), size = 1,
               arrow = arrow(length = unit(0.1, "cm")))+
        geom_label(  aes(x=-5, y=dnorm(2,0,2)), col='black', label='Inflection point', show.legend = F) +
        geom_segment(aes(x=-3.5, xend=-2.1, y=dnorm(2,0,2), yend=dnorm(2,0,2)), size = 1,
               arrow = arrow(length = unit(0.1, "cm"))) + scale_x_continuous(breaks=seq (-6,6,1))
        @
% 
A normal distribution is suitable for continuous data: for example a variable that can take all possible values between -1 and 0. For many variables this is not true. Think for example of temperature measures: if the thermometer gives degrees centigrade with a precision of only 1 decimal, we can never have values of say 10.07 or -56.789. Our data will in fact be \textit{discrete}, showing rounded values like 10.1, 10.2, 10.3, but no values in between.

Nevertheless, the normal distribution can still be used in many such cases. Take for instance a data set where the temperature in Amsterdam in summer was predicted on the basis of a linear model. Fig \ref{fig:gen_2} shows the distribution of the residuals of that model:
% 
<<gen_2, fig.height=4, echo=FALSE, fig.align='center', message=F, fig.cap='Even if residuals are really discrete, the normal distribution can be a good approximation of their distribution.'>>=
set.seed(1234)
residual <- rnorm(1000, 0, 3) %>%  round(1)
density <- dnorm(residual, 0, 3)
data <- data.frame(residual, density)
data %>% ggplot(aes(x=residual, y=density)) + geom_histogram(aes(y=..density..))  + geom_line(aes(y=density))
@


The temperature measures were discrete with a precicsion of one tenth of a degree centigrade, but the distribution seems well approximated by a normal curve.


But let's look at an example where the discreteness is more prominent. In Figure \ref{fig:gen_3} we see the residuals of an analysis of exam results. Students had to do an asssignment that had to meet 4 criteria: 1) originality, 2) language, 3) structure, and 4) literature review. Each criterion was scored as either fulfilled (1) or not fulfilled (0). The score for the assignment was given on the basis of \textit{the number of criteria} that were met, so the scores could be 0, 1, 2, 3 or 4. The score was predicted on the basis of the average exam score on previous assignments using a linear model.


<<gen_3, fig.height=4, echo=FALSE, fig.align='center', message=F, fig.cap='Count data example where the normal distribution is not a good approximation of the distribution of the residuals.'>>=
set.seed(1234)
score <- rnorm(100, 1, 3) %>%  round(0)
score [score>4] <- 4
score [score<0] <- 0
x = rnorm(100, 0, 1)
data=data.frame(score, x)
residual <-  lm(score ~ x, data=data   )$res
density <- dnorm(residual, mean(residual), sd(residual))
data <- data.frame(residual, density)
data %>% ggplot(aes(x=residual, y=density)) + geom_histogram(aes(y=..density..))  + geom_line(aes(y=density))
@


Figure \ref{fig:gen_3} shows that the residuals are very discrete, and that the continous normal distribution is a very bad approximation of the histogram. We often see this phenomenon when our data consists of \textit{counts} with a limited maximum number.

An even more extreme case we observe when our dependent variable consists of whether or not students passed the assignment: only those assignments that fulfilled all 4 criteria are regarded as sufficient. If we score all students with a sufficient assignment as passed (1) and all students with an insufficient assignment as failed (0) and we predict this again by the average exam score on previous assignments using a linear model, we get the residuals displayed in Figure \ref{fig:gen_4}.


<<gen_4, fig.height=4, echo=FALSE, fig.align='center', message=F, fig.cap='Dichotomous data example where the normal distribution is not a good approximation of the distribution of the residuals.'>>=
set.seed(1234)
score <- rnorm(100, 1, 3) %>%  round(0)
score [score<=4] <- 0
score [score>3] <- 1
x = rnorm(100, 0, 1)
data=data.frame(score, x)
residual <-  lm(score ~ x, data=data   )$res
density <- dnorm(residual, mean(residual), sd(residual))
data <- data.frame(residual, density)
data %>% ggplot(aes(x=residual, y=density)) + geom_histogram(aes(y=..density..))  + geom_line(aes(y=density))
@
% 
% 
Here it is definitely evident that a normal approximation of the residuals will not do. When the dependent variable has only 2 possible values, a linear model will never work because the residuals can never have a distribution that is even remotely looking normal.

In the coming two chapters we will discuss how generalized linear models can be used to analyze data sets where the assumption of normally distributed residuals is not tenable. First we discuss the case where the dependent variable has only 2 possible values (dichotomous dependent variables like yes/no or pass/fail, heads/tails, 1/0). In the next chapter, we will discuss the case where the dependent variable consists of counts ($1, 2, 3, 4, \dots$).


\section{Logistic regression}

Imagine that we analyze results on an exam for third grade children. These children are usually either 6 or 7 years old, dependending on what month they were born in. The exam is on February 1st. A researcher wants to know whether the age of the child can explain why some children pass the test and others fail. She computes the age of the child in months. Each child that passes the exam gets a score 1 and all the others get a score 0. Figure \ref{fig:gen_5} plots the data.



She wants to use the following linear model:

\begin{eqnarray}
score = b_0 + b_1 age  + e \\
e \sim N(0, \sigma_e^2)
\end{eqnarray}

Figure \ref{fig:gen_6} shows the estimated regression line and Figure \ref{fig:gen_7} shows the distribution of the residuals as a function of age.


<<gen_5, fig.height=4, echo=FALSE, fig.align='center', message=F, fig.cap='Data example: Exam outcome (score) as a function of age, where 1 means pass and 0 means fail.'>>=
set.seed(12345)
age = rnorm(100, 78, 3) %>% round(1)
logoddsscore <- 0.51*(age-78) %>%  round(0)
score <- exp(logoddsscore)/ (1+logoddsscore)
score [score<=4] <- 0
score [score>3] <- 1
data.exam <- data.frame(score, age)
data.exam %>% ggplot(aes(x=age, y=score)) + geom_point() + xlab("age in months")
@


<<gen_6, fig.height=4, echo=FALSE, fig.align='center', message=F, , fig.cap='Example exam data with a linear regression line.'>>=
data.exam %>% ggplot(aes(x=age, y=score)) + geom_point() + xlab("age in months") + geom_smooth(formula=y~x, method="lm", se=F)
@
% 
% 
<<gen_7, fig.height=4, echo=FALSE, fig.align='center', message=F, fig.cap='Residuals as a function of age, after a linear regression analysis of the exam data.'>>=
residual <-  lm(score ~ age, data=data.exam   )$res
data.exam <- data.frame(residual, age, score)
data.exam %>% ggplot(aes(x=age, y=residual)) + geom_point() + xlab("age in months")
outglm <- glm(score~ age, data=data.exam)
@

Clearly a linear model is not appropriate. Here, the assumption that the dependent variable, score in this case, is scattered randomly around the predicted value with a normal distribution is not reasonable. The main problem is that the dependent variable score can only have 2 values: 0 and 1. When we have a dependent variable that is categorical, so not continuous, we generally use \textit{logistic regression}. In this chapter we cover the case when the dependent variable takes binary values, like 0 and 1.


\subsection{Bernoulli distribution}

Rather than using a normal distribution, we could try a Bernoulli distributiuon. The Bernoulli distribution is the distribution of a coin flip. For example, if the probability of heads is 0.1, we can expect that if we flip the coin, on average we expect to see $0.1$ times heads and 0.9 times tails. Our best bet then is that the outcome is tails. However, if we actually flip the coin, we might see heads anyway. There is some randomness to be expected. Let $y$ be the outcome of a coin flip: heads or tails. If we have a Bernoulli distribution for variable $y$ with probability $p$ for heads, we \textit{expect} to see heads $p$ times, but we actually \textit{observe} heads or tails.

\begin{equation}
y \sim Bern(n, p)
\end{equation}

The same is true for the normal distribution in the linear model case: we \textit{expect} that the observed value of $y$ is exactly equal to its predicted value ($b_0 + b_1 X$), but we always \textit{observe} that it is different.

\begin{equation}
y \sim N(\mu= b_0 + b_1 X, \sigma^2_e)
\end{equation}

In our example, the pass rate could also be conceived as the outcome of a coin flip: pass instead of heads and fail instead of tails. So would it be an idea to predict the \textit{probability} of success on the basis of age? And then for every predicted probability, we allow for the fact that actually the observed success can differ. Our linear model could then look like this:


\begin{eqnarray}
p_i = b_0 + b_1 age_i \\
score_i \sim Bern(p_i)
\end{eqnarray}

So for each child $i$, we predict the probability of success, $p_i$, on the basis of her/his age. Next, the randomness in the data comes from the fact that a probability is only a probability, so that the observed success of a child $score_i$, is like a coin toss with probability of $p_i$ for success.

For example, suppose that we have a child with an age of 80 months, and we have $b_0=-3.8$ and $b_1=0.05$. Then the predicted probability $p_i$ is equal to $-3.8 + 0.05 \times 80 = 0.20$. The best bet for such a child would be that it fails the exam. But 0.20 is only a probability, so by chance the child could pass the exam. This model also means that if we would have 100 children of age 80 months, we would \textit{expect} that 20 of these children would pass the test and 80 would fail.  But we can't make predictions for one individual alone: we don't know which child exactly will pass and which child won't. Note that this is similar to the normally distributed residual in the linear model: in the linear model we expect a child to have a certain value for $y$, but we know that there will be a deviation from this predicted value: the residual. For a whole group of children with the same predicted value for $y$, we know that the whole group will show residuals that have a normal distribution. But we're not sure what the residual will be for each individual child.

Unfortunately, this model for probabilities is not very helpful. If we use a linear model for the probability, this means that we can predict probability values of less than 0 and more than 1, and this is not possible for probabilities. If we use the above values of $b_0=-3.8$ and $b_1=0.05$, we predict a probability of -.3 for a child of 70 months and a probability of 1.2 for a child of 100 months. Those values are meaningless!



\subsection{Odds and logodds}
Instead of predicting probabilities, we could predict \textit{odds}. The nice property of odds is that they can have very large values, much larger than 1.

What are odds again? Odds are a different way of talking about probability. Suppose the probability of winning the lottery is 1\%. Then the probability of loosing is $99\%$. This is equal to saying that the odds of winning against loosing are 1 to 99, or $1:99$, because the probability of success is 99 times smaller than the probability of loosing.

As another example, suppose the probability of being alive tomorrow is equal to 0.9999. Then the probability of not being alive tomorrow is $1-0.9999=0.0001$. Then the probability of being alive tomorrow is $0.9999/0.0001=9999$ times larger than the the probability of not being alive. Therefore the odds of being alive tomorrow against being dead is 9999 to 1 (9999:1).

If we have a slightly biased coin, the probability of heads might be 0.6. The probability of tails is then 0.4. Then the probability of heads is then 1.5 times larger than the probability of heads (0.6/0.4=1.5). So the odds of heads against tails is then 1.5 to 1. For the sake of clarity, odds are often multiplied by a constant to get integers, so we can also say the odds of heads aganst tails are 3 to 2. Similarly, if the probablity of heads were 0.61, the odds of heads against tails would be 0.61 to 0.39, which can be modified into 61 to 39.

Now that we know how to go from probability statements to statements about odds, how do we go from odds to probability? If someone says the odds of heads against tails is 10 to 1, this means that for every 10 heads, there will be 1 tails. In other words, if there were 11 coin tosses, 10 would be heads and 1 would be tails. We can therefore transform odds back to probabilities by noting that 10 out of 11 toin tosses is heads, so $10/11 = 0.91$, and 1 out of 11 is tails, so $1/11=0.09$.

If someones says the odds of winning a gold medal at the Olympics is a thousand to one (1000:1), this means that if there were $1000+1=1001$ opportunities, there would be a gold medal in 1000 cases and failure in only one. This corresponds to a probability of 1000/1001 for winning and 1/1001 for failure.

As a last example, if at the horse races, the odds of Bruno winning against Sacha are four to five (4:5), this means that for every 4 winnings by Bruno, there would be 5 winnings by Sacha. So out of a total of 9 winnings, 4 will be by Bruno and 5 will be by Sacha. The probability of Bruno outrunning Sacha is then $4/9=0.44$.
\\
\\
If we would summarize the odds by doing the division, we have just one number. For example, if the odds are 4 to 5 (4:5), the odds are $4/5=0.8$, and if the odds are a thousand to one (1000:1), then we can also say the odds are 1000. Odds, unlike probabilities, can have values that are larger than 1. 

However, note that odds can never be negative: a very small odds is one to a thousand (1:1000). This can be summarized as an odds of 0.000999001, but that is still larger than 0. In summary: probabilties range from 0 to 1, and odds from 0 to infinity.

Because odds can never be negative, mathematicians have proposed to use the \textit{natural logarithm}\footnote{The natural logarithm of a number is its logarithm to the base of the constant $e$, where $e$ is approximately equal to 2.7. The natural logarithm of $x$ is generally written as
$ln x$ or $log^e x$. The natural logarithm of $x$ is the power to which $e$ needs to be raised to equal $x$. For example, $ln(2)$ is 0.69, because $e^{0.69} = 2$, and $ln(0.2)=-1.6$ because $e^{-1.6}=0.2$. The natural logarithm of $e$ itself, $ln(e)$, is 1, because $e^1 = e$, while the natural logarithm of 1, $ln(1)$, is 0, since $e^0 = 1$.} of the odds  as the preferred transformation of probabilities. For example, suppose we have a probability of heads of 0.42. This can be transformed into an odds by noting that in 100 coin tosses, we would expect 42 times heads and 58 times tails. So the odds are 42:58, which is equal to $\frac{42}{58}=\Sexpr{42/58}$. The \textit{natural} logarithm of \Sexpr{42/58} equals \Sexpr{log(42/58)} (use the $ln$ button on your calculator!). If we have a value between 0 and 1 and we take the logarithm of that value, we always get a value smaller than 0. In short: a probability is never negative, but the corresponding logarithm of the odds can be negative.




<<gen_8, fig.height=4, echo=FALSE, fig.align='center', message=F, fig.cap='The relationship between a probability and the natural logarithm of the corresponding odds.'>>=
probability <-  seq(0.001,0.999, 0.001)
logodds <- log( probability/(1-probability)  )
data <- data.frame(probability, logodds)
data %>% ggplot(aes(x=probability, y=logodds)) + geom_line() + ylab("natural logarithm of the odds")
@

Figure \ref{fig:gen_8} shows the relationship between a probability (with values between 0 and 1) and the natural logarithm of the corresponding odds (the \textit{logodds}). The result is a mirrored S-shaped curve on its side. For large probabilities close to one, the equivalent logodds becomes infinitely positive, and for very small probabilities close to zero, the equivalent logodds becomes infinitely negative. A logodds of 0 is equal to a probability of 0.5. If a logodds is larger than 0, it means the probability is larger than 0.5, and if a logodds is smaller than 0 (negative), the probability is smaller than 0.5.
\\
\\
In summary, if we use a linear model to predict probabilities, we have the problem of predicted probabilities smaller than 0 and larger than 1 that are meaningless. If we use a linear model to predict odds we have the problem of predicted odds smaller than 0 that are meaningless: they are impossible! If on the other hand we use a linear model to predict \textit{the natural logarithm of odds} (logodds), we have no problem whatsoever. We therefore propose to use a linear model to predict \textit{logodds}: the natural logarithm of the odds that correspond to a particular probability.
\\
\\
Returning back to our example of the children passing the exam, suppose we have the following linear equation for the relationship between age and the logarithm of the odds of passing the exam


\begin{eqnarray}
logodds=\Sexpr{round(outglm$coef[1],2)} + \Sexpr{round(outglm$coef[2],2)} age, \nonumber
\end{eqnarray}


This equation predicts that a child aged 70 months has a logodds of $\Sexpr{round(outglm$coef[1],2)} + \Sexpr{round(outglm$coef[2],2)} \times 70 =\Sexpr{round(predict.glm(outglm, newdata=data.frame(age=70)),2)}$. In order to transform that logodds back to a probability, we first have to take the exponential of the logodds\footnote{If we know $ln(x)=60$, we have to infer that $x$ equals $e^{60}$, because $ln(e^{60})=60$ by definition of the natural logarithm, see previous footnote. Therefore, if we know that $ln(x)=c$, we know that $x$ equals $e^c$. The exponent of $c$, $e^c$, is often written as $exp(c)$. So if we know that the logarithm of the odds equals $c$, $logodds=ln(oddsratio)=c$, then the odds is equal to $exp(c)$.} to get the odds:


\begin{eqnarray}
odds = exp(logodds)= e^{logodds}=e^{\Sexpr{round(predict.glm(outglm, newdata=data.frame(age=70)),2)}}=\Sexpr{round(exp(predict.glm(outglm, newdata=data.frame(age=70))),2)} \nonumber
\end{eqnarray}

An odds of \Sexpr{round(exp(predict.glm(outglm, newdata=data.frame(age=70))),2)} means that the odds of passing the exam is \Sexpr{round(exp(predict.glm(outglm, newdata=data.frame(age=70))),2)} to 1 (\Sexpr{round(exp(predict.glm(outglm, newdata=data.frame(age=70))),2)}:1). So out of $1 + \Sexpr{round(exp(predict.glm(outglm, newdata=data.frame(age=70))),2)}= \Sexpr{round(1+exp(predict.glm(outglm, newdata=data.frame(age=70))),2)}$ times, we expect \Sexpr{round(exp(predict.glm(outglm, newdata=data.frame(age=70))),2)} successes and 1 failure. The probability of success is therefore $\frac{\Sexpr{round(exp(predict.glm(outglm, newdata=data.frame(age=70))),2)}}{1+\Sexpr{round(exp(predict.glm(outglm, newdata=data.frame(age=70))),2)}} = \Sexpr{round(exp(predict.glm(outglm, newdata=data.frame(age=70)))/ (1+exp(predict.glm(outglm, newdata=data.frame(age=70)))),2)}$. Thus, based on this equation, the expected probability of passing the exam for a child of 70 months equals 0.42.

If you find that easier, you can also memorize the following formula for the relationship between a logodds of $x$ and the corresponding probability:


\begin{equation}
\label{eq:logistic1}
p_x = \frac{exp(x)}{1+exp(x)}
\end{equation}

Thus, if you have a logodds $x$ of $\Sexpr{round(predict.glm(outglm, newdata=data.frame(age=70)),2)}$, the odds equals $exp(-0.34)=\Sexpr{round(exp(-0.34),2)}$, and
the corresponding probability is $\frac{0.71}{1+0.71} = \Sexpr{round(exp(predict.glm(outglm, newdata=data.frame(age=70)))/ (1+exp(predict.glm(outglm, newdata=data.frame(age=70)))),2)}$.


\subsection{Exercises}

From probability to logodds:

Given:
In the Netherlands, 51\% of the inhabitants is female.
\begin{enumerate}

\item
If we randomly pick someone from this Dutch population, what is the probability that that that person is female?


\item
If we randomly pick someone from this Dutch population, what are the odds that that that person is female over being male? ( : )

\item
If we randomly pick someone from this Dutch population, what are the odds that that that person is male over being female? ( : )

\item
What is the odds of randomly picking an inhabitant that is female, expressed as one number?

\item
What is the odds of randomly picking an inhabitant that is male, expressed as one number?


\item
What is the logodds of randomly picking an inhabitant that is female?

\item
What is the logodds of randomly picking an inhabitant that is male?


\end{enumerate}

Answers:

\begin{enumerate}

\item
0.51


\item
51 to 49 (51:49).

\item
49:51.

\item
51/49=1.04

\item
49/51=0.96


\item
ln(51/49)= ln(1.04)=0.04

\item
ln(49/51)= ln(0.96)=-0.04


\end{enumerate}

From logoddss to probabilities:

Given:
In the Netherlands, 51\% of the inhabitants are female. Females tend to get older than males, so if we predict sex by age, we should expect a higher probability of a female for older ages. Suppose we have the following linear model for the relationship between age (in years) and the logodds of being female:


\begin{eqnarray}
logodds_{female}=-0.01 + 0.01 \times age, \nonumber
\end{eqnarray}

\begin{enumerate}

\item
What is the predicted logodds of being female for a person of age 20?

\item
What is the predicted logodds of being female for a person of age 90?

\item
What is the predicted odds of being female for a person of age 20?

\item
What is the predicted odds of being female for a person of age 90?

\item
What are the predicted odds of being female for a person of age 20?

\item
What are the predicted odds of being female against being male for a person of age 90?

\item
What is the predicted probability of being female against being male for a person of age 20?

\item
What is the predicted probability of being female for a person of age 90?

\item
What is the predicted probability of being MALE for a person of age 90?


\end{enumerate}

Answers:

\begin{enumerate}

\item
$-0.01 + 0.01 \times 20 = 0.19$

\item
$-0.01 + 0.01 \times 90 = 0.89$

\item
$exp(0.19)=1.21$

\item
$exp(0.89)=2.44$

\item
1.21 to 1, or 1.21:1

\item
2.44 to 1, or 2.44:1

\item
1.21/ (1.21 + 1)= 0.55

\item
2.44 / (2.44 + 1)= 0.71

\item
1 - 0.71 = 0.29


\end{enumerate}


A big data analyst constructs a model that predicts whether an account on Twitter belongs to either a real person or organisation, or to a bot.

\begin{enumerate}

\item
For one account, a user of this model finds an logodds of 4.5 that the account belongs to a bot. What is the corresponding probability that the twitter account belongs to a bot? Give the calculation.

\item
For a short tweet with only a hyperlink, the probability that it comes from a bot is only 10\%. What is the logodds that corresponds to this probability? Give the calculation.


\end{enumerate}



Answers:
\begin{enumerate}

\item The logodds is 4.5, so the oddsratio is exp(4.5)=90.0.
The odds of being a bot is then 90:1.
The probability of being a bot is 90/ (90+1)= 0.99

\item
Out of 100 tweets with only a hyperlink, 10 are by bots and 90 are by real persons or organisations. So the odds of coming from a bot are 10:90. The odds is therefore 10/90 = 0.11. When we take the natural logarithm of this odds, we get the logodd: ln(0.11) = -2.21.

\end{enumerate}



\subsection{Logistic link function}

In previous pages we have seen that logodds have the nice property of having meaningful values between $-\infty$ and $+\infty$. This makes them suitable for linear models. In essence, our linear model for our exam data in children might then look like this:


\begin{eqnarray}
logodds_{pass}= b_0 + b_1 age\\
y \sim Bern(p_{pass})
\end{eqnarray}

Note that we can write the odds as $p/(1-p)$, $p$ is a probability (or a proportion). So the logodds that corresponds to the probability of passing the exam, $p_{pass}$, can be written as $ln\frac{p_{pass}}{1- p_{pass}}$, so that we have


\begin{eqnarray}
ln\frac{p_{pass}}{1- p_{pass}}= b_0 + b_1 age \\
y \sim Bern(p_{pass})
\end{eqnarray}

Note that we do not have a residual anymore: the randomness around the predicted values is no longer modelled using a residual $e$ that is normally distributed, but is now modelled by a $y$-variable with a Bernoulli distribution.
Also note the strange relationship between the probability parameter $p_{pass}$ for the Bernoulli distribition, and the dependent variable for the linear equation $b_0+b_1 age$. The linear model predicts the logodds, but for the Bernoulli distribution, we use the probability. But it turns out that this model is very flexible and useful in many real-life problems. This model is often called a \textit{logit} model: one often writes that the \textit{logit of the probability} is predicted by a linear model.

\begin{eqnarray}
logit(p_{pass}) = b_0 + b_1 age \\
y \sim Bern(p_{pass})
\end{eqnarray}

In essence, the logit function transforms a $p$-value into a logodds:

\begin{equation}
logit(p)= ln( \frac{p}{1-p} ) \nonumber
\end{equation}

So what does it look like, a linear model for logodds (or logits of probabilities)?

In Figure \ref{fig:gen_9} we show a hypothetical example of a linear model for the logit of probabilities of passing an exam. These logits or logodds are predicted by age using a straight, linear regression line:


<<gen_9, fig.height=4, echo=FALSE, fig.align='center', message=F,fig.cap='Example of a linear model for the logit of probabilities of passing an exam.'>>=
age <- seq(0,200, 1)
logoddsratio <- predict.glm(outglm, newdata=data.frame(age))
data <- data.frame(age, logoddsratio)
data %>% ggplot(aes(x=age, y=logoddsratio)) + geom_line() + ylab("logit(p)=logarithm of the odds") + xlab("age in months") +
        geom_label(  aes(x=125, y=0.2), col='black', label='intercept: -3.8, slope=0.05', show.legend = F)
@
% 
When we take all these predicted logodds and convert them back to probabilities, we obtain the plot in Figure \ref{fig:gen_10}. Note the change in the scale of the vertical axis, the rest of the plot is the same as in Figure \ref{fig:gen_9}.

<<gen_10, fig.height=4, echo=FALSE, fig.align='center', message=F,fig.cap='Example with logodds transformed into probabilties (vertical axis).'>>=
probs <- exp(logoddsratio)/(1+exp(logoddsratio))
data <- data.frame(age, probs)
data %>% ggplot(aes(x=age, y=probs)) + geom_line() + ylab("probability of passing the exam") + xlab("age in months")
@

Here again we see the S-shape relationship between probabilities and the logodds. We see that our model predicts probabilities close to 0 for very young ages, and probabilities close to 1 for very old ages. There is a clear positive effect of age on the probability of passing the exam. But note that the relationship is not linear on the scale of the probabilities: it is linear on the scale of the logit of the probabilities, see Figure \ref{fig:gen_9}!

The curvilinear shape we see in Figure \ref{fig:gen_10} is called a \textit{logistic} curve. It is based on the logistic function: here $p$ is a logistic function of age (and note the similarity with Equation \ref{eq:logistic1}):


\begin{equation}
p = logistic(b_0 + b_1 age) = \frac{exp(b_0 + b_1 age)}{1+exp(b_0+ b_1 age)} \nonumber
\end{equation}

In summary, if we go from logodds to probabilties, we use the logistic function, $logistic(x)=\frac{exp(x)}{1+exp(x)}$. If we go from probabilities to logodds, we use the logit function, $logit(p)=ln\frac{p}{1-p}$. The logistic regression model is a generalized linear model with a logit link function, because the linear equation $b_0 + b_1 X$ predicts the logit of a probability. It is also often said that we're dealing with a logistic link function, because the linear equation gives a value that we have to subject to the logistic function to get the probability. Both terms, logit link function and logistic link function can be used.

If we go back to our data on the third-grade children that either passed or failed the exam, we see that this curve gives a description of our data, see Figure \ref{fig:gen_11}. The model predicts that around the age of 75 months, the probability of passing the exam is around 0.50. We indeed see in Figure \ref{fig:gen_11} that some children pass the exam (score=1) and some don't (score=0). On the basis of this analysis there seems to be a positive relationship between age in third-grade children and the probability of passing the exam in this sample.

<<gen_11, fig.height=4, echo=FALSE, fig.align='center', message=F, fig.cap='Transformed regression line and raw data points.'>>=
data %>% ggplot( aes(x=age, y=probs)) + geom_line() + ylab("probability of passing the exam") + xlab("age in months")  +
  geom_point(data=data.exam, aes(x=age, y=score)) +xlim(c(0, 200))
@

What we have done here is a \textit{logistic regression} of passing the exam on age. It is called logistic because the curve in Figure \ref{fig:gen_11} has a logistic shape. Logistic regression is one specific form of a \textit{generalized linear model}. Here we have applied a generalized linear model with a so-called \textit{logit link function}: instead of modelling dependent variable $y$ directly, we have modelled \textit{the logit of the probabilities of obtaining a $y$-value of 1}. There are many other link functions possible. One of them we will see in the section on generalized linear models for count data. But first, let's see how logistic regression can be performed in SPSS, and how we should interpret the output.

\section{Logistic regression in SPSS}

Imagine a data set on travellers from Amsterdam to Paris. From 1000 travellers, randomly sampled in 2017, we know whether they took the train to Paris, or whether they used other means of transportation. Of these travellers, we know their age, sex, yearly income, and whether they are travelling for business or not.

Part of the data are displayed in Table \ref{tab:gen_12}. A score of 1 on the variable \textbf{train} means they took the train, a score of 0 means they did not.



<<gen_12, fig.height=4, echo=FALSE, fig.align='center', message=F, results="asis", warning=F>>=
set.seed(123)
age <- runif(1000, 17, 80)
sex_male <- rbinom(1000, 1, 0.6)
income <- exp (rnorm(1000, log(30000), log(4))) %>% round(0)
business <- rbinom(1000, 1, 0.6)
logodds = 65.5  - 0.006 *income
probs = exp(logodds) /(1+exp(logodds))
train <- rbinom(1000, 1, probs)
data.train <- data.frame(train, age, sex_male, income, business )
data.train %>%
        head() %>%
        xtable(caption="Taking the train to Paris data.", label="tab:gen_12") %>%
        print(include.rownames=F, caption.placement = "top")
# glm(train~ income + sex_male +sex_male*income, data=data.train, family="binomial") %>% summary()
out.train <- glm(train~ income , data=data.train, family="binomial")
source('/Users/stephanievandenberg/Dropbox/Statistiek_Onderwijs/Data Analysis/writeForeignCode.R')
write.foreign(data.train,
              '/Users/stephanievandenberg/Dropbox/Statistiek_Onderwijs/Data Analysis/spss examples logistic/train.sav',
              '/Users/stephanievandenberg/Dropbox/Statistiek_Onderwijs/Data Analysis/spss examples logistic/train.sps',
              package = c("SPSS"))
@


Suppose we want to know what kind of people are more likely to take the train to Paris. We can use a logistic regression analysis to predict whether people take the train or not, on the basis of their age, sex, income, and main purpose of the trip.

Let's first see whether income predicts the probability of taking the train. The syntax for such a model involves the GENLIN procedure, which stands for GENeralized LINear model.



\begin{verbatim}
GENLIN train (REFERENCE=FIRST) WITH income
  /MODEL income
 DISTRIBUTION=BINOMIAL LINK=LOGIT
  /PRINT CPS DESCRIPTIVES SOLUTION.
\end{verbatim}


Note the similary with the GLM and MIXED procedures: start with the dependent variable (\textbf{train} in this case, with only two possible values) and then after the WITH word the variables that you'd like to treat quantitatively, here \textbf{income}. Under the MODEL subcommand we specify the model, here only a main effect of \textbf{income}. But further we have to specify that we want to use the Bernoulli distribution and a logit link function. So LINK=LOGIT, but why a binomial distribution? Well, a Bernoulli distribution (one coin flip) is only a special case of the Binomial distribution (the distribution of several coin flips). So here we use a binomial distribution for one coin flip, which is equivalent to a Bernoulli distribution. The last line indicates what type of output we want to see: case processing statistics, descriptives and the solution in terms of parameter estimates.

One very important part of the syntax is the (REFERENCE = FIRST) statement for the dependent variable. The default SPSS syntax uses (REFERENCE = LAST), so that's what you get when you do not specify this part. (REFERENCE = LAST) means that the reference category of the train variable is the last value. Since there are only two values, 0 an 1, the last value is equal to 1. In that case, SPSS will derive a model that predicts the logoddss for NOT taking the train, since it estimates the effect of income on the dependent variable \textit{relative to taking the train}. In our case, it makes more sense to derive a model for the logoddss of taking the train. We want to predict logodddsratios for taking the train, so we need to specify that our first value, 0, is our reference category: (REFERENCE = FIRST).


In Figure \ref{fig:train1} we see the parameter estimates from this generalized linear model run on the train data.


\begin{figure}[h]
    \begin{center}
       \includegraphics[scale=0.7, trim={0cm 22cm 0cm 0cm}]{/Users/stephanievandenberg/Dropbox/Statistiek_Onderwijs/Data" "Analysis/spss" "examples" "logistic/train1.pdf}
    \end{center}
     \caption{SPSS output of a generalized linear model for predicting taking the train from income.}
    \label{fig:train1}
\end{figure}


The parameter estimates table from a GENLIN analysis looks very much like that of the ordinary linear model and the linear mixed model. The only difference is that we no longer see $t$-statistics, but Wald Chi-Square statistics. This is because with logistic models, the ratio $B/SE$ does not have a $t$-distribution. In ordinary linear models, the ratio $B/SE$ has a $t$-distribution because in linear models, the variance of the residuals, $\sigma^2_e$, has to be estimated. If the residual variance was known, $B/SE$ would have a standard normal distribution. In logistic models, there is no $\sigma^2_e$ that needs to be estimated, so the ratio $B/SE$ has a standard normal distribution\footnote{This is the reason why you see (scale) equal to constant 1 in the SPSS output, right under the parameter for \textbf{income}. In the logistic model, the variance (scale) is fixed (assumed known).}. One could therefore calculate a $Z$-statistic $Z=B/SE$ and see whether that value is smaller than 1.96 or larger than 1.96, if you want to test with a Type I error rate of 0.05. SPSS has chosen to not compute such a $Z$-statisic, but to compute a chi-square statistic $X^2= B^2/SE^2$. This chi-square or $X^2$-statistic has a $\chi^2$ distribution with 1 degree of freedom. Both approaches, computing $Z$ or $X^2$, are equivalent.
\\
\\
The interpretation of the $B$-parameters is very similar to other linear models. Note that we have the following equation for the logistic model:



\begin{eqnarray}
logit(p_{train}) = b_0 + b_1 income \nonumber \\
train \sim Bern(p_{train})
\end{eqnarray}

If we fill in the values from the SPSS output, we get


\begin{eqnarray}
logit(p_{train}) = 90.017 - 0.008 \times income \nonumber \\
train \sim Bern(p_{train})
\end{eqnarray}


We can interpret these results by making some predictions. Imagine a traveller with a yearly income of 11,000 Euros. Then the predicted logodds equals $90.017 - 0.008 \times 11000= \Sexpr{90.017 - 0.008 * 11000}$. When we transform this back to a probability, we get $\frac{exp(\Sexpr{90.017 - 0.008 * 11000}) } {1+ exp(\Sexpr{90.017 - 0.008 * 11000}) }= \Sexpr{round(predict(out.train, newdata=data.frame(income=11000), type="response"), 3) }  $. So this model predicts that for people with a yearly income of 11,000, about 52\% of them take the train (if they travel at all, that is!).

Now imagine a traveller with a yearly income of 100,000. Then the predicted logodds equals $6.752 - 0.001 \times 100000= \Sexpr{90.017 - 0.008 * 100000}$. When we transform this back to a probability, we get $\frac{exp(\Sexpr{90.017 - 0.008 * 100000}) } {1+ exp(\Sexpr{90.017 - 0.008 * 100000}) }= \Sexpr{round(predict(out.train, newdata=data.frame(income=100000), type="response"), 3)}$. So this model predicts that for people with a yearly income of 100,000, close to none of them take the train.
Going from 11,000 to 100,000 is a big difference. But the change in probabilities is also huge: it goes down from 0.52 to 0.

We found a difference in this sample of 1000 travellers, but is there also a difference in the entire population of travellers between Amsterdam and Paris? The SPSS table shows us that the effect of income, $- 0.008$, is statistically significant, $X^2(1)=7.541, p<0.01$. We can therefore reject the null-hypothesis that income is not related to whether people take the train or not.

Note that similar to other linear models, the intercept can be interpreted as the predicted logodds for people that have values 0 for all other variables in the model. Therefore, 90.017 means in this case that the predicted logodds for people with zero income equals 90.017. This is equivalent to a probability of very close to 1.



\subsection{Exercises}

Using the train data, we try to predict whether people take the train or not by their purpose of their trip: business or not.


\begin{enumerate}

\item

What does the SPSS syntax look like? Note the data in Table \ref{tab:gen_12}.


\item
Suppose the results look like those in Figure \ref{fig:train2}. What is the predicted probability of taking the train for people that travel for business? Provide the calculations.


\begin{figure}[h]
    \begin{center}
       \includegraphics[scale=0.7, trim={0cm 22cm 0cm 0cm}]{/Users/stephanievandenberg/Dropbox/Statistiek_Onderwijs/Data" "Analysis/spss" "examples" "logistic/train2.pdf}
    \end{center}
     \caption{SPSS output of a generalized linear model for predicting taking the train from purpose of the trip.}
    \label{fig:train2}
\end{figure}


\item Suppose the results look like those in Figure \ref{fig:train2}. What is the predicted probability of taking the train for people that travel NOT for business? Provide the calculations.



\item
Suppose the results look like those in Figure \ref{fig:train3}. What is the predicted probability of taking the train for people that travel for business? Provide the calculations.


\begin{figure}[h]
    \begin{center}
       \includegraphics[scale=0.7, trim={0cm 22cm 0cm 0cm}]{/Users/stephanievandenberg/Dropbox/Statistiek_Onderwijs/Data" "Analysis/spss" "examples" "logistic/train3.pdf}
    \end{center}
     \caption{SPSS output of a generalized linear model for predicting taking the train from purpose of the trip.}
    \label{fig:train3}
\end{figure}


\item Suppose the results look like those in Figure \ref{fig:train3}. What is the predicted probability of taking the train for people that travel NOT for business? Provide the calculations.


\item On the basis of this SPSS output, do business travellers tend to take the train more or less often than non-business travellers? Motivate your answer.


\item
Suppose in SPSS output for logistic regression, you find an intercept value of 0.5 with a standard error of 0.1. There is a corresponding Wald chi-square value of $\Sexpr{0.5^2/0.1/0.1}$. Explain where this Wald chi-square value comes from.

\item

Suppose we have the data on coin flips in following table:

<<gen_13, fig.height=4, echo=FALSE, fig.align='center'>>=
set.seed(1234)
ID <- seq(1:5)
Heads <- rbinom(5, 1, 0.5)
weight <- rchisq(5, df=3)
type <- c("5cents", "10cents", "1Euro", "10cents","1Euro")
data <- data.frame(ID, Heads, weight, type )
data %>% kable()
@

If we want to predict the outcome of the coin flip, on the basis of the type of coin, should we use a linear model, a linear mixed model, or a generalized linear model? Motivate your answer.
\\
\\
If we want to predict the weight of the coin, on the basis of the type of the coin, should we use a linear model, a linear mixed model, or a generalized linear model? Motivate your answer.


\end{enumerate}


Answers:
\begin{enumerate}


\item
It could look like this (using WITH, treating the independent variable as quantitative):

\begin{verbatim}
GENLIN train (REFERENCE=FIRST) WITH business
  /MODEL business
 DISTRIBUTION=BINOMIAL LINK=LOGIT
  /PRINT CPS DESCRIPTIVES   SOLUTION.
\end{verbatim}


or like this (using BY, treating the independent variable as qualitative)

\begin{verbatim}
GENLIN train (REFERENCE=FIRST) BY business
  /MODEL business
 DISTRIBUTION=BINOMIAL LINK=LOGIT
  /PRINT CPS DESCRIPTIVES   SOLUTION.
\end{verbatim}


\item
People that travel for business score 1 on the business variable. So the predicted logodds for those people is $-1.155 - 0.050 \times 1 = -1.205$. The odds is the $exp(-1.205)=\Sexpr{exp(-1.205)} $. So the odds of going by train are 0.30 to 1. This is equivalent to 3 to 10. So suppose we have 13 trips, 3 are by train and 10 are not by train. So the probability of a trip being by train equals $3/13=\Sexpr{round(3/13,2)}$.

\item
People that travel NOT for business score 0 on the business variable. So the predicted logodds for those people is $-1.155 - 0.050 \times 0 = -1.155$. The odds is the $exp(-1.155)=\Sexpr{exp(-1.155)} $. So the odds of going by train are 0.32 to 1. This is equivalent to 32 to 100. So suppose we have 132 trips, 32 are by train and 100 are not by train. So the probability of a trip being by train equals $32/132=\Sexpr{round(32/132,2)}$.

\item


\item

\item


If we want to predict the outcome of the coin flip, on the basis of the type of coin, we should use a generalized linear model, because the dependent variable is dichotomous (has only 2 values), so the residuals can never have a normal distribution.
\\
\\
If we want to predict the weight of the coin, on the basis of the type of the coin, we should use a linear model, because the dependent variable is continuous.


\end{enumerate}




