
\documentclass[]{report}
\usepackage[english]{babel}
\usepackage{graphicx}





% Title Page
\title{Analyzing data using linear models}

\author{St\'ephanie van den Berg}
\date{Versie 0.1 \\ (\today)}



\begin{document}
\maketitle

<<libraries, echo=FALSE,  warning=FALSE, message=F>>=
library(ggplot2)
library(foreign)
library(dplyr)
library(lme4)
library(tidyr)
library(xtable)
library(scales)
library(DAAG)
@


\begin{abstract}
This book is intended to be of use to bachelor students in social sciences that want to learn how to analyze their data, with the specific aim to answer research questions. The book has a practical take on data analysis: how to do it, how to interpret the results, and how to report the results. All techniques are presented within the framework of linear models: this includes simple regression models, to linear mixed models, and generalized linear models. All methods can be carried out within one supermodel: the generalized linear mixed model. This approach is illustrated using SPSS.
\end{abstract}


\tableofcontents


% \Sexpr{knit_child('chapter_1.Rnw')} % exploring your data, descriptive statistics

\chapter{Linear modelling: introduction FULYA}
\section{Linear equations}

From secondary education you might remember linear equations. Suppose you have two quantities, $x$ and $y$, and there is a straight line that describes best their relationship. An example is given in Figure \ref{fig:lm_1}

<<lm_1,fig.height=4, echo=FALSE, fig.align='center', fig.cap='Straight line with intercept 0 and slope 2.'>>=
x = seq(-1,10,0.01)
y = 2*x
data_frame(x, y) %>% ggplot(aes(x,y)) + geom_point()+
        geom_vline(xintercept=0)+ geom_hline(yintercept = 0) + scale_x_continuous(breaks=seq(-1,10,1), minor_breaks =seq(-1,10,1) ) +
scale_y_continuous(breaks=seq(-0,20,1), minor_breaks =seq(-0,20,1))
@

We see that for every value of $x$, there is only one value of $y$. Moreover, the larger the value of $x$, the larger the value of $y$. If we look more closely we see that for each increase of 1 in x, there is an increase of 2 in y. For instance, if x=1, we see a y value of 2, and if x=2 we see a y-value of 4. So if we move from x=1 to x=2 (a step of one on the x-axis), we move from 2 to 4 on the y-axis, which is an increase of 2.

This increase of 2 for every step of 1 in x is the same for all values of x and y. For instance, if we move from 9 to 10 on the x-axis, we go from 18 to 20 on the y-axis: an increase of 2. This constant increase is typical of linear relationships. The increase in y for every unit increase in x is called the \textit{slope} of a straight line. In this figure, the slope is equal to 2.

The slope is one important characteristic of a straight line. The second important property of a straight line is the \textit{intercept}. The intercept is the value of y, if $x=0$. In Figure \ref{fig:lm_1} we see that if $x=0$, $y$ is 0, too. Therefore the intercept of this straight line is 0.

With the intercept and the slope, we completely describe this straight line: no other information is necessary. Such a straight line describes a linear relationship between x and y. The linear relationship can be formalized using a linear equation. The general form of a linear equation for two variables x and y is the following:

\begin{equation}
y = intercept + slope \times x
\end{equation}


For the linear relationship between x and y in Figure \ref{fig:lm_1} the linear equation is therefore

\begin{equation}
y = 0 + 2 x
\end{equation}

which can be simplified to

\begin{equation}
y =  2 x
\end{equation}


With this equation, we can find the y-value for all x-values. For instance, if we want to know the y-value for $x=3.14$, then using the linear equation we know that $y = 2 \times 3.14 = 6.28$. If we want to know the y value for $x=49876.6$, we use the equation to obtain $y=2\times 49876.6 = 99753.2$. In short, the linear equation is very helpful to quickly say what y-value is on the basis of the x-value, even if we don't have a graph of the relationship or if the graph does not extent to certain x-values.

Figure \ref{fig:lm_2} shows a different linear relationship between $x$ and $y$. First we look at the slope: we see that for every unit increase in $x$ (from 1 to 2, or from 4 to 5) we see an increase of 0.5 in $y$. Therefore the slope is equal to 0.5. Second we look at the intercept: we see that if $x=0$, $y$ has the value -2. So the intercept is -2. Again, we can describe the linear relationship by a linear equation, which is now:

\begin{equation}
y = -2 + 0.5 x
\end{equation}



<<lm_2,fig.height=4, echo=FALSE, fig.align='center', fig.cap='Straight line with intercept -2 and slope 0.5.'>>=
x = seq(-1,10,0.01)
y = -2+ 0.5*x
data_frame(x, y) %>% ggplot(aes(x,y)) + geom_point()+
        geom_vline(xintercept=0)+ geom_hline(yintercept = 0) + scale_x_continuous(breaks=seq(-1,10,1), minor_breaks =seq(-1,10,1) ) +
scale_y_continuous(breaks=seq(-2,20,1), minor_breaks =seq(-2,20,1))
@


Linear relationships can also be negative, see Figure \ref{fig:lm_3}. There, we see that if we move from 0 to 1, we see a \textit{decrease} of 2 in y (we move from y=-2 to y=-4), so that is our slope value. Further, if x=0, we see a y-value of -2, and that is our intercept. The linear equation is therefore:

\begin{equation}
y = -2 - 2 x
\end{equation}


<<lm_3,fig.height=4, echo=FALSE, fig.align='center', fig.cap='Straight line with intercept -2 and slope 0.5.'>>=
x = seq(-1,10,0.01)
y = -2 - 2*x
data_frame(x, y) %>% ggplot(aes(x,y)) + geom_point()+
        geom_vline(xintercept=0)+ geom_hline(yintercept = 0) + scale_x_continuous(breaks=seq(-1,10,1), minor_breaks =seq(-1,10,1) ) +
scale_y_continuous(breaks=seq(-60,0,1), minor_breaks =seq(-60,0,1))
@

\subsection{Exercises}

\begin{enumerate}
\item
For Figures \ref{fig:lm_4}, \ref{fig:lm_5} and \ref{fig:lm_6}, give the linear equations for the relationship between x and y.


<<lm_4,fig.height=4, echo=FALSE, fig.align='center', fig.cap='Straight line example.'>>=
x = seq(-1,10,0.01)
y = 3 - 1*x
data_frame(x, y) %>% ggplot(aes(x,y)) + geom_point()+
        geom_vline(xintercept=0)+ geom_hline(yintercept = 0) + scale_x_continuous(breaks=seq(-1,10,1), minor_breaks =seq(-1,10,1) ) +
scale_y_continuous(breaks=seq(-60,20,1), minor_breaks =seq(-60,20,1))
@

<<lm_5,fig.height=4, echo=FALSE, fig.align='center', fig.cap='Straight line example.'>>=
x = seq(-1,10,0.01)
y = 1.5 - 0.5*x
data_frame(x, y) %>% ggplot(aes(x,y)) + geom_point()+
        geom_vline(xintercept=0)+ geom_hline(yintercept = 0) + scale_x_continuous(breaks=seq(-1,10,1), minor_breaks =seq(-1,10,1) ) +
scale_y_continuous(breaks=seq(-60,20,1), minor_breaks =seq(-60,20,1))
@

<<lm_6,fig.height=4, echo=FALSE, fig.align='center', fig.cap='Straight line example.'>>=
x = seq(-1,10,0.01)
y = -1.5 + 0.3333333333*x
data_frame(x, y) %>% ggplot(aes(x,y)) + geom_point()+
        geom_vline(xintercept=0)+ geom_hline(yintercept = 0) + scale_x_continuous(breaks=seq(-1,10,1), minor_breaks =seq(-1,10,1) ) +
scale_y_continuous(breaks=seq(-60,20,1), minor_breaks =seq(-60,20,1))
@
\item
Try to sketch the straight line for the equation $y=1 - 2x$


\end{enumerate}


Answers:

The equations are

\begin{equation}
y = 3 - 1 x
\end{equation}

\begin{equation}
y = 1.5 - 0.5 x
\end{equation}

\begin{equation}
y = -2 + 0.33 x
\end{equation}

The straight line for $y=1 - 2x$ is presented in Figure \ref{fig:lm_7}.

<<lm_7,fig.height=4, echo=FALSE, fig.align='center', fig.cap='Straight line with based on y=1-2x.'>>=
x = seq(-1,10,0.01)
y = 1 -2*x
data_frame(x, y) %>% ggplot(aes(x,y)) + geom_point()+
        geom_vline(xintercept=0)+ geom_hline(yintercept = 0) + scale_x_continuous(breaks=seq(-1,10,1), minor_breaks =seq(-1,10,1) ) +
scale_y_continuous(breaks=seq(-60,20,1), minor_breaks =seq(-60,20,1))
@

\section{Linear regression}

In the previous section, we saw perfect linear relationships between quantities $x$ and $y$. For each x-value there was only 1 y-value, and the values are all desribed by a straight line. Such relationships we often see in physics, or in mathematics.

In social sciences we hardly ever see such perfectly linear relationships between quantities (variables). For instance, let us plot the relationship between yearly income and the amount of Euros spent on holidays. Yearly income is measured in thousands of Euros (kEuros), and money yearly spent on holidays is measured in Euros. We plot yearly income on the x-axis (horizontal axis) and holiday spendings on the y-axis (vertical axis). Suppose we find the following data from 100 women between 30 and 40 years of age, plotted in Figure \ref{fig:lm_8}.


<<lm_8,fig.height=4, echo=FALSE, fig.align='center', fig.cap='Data on holiday spending.'>>=
set.seed(12346)
x = runif(100, 10,100)
x[x>50 & x<99] <- runif(length(x[x>50& x<99]), 10,70)
y = 100+10* x+ rnorm(100,0,15 )
plot <- data_frame(x, y) %>% ggplot(aes(x,y)) + geom_point()+
        geom_vline(xintercept=0)+ geom_hline(yintercept = 0) + scale_x_continuous(breaks=seq(0,100,10), minor_breaks =seq(0,100,10) ) +
scale_y_continuous(breaks=seq(0,20000,100), minor_breaks =seq(0,20000,100)) +xlab("Yearly income in kEuros") + ylab("Euros yearly spent on holidays")
plot
@

In the scatterplot, we see that one women has a yearly income of 100,000 Euros, and that she spends almost 1100 Euros per year on holidays. We also see a couple of women who earn less between 10,000 and 20,000 Euros a year and they spend between 200 and 300 Euros per year on holiday.

The data obviously do not form a straight line. However, we tend to think that the relationship between yearly income and holiday spending is more or less linear: for every increase of 10,000 Euros in yearly income, we an increase of about 100 Euros.

Lets plot such a straight line with a slope of 100 straight through the data points. The result is seen in Figure \ref{fig:lm_9}. We see that the line with a slope of 100 is a nice approximation of the relationship between yearly income and holiday spendings. We also see that the intercept of the line is 100.

<<lm_9,fig.height=4, echo=FALSE, fig.align='center', fig.cap='Data on holiday spending with an added straight line.'>>=
plot + geom_abline(intercept=100, slope=10) +
        geom_segment(aes(x = x[order(x)][99]-25, y = 900, xend = 67, yend = 824),
                  arrow = arrow(length = unit(0.5, "cm")))+
        geom_label(x=x[order(x)][99]-25, y=900, label='Sandra Schmidt') 
@

Given the intercept and slope, the linear equation for the straight line approximating the relationship is

\begin{equation}
YearlyIncome = 100 + 100 \times HolidaySpendings
\end{equation}

In summary, data on two variables may not show a perfect linear relationship, but in many cases, a perfect straight line can be a very reasonable approximation of the data. Finding such a straight line to approximate the data points is called \textit{linear regression}. In linear regression we describe the behaviour of the dependent variable (the $y$-variable on the vertical axis) on the basis of the independent variable (the $x$-value on the horizontal axis) using a linear equation. We say that we regress variable y on variable x.




\section{Residuals}

Even though a straight line can be a good approximation of a data set, it is hardly ever perfect: there are always discrependencies between what the straight line describes and what the data actually tell us.

For instance, in Figure \ref{fig:lm_9}, we see a women, Sandra Schmidt, who makes \Sexpr{1000*x[order(x)][99]} Euros a year and who spends \Sexpr{round(y[order(x)][99],2)} Euros on holidays. According to the linear equation that describes the straight line, a woman that earns \Sexpr{1000*x[order(x)][99]} Euros a year would spend $100 + 100 * \Sexpr{1000*x[order(x)][99]}= \Sexpr{round(100 + 10*x[order(x)][99],2)}$ Euros on holidays. The discrependy between the actual amount spent and the one described by the linear equation equals $\Sexpr{round(y[order(x)][99],2)}-\Sexpr{round(100 + 10*x[order(x)][99],2)}=\Sexpr{round(y[order(x)][99],2)-round(100 + 10*x[order(x)][99],2)}$ Euros. This difference is rather small and the same holds for all the other women in this data set.

Such descrepencies between the actual amount spent and the amount as described or predicted by the straight line are called \textit{residuals} or \textit{errors}. Using the linear equation, we could predict holiday spending even for yearly incomes that are not in the data set. For instance, in this data set there is no woman with an income of 80,000, but still we can use the linear equation with a prediction that such a woman would probably spend around $100+100\times 80= 8100$ Euros.

The residual (or error) is the difference between a certain data point and what the linear equation predicts.


Let us look at another fictitious data set where the residuals (errors) are a bit larger. Figure \ref{fig:lm_10} shows the relationship between variables x and y. The dots are the actual data points and the blue straight line is an approximation of the actual relationship. The residuals are also visualized: sometimes the observed $y$-value is greater than the predicted $y$-value (dots above the line) and sometimes the oberved $y$-value is smaller than the predicted $y$-value. Let's denote the predicted $y$-value (the value of $y$ predicted by the blue line) as $\hat{y}$ (pronounced as y-hat), then we can define a residual or error as the discrepency between the observed $y$ and $\hat{y}$:

\begin{equation}
e = y - \hat{y}
\end{equation}

where $e$ stands for the error (residual).


<<lm_10,fig.height=4, echo=FALSE, fig.align='center', fig.cap='Data on variables x and y with an added straight line.'>>=
set.seed(111)
x = runif(200, 0,100)
x[x>97 & x<99] <- runif(length(x[x>97& x<99]), 10,70)
y = 500+20* x+ rnorm(200,0,120 )

out <- lm(y~x)
predicted <- predict(out)
residuals <- residuals(out)

plot <- data_frame(x, y) %>% ggplot(aes(x,y)) + geom_point()+
        geom_vline(xintercept=0)+ geom_hline(yintercept = 0) + scale_x_continuous(breaks=seq(0,100,10), minor_breaks =seq(0,100,10) ) +
scale_y_continuous(breaks=seq(0,20000,500), minor_breaks =seq(0,20000,500))
plot  + geom_segment(aes(xend = x, yend = predicted)) + geom_smooth(se=F, method='lm')
@

If we compute residual $e$ for every $y$-value in the data set, we can plot them using a histogram, as displayed in Figure \ref{fig:lm_11}. We see that the residuals are on average 0, and that the histogram has the shape of the normal distribution, more or less. Such normally-shaped distributions of residuals we see often in research. Here, the residuals show a normal distribution with mean 0 and variance of \Sexpr{round(var(residuals),0)} (a standard deviation of \Sexpr{round(sqrt(round(var(residuals),0)),0)}).


<<lm_11,fig.height=4, echo=FALSE, fig.align='center', fig.cap='Histogram of the residuals (errors).'>>=
data.frame(residuals) %>% ggplot(aes(x=residuals)) +
        geom_histogram(bins=15) +
        xlim(c(-410,410))
@


\section{Least squares regression lines}


You may ask yourself how to draw a straight line through the data points: How do you decide the exact slope and the exact intercept? And what if you don't want to draw the data points and the straight line by hand? That can be quite cumbersome if you have more than 2000 data points to plot!

First, because we are lazy, we always use a computer to draw the data points and the regression line. Second, since we could draw many different straight lines through a scatter of points, we need a criterion to determine a nice combination of intercept and slope. With such a criterion we can then let the computer determine the straight line with its equation for us.

The criterion that we use in this chapter is called Least Squares, or Ordinary Least Squares (OLS). To explain the Least Squares principle, look again at Figure \ref{fig:lm_10} where we see both small and large residuals. About half of them are positive (above the blue line) and half of them are negative (below the blue line).

The most reasonable idea is to draw a straight line that is more or less in the middle of the $y$-values, in other words, with about half of residuals positive and about half of them negative. Or perhaps we could say that on average, the residuals should be 0. A third way of saying the same thing is that the sum of the residuals should be equal to 0.

However, the criterion that all residuals should sum to 0 is not sufficient. In Figure \ref{fig:lm_12} we see a straight line with a slope of 0 where all residuals sum to 0. However, this regression line does not make intuitive sense: it does not describe the structure in the data very well. Moreover, we see that the residuals are much larger than in Figure \ref{fig:lm_10}.

<<lm_12,fig.height=4, echo=FALSE, fig.align='center', fig.cap='Data on variables x and y with an added straight line. The sum of the residuals equals 0.'>>=


plot  + geom_segment(aes(xend = x, yend = mean(y))) + geom_abline(intercept=mean(y), slope=0)
@

We therefore need a second criterion to find a nice straight line. We want the residuals to sum to 0, but also want the residuals to be as small as possible: the descrepencies between what the linear equation predicts (the $\hat{y}$-values) and the actual $y$-values should be as small as possible.

So now we have two criteria: we want the sum of the residuals to be 0 (half of them negative, half of them positive), and we want the residuals to be as small as possible. We can achieve both of these when we use as our criterion the idea that the sum of the \textit{squared} residuals be as small as possible. If the sum of the squared residuals is as small as possible, we know that the residuals are as small as possible. But we also know that the residuals then sum to zero\footnote{For the proof of this, see ....}. Thus, as our criterion we can use the regression line for which the squared differences between predicted and observed $y$-values are as small as possible. 

Figure \ref{fig:lm_13} shows three different regression lines for the same data set. Figure \ref{fig:lm_14} shows the respective distributions of the residuals. For the first line, we see that the residuals sum to 0, for the residuals are on average 0 (the red vertical line). However, we see quite large residuals. The residuals for the second line are smaller: we see very small positive residuals, but the negative residuals are still quite large. We also see that the residuals do not sum to 0. For the third line, we see both criteria optimized: the sum of the residuals is zero and the residuals are all very small. We see that for regression line 3, the sum of squared residuals is at its minimum value. 



<<lm_13,fig.height=4, echo=FALSE, fig.align='center', fig.cap='Three times the same data set, but with different regression lines.'>>=

plot <- data.frame(x=rep(x,3), y=rep(y,3), z=as.factor(rep(c(1,2,3), each=length(x))) ,
                   interc=rep(c((mean(y)-0.5*out$coef[2]*mean(x)),out$coef[1]+100,out$coef[1]), each=length(x)), 
                   slop= rep(c(0.5*out$coef[2],out$coef[2],out$coef[2]), each=length(x)) ) %>%
        ggplot(aes(x,y)) + geom_point() +
        geom_vline(xintercept=0)+ geom_hline(yintercept = 0) + 
        scale_x_continuous(breaks=seq(0,100,10), minor_breaks =seq(0,100,10) ) +
        scale_y_continuous(breaks=seq(0,20000,500), minor_breaks =seq(0,20000,500)) +                          facet_grid(~z) + geom_abline(aes(intercept=interc, slope=slop), col=2, size=1) 
plot  
@

<<lm_14,fig.height=4, echo=FALSE, fig.align='center', fig.cap='Histogram of the residuals (errors) for three different regression lines, and the respective sums of squared residuals (SSR).'>>=
pred1<- (mean(y)-0.5*out$coef[2]*mean(x)) + 0.5*out$coef[2]*x
pred2<- 650 + out$coef[2]*x
pred3 <- predict(out)
predictions <- c(pred1, pred2 ,pred3)       

bestresidual <-  out$residuals
residual1 <- y-pred1
residual2 <- y-pred2



residual <- c(residual1 , residual2, bestresidual)
squares1 <- (residual1)^2 %>% sum %>% round(0)
squares2 <- residual2^2 %>% sum %>% round(0)
squares3 <- bestresidual^2 %>% sum %>% round(0)
dif <- -1*(residual2-bestresidual)[1]

plot <- data.frame(residual, z=as.factor(rep(c(1,2,3), each=length(x)))) %>%
        ggplot(aes(x=residual)) + 
        geom_histogram( breaks = seq(-800,800,0.5*dif)   ) +
        facet_grid(~z) +
        geom_label(x= -200, y=50,label=rep(c(paste('SSR: ', squares1),paste('SSR:  ', squares2) ,paste('SSR: ', squares3) ),each=length(x))) +
        geom_vline(xintercept=0, col=2)
plot  
@

In summary, when we want to have a straight line that describes our data best, we'd like a line such that the residuals are on average 0 (i.e, sum to 0), and where we see the smallest residuals possible. We reach these criteria when we use the line in such a way that we have the lowest value for the sum of the squared residuals possible. This line is therefore called the least squares or OLS regression line. It turns out that this optimal regression slope can be found by a relatively simple computation using matrix algebra. In daily life, we do not do this by hand but let computers compute it for us, with software like for instance SPSS. 


\subsection{Exercises}

\begin{enumerate}



<<lm_15, fig.height=4, echo=FALSE, fig.align='center', message=F, results="asis", warning=F>>=
set.seed(123)
area <- runif(10, 30, 120) %>% round(0)
price <- rnorm(10, 0.9*area+80, 20) %>% round(0)

data_frame(Area=area, Price=price, PredictedPrice = " ", Residual=" ", SquaredResidual=" ") %>%
        head() %>%
        xtable(caption="Home prices.", label="tab:lm_15") %>%
        print(include.rownames=F, caption.placement = "top")

out.price <- lm(price ~ area , data=data_frame(area, price))
@

\item In Table \ref{tab:lm_15} you find a small data set on the price of homes with dependent variable price in kEuros and independent variable area in square meters. The least squares regression equation turns out to be $price = out.price$coef[1]+ out.price$coef[2]\times area$. Add a third column with the expected prices based on the regression equation ($\hat{y}$. Put the difference between the observed price and the expected price in the fourth column ($e$). Then compute the squared residuals and put those in the fifth column ($e^2)$. Take the sum of the squared residuals: How large is sum of the squared residuals? 


\item See website ....., try to find the Least Squares regression line for the given data set by changing both intercept and slope. How large is the sum of the squared residuals for that optimal regression line?


\item Do this exercice with one or more of your fellow students. Look at the data set plotted in Figure \ref{fig:lm_16}. Try to find the regression line with the lowest sum of squared residuals possible. 

<<lm_16,fig.height=4, echo=FALSE, fig.align='center', fig.cap='Plot of housing data.'>>=
set.seed(1234)
x <- runif(5, 30, 120) %>% round(0)
y <- rnorm(5, area+80, 20) %>% round(0)
y <- c(15, 20, 10, 10, 5)
x <- x/10

data_frame(x, y) %>%
        ggplot(aes(x, y)) +
        geom_point() +
        ylim(c(5,22))
 out <- lm(y~x) 
@


\end{enumerate}

Answers:
\begin{enumerate}

\item The predicted prices, the residuals and the squared residuals are displayed in Table \ref{tab:lm_17}. The sum of the squared residuals equals \Sexpr{sum(out.price$residuals^2)}.

<<lm_17, fig.height=4, echo=FALSE, fig.align='center', message=F, results="asis", warning=F>>=
data_frame(Area=area, Price=price, PredictedPrice = predict(out.price), Residual=out.price$residuals, SquaredResidual=out.price$residuals^2) %>%
                head() %>%
                xtable(caption="Home prices.", label="tab:lm_17") %>%
                print(include.rownames=F, caption.placement = "top")
@
\item

\item The lowest sum of squared residuals is \Sexpr{round(sum(out$residuals^2),0)}. This is the sum that you get with intercept \Sexpr{round(out$coef[1],1)} and slope \Sexpr{round(out$coef[2],1)}.

\end{enumerate}







\section{Pearson correlation}

For any set of two quantitative variables, we can determine the least squares regression line. However, it depends on the data set how well that regression line describes the data. 
Figure \ref{fig:lm_18} shows two different data sets on variables x and y. Both plots also show the least squares regression line, and they both turn out to be exactly the same: $y=100+10x$. 

<<lm_18,fig.height=4, echo=FALSE, fig.align='center', fig.cap='Plot of housing data.'>>=
set.seed(1234)
x <- rep(seq(1,100,1),2)
y <- rnorm(200, 100+x*10, 10) 
y[101:200] <- y[101:200]+rnorm(100,0,280)
z<- rep(c(1,2), each=100) %>% as.factor()

data_frame(x, y, z) %>%
        ggplot(aes(x, y)) +
        geom_point()  +
        facet_wrap(~z) +geom_abline(intercept=100, slope=10, col=4, size=1.5)
 out <- lm(y~x) 
@


We see that the regression line describes data set very well (left panel): the observed dots are very close to the line, which means that the residuals are very small. The regression line does a worse job for the second data set (right panel) since there are quite large discrepencies between the observed $y$-values and the predicted $y$-values. Put differently, the regressin equation can be used to predict $y$-values in data set 1 very well, almost without error, whereas the regression line cannot be used to predict $y$-values very precisely. The regression line is also the least squares regression line for data set 2, so any improvement by choosing another slope or intercept is not possible.

Francis Galton was the first to think about how to quantify this difference in the ability of a regression line to predict the dependent variable. Karl Pearson later worked on this measure so that it became to be called Pearson's correlation coefficient. It is a standardized measure, so that it can be used to compare different data sets. 

In order to get to Pearson's correlation coefficient, you first need to standardize both the independent variable, $x$, and the dependent variable, $y$. You standardize scores by taking their values, subtract the mean from them, and divide by the standard deviation. So, in order to obtain a standardized $x$-value we compute $Z_x$, 

\begin{equation}
Z_x = \frac{x- \bar{x}}{\sigma_x}
\end{equation}

and in order to obtain a standardized $y$-value we compute $Z_y$, 

\begin{equation}
Z_y = \frac{y- \bar{y}}{\sigma_y}.
\end{equation}



<<lm_19,fig.height=4, echo=FALSE, fig.align='center', fig.cap='Plot of housing data.'>>=
Z_x <- x %>% scale() %>% as.vector()
Z_y<-c()
Z_y[1:100] <- y[1:100] %>% scale() %>% as.vector() 
Z_y[101:200] <- y[101:200] %>% scale() %>% as.vector() 
  
data_frame(Z_x, Z_y, z) %>%
        ggplot(aes(Z_x, Z_y)) +
        geom_point()  +
        facet_wrap(~z) +geom_smooth(method='lm', se=F)
 out1 <- lm(Z_y[1:100]~Z_x[1:100]) 
 out2 <- lm(Z_y[101:200]~Z_x[101:200]) 
@

Let's do this both for data set 1 and data set 2, and plot the standardized scores. see Figure \ref{fig:lm_19}. If we then plot the least squares regression lines for the standardized values, we obtain different equations. For both data sets, the intercept is 0 because by standardizing the scores, the means become 0. But the slopes are different: in data set 1, the slope is \Sexpr{round(out1$coef[2],3)} and in data set 2, the slope is \Sexpr{round(out2$coef[2],3)}.

\begin{eqnarray}
Z_y = 0 + \Sexpr{round(out1$coef[2],3)}Z_x=\Sexpr{round(out1$coef[2],3)}Z_x \\
Z_y = 0 + \Sexpr{round(out2$coef[2],3)}Z_x=\Sexpr{round(out2$coef[2],3)}Z_x
\end{eqnarray}


These two slopes, the slope for the regression of standardized $y$-values on standardized $x$-values, are the correlation coefficients for data sets 1 and 2, respectively. For obvious reasons, the correlation is sometimes also referred to as the \textit{standardized slope coefficient}.

Correlation stands for the \textit{co-relation} between two variables. It tells you how strongly one variable can be predicted from the other. The correlation is bi-directional: the correlation between $y$ and $x$ is the same as the correlation between $x$ and $y$. For instance in Figure \ref{fig:lm_19}, if we would have put the $Z_x$-variable on the $Z_y$-axis, and the $Z_y$-variable on the $Z_x$-axis, the slopes would be exactly the same. This is true because the variances of the $y$ and $x$-variables are equal after standardization (both variances equal to 1). 

Since a slope can be negative, a correlation can be negative too. Furthermore, a correlation is always between -1 and 1. Look at Figure \ref{fig:lm_19}: the correlation between $x$ and $y$ is \Sexpr{round(out1$coef[2],3)}. The dots are almost on a straight line. If the dots would all be exactly on the straight line, the correlation would be 1. 

<<lm_20,fig.height=4, echo=FALSE, fig.align='center', fig.cap='Plot of housing data.'>>=
set.seed(111)
x <- rnorm(800,100, 5)
y[1:200] <- -2000 + 20 * x[1:200] + rnorm(100,0,170)
y[201:400] <- 400 - 5 * x[201:400] + rnorm(100, 0, 3)
y[401:600] <- -600 + rnorm(100, 0, 10)
y[601:800] <- 1000 - 16 * x[601:800] + rnorm(100, 0, 190)
z1 <- rep(c(1,2),each=400)  
z2 <- rep(rep(c(1,2), each=200),2) 
plot <- data_frame(x,y, z1, z2) %>%
        ggplot(aes(x,y)) +
        geom_point(size=0.5)  +
        facet_wrap(~z1+z2) +geom_smooth(method='lm', se=F, size=0.5)
c1 <- cor(x[1:200], y[1:200]) %>% round(2)
c2 <- cor(x[201:400], y[201:400]) %>% round(2)
c3 <- cor(x[401:600], y[401:600]) %>% round(2)
c4 <- cor(x[601:800], y[601:800]) %>% round(2)
plot + geom_label(x= 90, y=-1000,label=rep(c(paste(c1),paste(c2) ,paste(c3), paste(c4) ),each=200))
@


Figure \ref{fig:lm_20} shows a number of scatterplots of $x$ and $y$ with different correlations. Note that if dots are very close to the regression line, the correlation can still be close to 0. If the slope is 0 (bottom-left panel), then one variable cannot be predicted from the other variable, hence the correlation is 0, too. 

In summary, the correlation coefficient indicates how well one variable can be predicted from the other variable. It is the slope of the regression line if both variables are standardized. If prediction is not possible (when the regression slope is 0), the correlation is 0, too. If the prediction is perfect, without errors (no residuals) and with a slope unequal to 0, then the correlation is either -1 or +1, depending on the sign of the slope.

\section{Covariance}

The correlation is a standardized measure for how much two variables co-relate. There exists also an unstandardized measure for how much two variables co-relate: the \textit{covariance}. The correlation is the slope when two variables have each variance 1. When you multiply the correlation by a number indicating the variances of the two variables, you get the covariance. This number is the product of the two respective standard deviations.

The covariance between variables $x$ and $y$, Cov(x,y) can be computed as:


\begin{equation}
Cov(x,y)= Cor(x,y) \times \sigma_x \sigma_y
\end{equation}

For example, if the variance of $x$ equals 49 and the variance of $y$ equals 25, then the respective standard deviations are 7 and 5. If the correlation between $x$ and $y$ equals 0.5, then the covariance between $x$ and $y$ is equal to $0.5 \times 7 \times 5 = \Sexpr{0.5*7*5}$.

Similar to correlation, the covariance of two variables indicates by how much they co-vary. For instance, if the variance of $x$ is 3 and the variance of $y$ is 5, then a covariance of 2 indicates that $x$ and $y$ co-vary: if $x$ increases by a certain amount, $y$ also increases. If you want to know how many standard deviations $y$ increases if $x$ increases with one standard deviation, you can turn the covariance into a correlation my dividing the covariance by the respective standard deviations. 

\begin{equation}
Cor(x,y)= \frac{Cov(x,y)} { \sigma_x \sigma_y}= \frac{2} { \sqrt{3} \sqrt{5}}=\Sexpr{round(2/(sqrt(3)*sqrt(5)),2)}
\end{equation}

Similar to correlations and slopes, covariances can also be negative.


\subsection{Exercises}

\begin{enumerate}

\item The correlation between brain size and intelligence in 9-year old children equals 0.30. Suppose the variance in brain size equals 45 and the variance in intelligence 225. Compute the covariance.  



\item The covariance between intelligence and extraversion equals 1. The variance of intelligence is 225 and the variance of extraversion is 9. What is the correlation?




\item Suppose the correlation between intelligence and extraversion is 0.10. What does this mean?



\item Suppose the correlation between intelligence and extraversion is -0.05. What does this mean?



 
\item Suppose the correlation between intelligence and extraversion is 0.30. What is the regression slope if the variance of intelligence is 225 and the variance of extraversion is 9?



\end{enumerate}

\begin{enumerate}

\item 

\begin{equation}
Cov(x,y)= Cor(x,y) \times \sigma_x \sigma_y= 0.30 \times \sqrt{45}\times \sqrt{225}=\Sexpr{round(0.30*sqrt(45)*15)}
\end{equation}



\item 

\begin{equation}
Cor(x,y)= \frac{Cov(x,y)} { \sigma_x \sigma_y}= \frac{1} { \sqrt{225} \sqrt{9}}=\Sexpr{round(1/(15*3),2)}
\end{equation}

\item 
If you increase intelligence by 1 standard deviation, then extraversion increases with a tenth of a standard deviation.

\item 
If you increase intelligence by 1 standard deviation, then extraversion increases with 0.05 standard deviations.

\item
The correlation is 0.30, so if you increase intelligence by one standard deviation (which is $\sqrt{225}=15$), extraversion increases by 0.30 standard deviations (which equals $0.30 \times \sqrt{9}=0.90$). Therefore, if you increase intelligence by 15 points, you increase extraversion by 0.90 points. Thus if you increase intelligence by 1 point, you increase extraversion by $0.90/15=\Sexpr{round(0.9/15,2)}$ points. The slope for the regression of extraversion on intelligence is therefore \Sexpr{round(0.9/15,2)}.


\end{enumerate}


\section{Regression using SPSS}

\section{Linear models}

By performing a regression analysis of $y$ on $x$, we try to predict the $y$-value from a given $x$-value on the basis of a linear equation. We try to find an intercept and a slope for that linear equation such that our prediction is best. We define best as the linear equation for wich we see the lowest possible value for the sum of the squared residuals (least squares principle).

Thus, the predicted value of $y$ ($\hat{y}$) can be computed by the linear equation

\begin{equation}
\hat{y}= b_0 + b_1 x
\end{equation}

In reality, the predicted values of $y$ always deviate from the observed values of $y$. So, there is always an error $e$ that is the difference between $\hat{y}$ and $y$. Thus we have for the observed values of $y$

\begin{equation}
y = \hat{y} + e = b_0 + b_1 x + e
\end{equation}

Typically, we assume that the residuals $e$ are on average 0 and have a normal distribution with a certain variance $\sigma^2_e$. Taking the linear equation and the normally distributed residuals together, we have \textit{a linear model} for the two variables $x$ and $y$.


\begin{equation}
y = b_0 + b_1 x + e\\
e ~ N(0,\sigma^2_e)
\end{equation}

In this book, we will see a great variety of linear models between 2 or more variables. They can all be seen as extensions and variations of this basic linear model. They all aim to predict as best as possible one dependent variable from a number of independent variables. 



% \Sexpr{knit_child('chapter_inference_I.Rnw')} 


% \Sexpr{knit_child('chapter_inference_II.Rnw')}






% \Sexpr{knit_child('chapter_3.Rnw')} % multiple regression
% 
% \Sexpr{knit_child('chapter_5.Rnw')} % dummy variables and categorical predictors
% \Sexpr{knit_child('chapter_6.Rnw')} % moderation
% \Sexpr{knit_child('chapter_7.Rnw')} % assumptions
% \Sexpr{knit_child('chapter_8.Rnw')} % advanced topics linear models
% 
% %% contrasts en post hoc zijn nog te lastig te volgen, en zorg dat data niet 1 2 3 is, maar met betere labels, strings. niet te veel stapjes met contrast equations.
% 
% \Sexpr{knit_child('chapter_9.Rnw')} % nonparametric alternatives linear models
% \Sexpr{knit_child('chapter_10.Rnw')} % introduction linear mixed models
% \Sexpr{knit_child('chapter_11.Rnw')} % nonparametrics for within designs
% %
% 
% \Sexpr{knit_child('chapter_12.Rnw')} % generalized linear models: logistic regression
% % 
% \Sexpr{knit_child('chapter_13.Rnw')} % generalized linear models: poisson models

\end{document}