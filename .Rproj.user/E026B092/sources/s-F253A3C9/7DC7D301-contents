
\chapter{Planned and post hoc comparisons}\label{chap:advanced}



\section{Categorical variables and factors in R}

In this chapter we are going to look at the penguins dataset, accessible via the \textbf{palmerpenguins} package:



<<>>=
# install.packages("palmerpenguins")
library(palmerpenguins)
@



We use this dataset to gain a better insight in the workings of R. We can use this insight later to get better, more focused statistical answers to our research questions.

The data set contains information on three types of penguins: Adelie, Chinstrap and Gentoo.



<<>>=
penguins %>%
  dplyr::select(species) %>%
  table()
@


We saw in Chapter that we can model group differences using a linear regression model. Let us look at the variable on body mass: \texttt{body\_mass\_g}. The data are presented in Figure \ref{fig:penguins}.



<<penguins, message = F, warning = F, echo = F, fig.height = 3.5, fig.cap = "Body mass data on Adelie, Gentoo and Chinstrap penguins.">>=
penguins %>% 
  ggplot(aes(x = species, y = body_mass_g, colour = species)) +
  geom_jitter(show.legend = FALSE) +
  scale_colour_brewer(palette = "Set1")
@

It seems that the Gentoo penguins are generally heavier than the Adelie and Chinstrap species. The quantitative differences can be modelled with linear regression, if we use dummy coding. When \texttt{species} is a factor variable, R will do the dummy coding automatically for you. If we would run such a linear model and look at the output, we can deduce that R uses two dummy variables: one variable that equals 1 when a penguin is a Chinstrap penguin (and 0 otherwise), and one variable that equals 1 when a penguin is a Gentoo penguin (and 0 otherwise).

<<>>=
out <- penguins %>% 
  lm(body_mass_g ~ species, data = .)
out %>% 
  tidy()
@

We can see the actual dummy variables when we use the function \texttt{model.matrix()}. If we want to see the variables for the first 3 penguins we type:

<<>>=
model.matrix(body_mass_g ~ species, data = penguins) %>% 
  head(3)
@

and the last three penguins:

<<>>=
model.matrix(body_mass_g ~ species, data = penguins) %>% 
  tail(3)
@

We see that this \textit{model matrix} is a data matrix, consisting of three new variables: \texttt{(Intercept)}, \texttt{speciesChinstrap} and \texttt{speciesGentoo}. A more common name for this type of matrix is the \textit{design matrix}.

The first three penguins are Adelie penguins. We see that the dummy variable \texttt{speciesChinstrap} is coded 0, and the dummy variable \texttt{speciesGentoo} is also coded 0, so the penguins must be Adelie. If we want to predict the weight for these Adelie penguins, we can fill in the regression equation with these two dummy variables and the \texttt{(intercept)} variable: 

\begin{equation}
\widehat{weight} = 1 \times 3701 + 0 \times 32.4 + 0 \times 1375 = 3701.
\end{equation}


Thus, using these three variables (one column of 1s for the intercept and two dummy variables) in combination with the regression equation gives us the expected group mean for the Adelie penguins. We can do the same exercise for the Chinstrap penguins. They are at the end of the data file. They have a code 1 for the intercept variable, a 1 for the dummy variable \texttt{speciesChinstrap} and a 0 for the dummy variable \texttt{speciesGentoo}. The expected group mean for the Chinstrap penguins is therefore $ 1 \times 3701 + 1 \times 32.4 + 0 \times 1375 = \Sexpr{3701 + 32.4}$. Similarly, the Gentoo penguins have a code 1 for the intercept, a 0 for the dummy variable \texttt{speciesChinstrap} and a 1 for the dummy variable \texttt{speciesGentoo}, resulting in an expected group mean of $ 1 \times 3701 + 0 \times 32.4 + 1 \times 1375 = \Sexpr{3701 + 1375}$.

But how does R decide what dummy variables to compute? And do we think R always chooses wisely?

\section{Factors in R}

Let's start with the first question: how does R decide what dummy variables to compute? R computes dummy variables for factor variables. A factor variable in R has a certain structure. If we look at the \texttt{species} factor, we see that it has three levels.

<<>>=
str(penguins$species)
@

Factors are a way of storing information effectively. Imagine that you have five penguins, of the species "Adelie", "Adelie", "Gentoo", "Chinstrap" and "Chinstrap", respectively. That's OK if you have only 5 penguins, but it takes a lot of text to describe the species of 5000 penguins. It would take less storage if you coded all Adelie penguins as 1, all Chinstrap penguins as a 2, and all Gentoo penguins as a 3. The five penguins would then be coded as 1, 1, 3, 2, and 2, respectively. That's a lot less text to store if you have 5000 penguins!

R does a similar thing. The variable \texttt{species} has three levels, internally coded as 1, 2, and 3, associated with the labels Adelie, Chinstrap, and Gentoo, respectively. 

<<>>=
levels(penguins$species)
@

This output tells us that Adelie is stored as 1, Chinstrap as 2, and Gentoo as 3. 

By default, the levels (labels) are the unique data values sorted alphabetically, but it does not need to be so, so always check. For example, we could give the levels different labels. Let's re-label the Adelie penguins, internally coded as 1, as "ZZz": 

<<>>=
levels(penguins$species)[1] <- "ZZz"
str(penguins$species)
levels(penguins$species)
@

We see that ZZz penguins (actually, Adelie) are still internally coded as 1, even while the label "ZZz" comes later alphabetically. 

This ordering is important, because it determines what dummy variables will be created for a linear model, and consequently the choice of the reference category. 

\section{Contrasts}


The information that decides what dummy variable will be created is dictated by the \textit{contrasts} for the factor variable. If we type

<<>>=
contrasts(penguins$species)
@

we see a matrix of 3 by 2 (3 rows, 2 columns). This is called a \textit{contrast matrix}. The two columns represent two contrasts for the \texttt{species} variable: one called Chinstrap (first column), and one called Gentoo (second column). These contrasts can be seen as the 'recipe' for dummy variables: the Chinstrap column is 1 for Chinstrap penguins (and 0 otherwise), and the Gentoo column is 1 for Gentoo penguins (and 0 otherwise). This information is used when actually creating the dummy variables for all penguins in the dataset: the dummy variables, based on the contrasts, are stored in the design matrix. Together with the intercept variable (all 1s), these dummy variables in the design matrix are used in the linear model for the factor variable \texttt{species}.  

The reference category will be by default the category that is coded internally as 1 in the factor. Often this is the one that comes first in the alphabet, but we saw that that does not have to be the case. 

Knowing that the recipe for the dummy coding is coded in the contrast matrix, can help us if we want to change the dummy coding. Suppose that we want the Gentoo penguins to be the reference category, and we want to estimate the difference with the Adelie and the Chinstrap separately. Then the default way of R would give only part of the information. In the output below we see that the difference between the Adelie and Gentoo penguins is somewhere between 1265 and 1486 grams, but the output does not tell us directly what the difference is between Gentoo and Chinstrap penguins. 


<<>>=
out %>% 
  tidy(conf.int = TRUE)
@


\section{Choosing the reference group yourself}

If we want the Gentoo species to be the reference group, instead of the Adelie penguins, there are two different approaches to do this. The most direct way is to specify that the Gentoo label is associated with the internal value 1. We can do that with the function \texttt{relevel()}. When you specify that the Gentoo category should be the reference category, R associates the internal value 1 of the factor with Gentoo, and the other two values with the remaining categories. 

<<>>=
penguins$species <- relevel(penguins$species, ref = "Gentoo")
levels(penguins$species) # Gentoo is now first category
@

If we now run the linear model, Gentoo is the reference category:

<<>>=
out1 <- penguins %>% 
  lm(body_mass_g ~ species, data = .)
out1 %>% 
  tidy(conf.int = TRUE)
@

Because we made the change of levels in the dataframe \texttt{penguins}, every time we use this dataframe again, the reference category for the species variable will be Gentoo. The default contrast matrix for this factor variable will be changed, 

<<>>=
contrasts(penguins$species)
@

and because the contrasts have changed, so will the dummy coding:

<<>>=
model.matrix(body_mass_g ~ species, data = penguins) %>% 
  tail(3)
@


There is an alternative method to change the reference category that is slightly more complicated, but can later help us to answer some interesting research questions. This alternative way leaves the levels of the factor variable alone, but changes the contrast matrix. 

Let's first put the levels of the factor variable in the original order:

<<>>=
penguins$species <- factor(penguins$species, levels = c("ZZz", "Chinstrap", "Gentoo"))
levels(penguins$species)
@

and change the label of "ZZz" back into "Adelie":

<<>>=
levels(penguins$species)[1] <- "Adelie"
levels(penguins$species)
@

If we let R specify contrasts, it always puts the category coded internally as 1 as the reference category. We see that when we look at the default contrast matrix


<<>>=
contrasts(penguins$species)
@

This type of combination of contrasts is called \textit{treatment contrasts}. It is specifically intended for data set-ups where the first group, seen as the reference group, is the 'control' group in an experiment, and the remaining groups are treatment groups. Then each treatment is compared to the control group. For example, suppose you do a medical research project where you have one group of patients receiving treatment 1, another group of patients receiving treatment 2, and you have a third group of patients that don't get any treatment (the control group). If you put the control group as level 1, an \texttt{lm} analysis, by default with the treatment contrasts, will yield two slope parameters: one estimating the difference between treatment 1 and control ("the effect of treatment 1"), and one parameter estimating the difference between treatment 2 and control ("the effect of treatment 2").

Now we are going to make our own contrasts, that specify that the Gentoo penguins are the reference category. The first contrast is (1, 0, 0), that specifies for the dummy coding that the first factor level (Adelie) should be dummy coded as 1 and the others as 0. The second contrast is (0, 1, 0), that specifies for the dummy coding that the second level of the factor (Chinstrap) should be coded as 1 and the others as 0. Let's use explanatory names for these two contrasts:

<<>>=
new_contrasts <- cbind("Adelie vs Gentoo" = c(1, 0, 0),
                       "Chinstrap vs Gentoo" = c(0, 1 ,0)) 
new_contrasts
@

Next we specify for a new analysis that we want to use this new contrast instead of the default one:

<<>>=
out2 <- penguins %>% 
  lm(body_mass_g ~ species, data = ., contrasts = list(species = new_contrasts))
out2 %>% tidy(conf.int = TRUE)
@

We see in the output that now Gentoo is the reference category, even though we haven't changed the data. The first slope parameter quantifies how much heavier the Adelie penguins are (compared to Gentoo), and the second slope parameter quantifies how much heavier the Chinstrap penguins are compared to Gentoo. We see that the Adelie penguins are -1377 grams heavier than Gentoo, which means they are 1377 grams \textit{lighter} than the Gentoo penguins. Similarly, we see that the Chinstrap penguins are 1343 grams \textit{lighter} than the Gentoo. We also now see the 95\% confidence intervals for these differences.


In short, the contrast matrix contains the recipe for the coding of new (dummy) variables. It tells the values for these new variables and for which categories these values should be used. These new variables are put into a model matrix or \textit{design matrix}.

The interesting thing is how this contrast matrix and these new (dummy) variables are used to estimate quantities of interest. For example, we see that the first slope parameter of the last analysis quantifies the difference in mean weight between on the one hand the Adelie penguins and on the other hand the Gentoo penguins. Actually it quantifies the difference between $\mu_{Adelie} - \mu_{Gentoo}$, where $\mu_{Adelie}$ is the population mean of the Adelie penguins and $\mu_{Gentoo}$ is the population mean of the Gentoo penguins. Let us call this quantity $q_1$. We could also write $q_1 = \mu_{Adelie}   - \mu_{Gentoo}$ as:




\begin{equation}
q_1 = 1 \times \mu_{Adelie} + 0\times \mu_{Chinstrap} - 1 \times \mu_{Gentoo} 
\end{equation}

following the order of the levels in the factor

<<>>=
levels(penguins$species)
@


Given the levels of a factor, we could summarise or code this quantity as the vector (1, 0, -1), for Adelie, Chinstrap and Gentoo penguins respectively.

We could do the same thing for the other quantity that we estimate with this model: the difference between Chinstrap and Adelie penguins, $\mu_{Chinstrap} - \mu_{Gentoo}$. Following the order of the factor levels, we can write this quantity $q_2$ as

\begin{equation}
q_2 = 0 \times \mu_{Adelie} + 1 \times \mu_{Chinstrap} - 1 \times \mu_{Gentoo} 
\end{equation}

which can be summarised as the vector (0, -1, +1)

The third quantity that we estimate with this model is the intercept. In this setting it represents the mean of the reference category, in this case Gentoo. The third quantity $q_3$ therefore estimates $\mu_{Gentoo}$. This can be written as 

\begin{equation}
q_3 = 0 \times \mu_{Adelie} + 0  \times \mu_{Chinstrap} + 1 \times \mu_{Gentoo} 
\end{equation}


which in turn can be simplified to the vector (0, 0, 1). 

We can combine these three vectors into a matrix, where each row is one vector. Let's use the vector for the intercept first:



<<message = F>>=



q3 <- c(0, 0, 1)
q1 <- c(+1, 0, -1)
q2 <- c(0, +1, -1)





@


If we multiply these vectors with the vector of population means, ($\mu_{Adelie}$, $\mu_{Chinstrap}$, $\mu_{Gentoo}$), we obtain the following 

\begin{gather}
\begin{pmatrix} 
0&0&1 \\ 
1&0&-1  \\ 
0&1&-1  
\end{pmatrix}
 \quad 
\begin{pmatrix} 
\mu_{Adelie} \\ 
\mu_{Chinstrap} \\ 
\mu_{Gentoo} 
\end{pmatrix}
  = 
\begin{pmatrix} 
\mu_{Gentoo} \\ 
\mu_{Adelie} - \mu_{Gentoo}\\ 
\mu_{Chinstrap} - \mu_{Gentoo} 
\end{pmatrix}
= 
\begin{pmatrix} 
\beta_{0} \\ 
\beta_1  \\ 
\beta_2
\end{pmatrix}
  \end{gather}
  
In other words, this matrix defines what quantities are represented by the regression coefficients: $\beta_0$ represents the population mean of Gentoo, $\beta_1$ represents the population mean of Adelie minus the population mean of Gentoo, and $\beta_2$ represents the population mean of Chinstrap minus the population mean of Gentoo. 

Thus, with this matrix we specify what we want to estimate with our model, by determining what the regression coefficients represent. 

And if we have specified these quantities, how does R know what dummy variables to create? It turns out that there is a nice trick: a special relationship between this quantity matrix $Q$ and the contrast matrix.

Let's start with the linear equation $Y = b_0 + b_1X_1 + b_2X_2 + e$, where $X_1$ and $X_2$ are the dummy variables that we don't know how to code yet. But we do know that the means of the three groups, $\widebar{Y}_{Adelie}$, $\widebar{Y}_{Chinstrap}$ and $\widebar{Y}_{Gentoo}$, are predicted by $b_0 + b_1X_1 + b_2X_2$, where $X_1$ and $X_2$ have different values for different groups. We can summarise these three equations using as the linear system:


\begin{eqnarray}
b_{0} + b_1X_{11} + b_2X_{21}&=&\widebar{Y}_{Adelie} \nonumber\\
b_{0} + b_1X_{12} + b_2X_{22}&=&\widebar{Y}_{Chinstrap} \nonumber\\
b_{0} + b_1X_{13} + b_2X_{23}&=&\widebar{Y}_{Gentoo} \nonumber\\
\end{eqnarray}

If we separate the regression parameters from the variables X, and realise that the group means are the estimates for the population means, we obtain:


\begin{gather}
\begin{pmatrix} 
1&X_1&X_2 \\ 
1&X_{11}&X_{11}  \\ 
1&X_{11}&X_{11}  
\end{pmatrix}
 \quad 
\begin{pmatrix} 
\beta_{0} \\ 
\beta_{1} \\ 
\beta_{2}  
\end{pmatrix} 
 =
\begin{pmatrix} 
\hat{\mu}_{Adelie} \\ 
\hat{\mu}_{Chinstrap} \\ 
\hat{\mu}_{Gentoo} 
\end{pmatrix}
  \end{gather}

If we substitute Equation \ref{eq:}, we get

\begin{gather}
\begin{pmatrix} 
1&X_1&X_2 \\ 
1&X_{11}&X_{11}  \\ 
1&X_{11}&X_{11}  
\end{pmatrix}
 \quad 
\begin{pmatrix} 
0&0&1 \\ 
1&0&-1  \\ 
0&1&-1  
\end{pmatrix}
 \quad 
\begin{pmatrix} 
\mu_{Adelie} \\ 
\mu_{Chinstrap} \\ 
\mu_{Gentoo} 
\end{pmatrix} 
=
\begin{pmatrix} 
\hat{\mu}_{Adelie} \\ 
\hat{\mu}_{Chinstrap} \\ 
\hat{\mu}_{Gentoo} 
\end{pmatrix}
  \end{gather}




<<>>=
Q <- rbind(q3, q1, q2) # combine vectors as rows in a matrix
Q


new_contrasts <- cbind("Adelie vs Gentoo" = c(1, 0, 0),
                       "Chinstrap vs Gentoo" = c(0, 1 ,0)) 


group_means <- penguins %>%
  dplyr::group_by(species) %>%
  dplyr::summarise(mean = mean(body_mass_g, na.rm = TRUE))

Q %*% as.matrix(group_means[,2])

lm(body_mass_g ~ species, data = penguins, contrasts = list(species = new_contrasts))

@


Next, we can use this new contrast matrix in our linear model. We tell R that we want to use a non-standard contrast for the categorical \texttt{species} variable. 


<<>>=
out2 <- penguins %>% 
  lm(body_mass_g ~ species, 
     data = ., 
     contrasts = list(species = new_contrasts))
out2 %>% 
  tidy(conf.int = TRUE)
@

In the output, we now see that the difference between species 1 (Adelie) and the reference category is somewhere between -1486 and -1265 (comparable to our earlier result where Adelie was the reference category). But what we now also have is a confidence interval for the contrast between Chinstrap and the Gentoo penguins: between -1480 and -1206 grams.

Although changing the reference category directly is easier, the method of using a pre-specified contrasts gives us the advantage that we can do more than simply changing the reference category. Sometimes research questions are not about differences between one group and another group, but between \textit{sets} of groups.



There are four steps when you want to test a specific research question about a quantity.

\begin{numerate}
\item   Define the quantities $q$ that you are looking for in terms of the population means. 

\item  Put these definitions in the shape of a Q matrix. Each row is a quantity.

\item  Calculate the contrasts by taking the inverse of the Q matrix. The contrasts are the columns of this matrix.

\item  Use this contrasts matrix in your linear model analysis. R will calculate the design matrix for you, based on this 'recipe'. 

\item In the output you will see the estimates for the quantities you are after.
\end{numerate}


Example. We have the factor species with levels Adelie, Chinstrap and Gentoo. We want to estimate the difference between the average weight of Adelie and Chinstrap penguins, and the average weight of the Gentoo penguins. 

The average weight of Gentoo penguins is $\mu_{Gentoo}$. The average weight of Adelie and Chinstrap combined is $\frac{\mu_{Adelie} + \mu_{Chinstrap}}{2} = \frac{1}{2} \mu_{Adelie} + \frac{1}{2} \mu_{Adelie}$. The difference with the Gentoo average equals the quantity we are after: $q_1 = \frac{1}{2} \mu_{Adelie} + \frac{1}{2} \mu_{Adelie}-\mu_{Gentoo}$. This can be coded as a vector (0.5, 0.5, -1). 

The second quantity we are after is the average weight of Gentoo penguins $\mu_{Gentoo}$: $q_2 = \mu_{Gentoo}$. This can be coded as the vector (0, 0, 1).

The third quantity might be the difference between Chinstrap and Adelie penguins: $q_3 =  \mu_{Adelie} -\mu_{Chinstrap}$. This can be coded as (1, -1, 0).

We compute Q by putting these vectors as rows in a matrix:

<<>>=
q1 <- c(0.5, 0.5, -1)
q2 <- c(0, 0, 1)
q3 <- c(1, -1, 0)
Q <- rbind(q1 = q1, q2 = q2, q3= q3)
Q
@


Then we take the inverse to make contrast matrix C:

<<>>=
C <- solve(Q)
C

cor(C[,-2]
@





\section{Planned comparisons}


Contrasts that you specify yourself can be used to ask specific research questions about \textit{planned comparisons}. Planned comparisons are comparisons between (sets of) groups that you were interested in before collecting and analysing the data. For example, because of what you suspect of these three species of penguins, you are interested in the average weight difference in Chinstrap and Gentoo penguins. In addition, you might be interested in the average weight difference between on the one hand Gentoo penguins and on the other hand Adelie and Chinstrap penguins. 

You collect the data and for your analysis of the data, you might consider a straightforward linear model analysis. But then you are dependent on what the software chooses for contrasts (and consequently what the reference group is). By default, R chooses the \textit{treatment contrast} (the one that you are familiar with, with the first group being the reference group and two dummy variables for the second and third category). Alternatively, you might consider an ANOVA, but that only gives you an answer to the question whether there is a significant difference between the three groups, which is clearly not of interest here. But based on what you suspected before collecting the data, you can set up a new contrast matrix, that will help you answer your specific research questions. For the first one, we can use the dummy coding, but for the second one, we need to find a different solution. 


package contrast

Suppose you have height data from three countries: Greece, Italy, and Norway. You might wish to know whether in these populations there is a difference in average height. If that is all you want to know, you can perform the linear model analysis described in earlier chapters. In that case, the null-hypothesis is
\\
\\
$H_0: \mu_{Greece}=\mu_{Italy}=\mu_{Norway}$
\\
\\
However, suppose your most important hypothesis is really much more specific: you only want to know whether the average height in Italy is different from the average height in Greece. The corresponding hypothesis would then be: 
\\
\\
$H_0: \mu_{Greece}=\mu_{Italy}$
\\
\\
In such cases, where the alternative hypothesis is more specific than simply stating "there are differences among the groups", then you should perform \textit{planned comparisons}. Here you would like to make a comparison between the average heights of Greece and Italy. You could also say you'd like to \textit{contrast} the average height of Greece with that of Italy. 

We then have to define this contrast in such a way that SPSS knows what we want. We could define our contrast in a similar vein as the null-hypothesis. Let's call the contrast $\gamma_1$.
\\
\\
$\gamma_1: \mu_{Greece}=\mu_{Italy}$
\\
\\

This contrast could also be written such that there is zero on the right-hand side of the equation, like this:
\\
\\
$\gamma_1: \mu_{Greece}-\mu_{Italy}=0$
\\
\\
This is the preferred way of specifying contrasts: having a zero on the right-hand side. And how about Norway? How do we add Norway into this contrast? Well, notice that you could also write the contrast like this:
\\
\\
$\gamma_1: (1)\times \mu_{Greece} + (-1) \times \mu_{Italy} + (0) \times \mu_{Norway} =0$
\\
\\
So we could code this specific contrast with the numbers preceding the group means: 1 for Greece, -1 for Italy and 0 for Norway. For statistical software therefore, to specify a contrast, we only need this coding. 


\section{Specifying a contrast in R}


This can be done in the following way. Let's use the \texttt{mpg} data set, where we have three types of cars: four-wheel drive, front-wheel drive and rear-wheel drive. As dependent variable we may use \texttt{cty}, city miles per gallon. The data are summarised in the box plot in Figure \ref{fig:fig14178}. Note that in this data set, the \textbf{drive} variable is coded "f" for front-wheel drive, "r" for rear-wheel drive and "4" for four-wheel drive. Then we could use the following code to ask for the specific comparison (or contrast) of the first and the second group, that is, "4" and "f", respectively (these groups are alphabetically first). So group 1 (= "4") gets a 1, group 2 (= "f") gets a -1, and group 3 (= "r") gets a 0, so our coded contrast looks like $(1, -1, 0)$.



<<fig1416, fig.height=4, echo=FALSE, fig.align='center', warning=FALSE, fig.cap="Data on height in three countries.">>=
set.seed(1234)
country <- rep(c('Italy', 'Greece', 'Norway'), 50)
NorthernEurope <- country=='Norway'
NorthernEurope <- as.integer(NorthernEurope)
height <- 165+ NorthernEurope*2 + rnorm(150, 0, 4)
data <- data.frame(country, NorthernEurope, height)
ggplot(data, aes(x=country, y=height)) + 
  geom_boxplot()
# source('/Users/stephanievandenberg/Dropbox/Statistiek_Onderwijs/Data Analysis/writeForeignCode.R')
# write.foreign(data, 
#               '/Users/stephanievandenberg/Dropbox/Statistiek_Onderwijs/Data Analysis/spss examples linear model/planned comparisons/planned_comparisons_anova.sav', 
#               '/Users/stephanievandenberg/Dropbox/Statistiek_Onderwijs/Data Analysis/spss examples linear model/planned comparisons/getdata.sps',
#               package = c("SPSS"))
@


<<fig14178, fig.height = 3.5>>=
mpg %>% 
  ggplot(aes(x = drv, y = cty)) +
  geom_boxplot()
@

<<>>=
contrasts(factor(mpg$drv)) <- c(0, 1, 0)
@




\begin{figure}[h]
    \begin{center}
       \includegraphics[scale=1,trim={0cm 11cm 0cm 0cm}, clip,width=0.9\linewidth]{/Users/stephanievandenberg/Dropbox/Statistiek_Onderwijs/Data" "Analysis/spss" "examples" "linear" "model/planned" "comparisons/contrast1.pdf}
    \end{center}
    \caption{Output of the contrast analysis on height data from three countries.}
    \label{fig:contrast1}
\end{figure}



In Figure \ref{fig:contrast1} we see the output. We see first that the general null-hypothesis that all three countries have the same average height is rejected, $F(2, 147)= 6.14, MSE=14.77, p = 0.003$. This test has 2 degrees of freedom, one for each of the two dummy variables that are needed to test this model. The error degrees of freedom equals 147: that is the number of data points (150) minus the number of parameters in the model: one for the intercept and two for the dummy variables, so $150-3=147$.





Next, we see the results for our \textit{specific} null-hypothesis (the contrast): that the means for Greece and Italy are equal, irrespective of Norway. We see a Contrast Estimate of 1.008 and a significance level of $p=0.19$. So the contrast is positive: what does this mean? Well, let's put the 1.008 into our contrast above:
\\
\\
$\gamma_1: (1)\times \mu_{Greece} + (-1) \times \mu_{Italy} + (0) \times \mu_{Norway} = 1.008$
\\
\\
which can be simplified to
\\
\\
$\gamma_1:  \mu_{Greece} - \mu_{Italy}  =  1.008$
\\
\\
So the 1.008 indicates that the average height in Italy is 1.008 cm shorter than in Greece, at least in our sample. The relatively high $p$-value indicates that the population means are however not different: we do not reject the null-hypothesis that the average heights are the same.

We also see an $F$-test for this contrast, $F(1, 147)=1.721, MSE= 14.76, p=0.19$.
Since the first degrees of freedom number is a 1, we know that $F$ is the same as a squared $t$, $t^2$, So, an equivalent presentation of the contrast effect would be $t(147)=1.312, p=0.19$.

Now perhaps you realize, why didn't we do a $t$-test in the first place? We might have taken the Greece and Italy data separately, used a dummy variable and then run an ordinary linear model. Well, let's see what would happen then.

First we let SPSS select only the Greece and Italy data with the SELECT IF syntax, and next run an UNIANOVA, using the BY keyword for \textbf{country} to indicate that we treat it as a categorical variable. We also let the selection be preceded by a TEMPORARY command, to indicate that the selection only applies to the analysis that follows.

\begin{verbatim}
TEMPORARY.
SELECT IF (country<3).
UNIANOVA height BY country
/DESIGN=country
/PRINT = PARAMETER.  
\end{verbatim}


In the output in Figure \ref{fig:simplettest} we see that now only the data from Greece and Italy are compared. From the Parameter Estimates table, we see that country 2 is the reference category, which is Italy. We also see that the average height in Greece is 1.008 cm taller than Italy. This effect is not significant, $t(98)=1.371,p=0.17$. We use 98 degrees of freedom because we have 100 people and loose 2 degrees of freedom, one for the intercept and once for the regression coefficient for country=1. Note that this number is also indicated in the Tests of Between-Subjects Effects table, as the df of the Error.

\begin{figure}[h]
    \begin{center}
       \includegraphics[scale=0.4]{/Users/stephanievandenberg/Dropbox/Statistiek_Onderwijs/Data" "Analysis/spss" "examples" "linear" "model/planned" "comparisons/simplettest.png}
    \end{center}
    \caption{Output of an analysis after selecting only the data from Greece and Italy.}
        \label{fig:simplettest}
\end{figure}

 

So what is different from the contrast effect? First of all, we see that the $p$-value is different: the $p$-value for the contrast effect analysis is somewhat larger than the one for the 'ordinary' $t$-test. Second, we see that the $t$-statistic is different: In the contrast analysis the $t$-value was 1.312 and in the ordinary analysis it was 1.371. So part of the reason that the $p$-value is different for the contrast is that the $t$-value is lower. Third, we see that the degrees of freedom has changed from 147 to 98. Well, we know that the significance level of a $t$-value depends on its size, the higher the $t$-value the lower the $p$-value, and we know that significance also depends on the degrees of freedom: the more degrees of freedom, the lower the $p$-value. 

So why has the $t$-value changed? Well, we know that the $t$-value is nothing but the regression coefficient divided by its standard error, $t=B/{SE}$. If we compare the two outputs, we see that the effect of $B$ is the same, the regression coefficient is equal to 1.008, and the contrast estimate in the Contrast Results (K-Matrix) is 1.008. So the sizes are the same. 

So the only difference between the two analyses can be the standard error. It turns out the standard error is computed in different ways: in the first contrast analysis, this SE is computed on the basis of \textit{all} the data, including the Norway data (150 people). We can see that from the degrees of freedom, the number of people minus 3. Why three? Because we also compute the variance of the Norway data, for which we have to estimate the mean (variance is the average squared difference from the mean). In the second analysis, we only used the data from 100 Greeks and Italians. The Norway data was not used at all, that's why the degrees of freedom is different and also the standard error for our $t$-statistic. 
\\
\\
Thus in summary: if we have a very specific hypothesis about the difference in means among two groups, it's better to use a contrast analysis, rather than a simple analysis regarding only those two groups. The reason for this is that \textit{more information is used}, even from groups for which we have no specific hypothesis. 
\\
Note that here we saw a higher $p$-value for the contrast analysis, but still, in general it is wiser to use as much data as possible, so we prefer a contrast analysis over a simple analysis excluding other groups.
\\
\\
The above example of comparing only the means of two groups, at the same time making use of the data in other groups, is called a \textit{simple contrast}. Now let's look at a \textit{complex contrast}. Suppose your hypothesis is that average height is different in Northern-European countries than in Southern-European countries, then you would like to know whether Italy \textit{and} Greece taken together differ regarding the average height from Norway. So we'd like to compare the average height in Norway to the average height in Greece and Italy together. We could write that null-hypothesis as follows:

\begin{eqnarray}
H_0 : \frac{\mu_{Greece}+ \mu_{Italy}}{2} = \mu_{Norway}
\end{eqnarray}


In other words, the mean height in Norway is equal to the \textit{average} mean height in the two other countries. If we want to test this hypothesis, we have to define a contrast. So we get the zero on the right-hand side in the following way:

\begin{eqnarray}
\gamma_2 : \frac{\mu_{Greece} + \mu_{Italy}} {2} - \mu_{Norway}  = 0 
\end{eqnarray}


This in turn we can write as 

\begin{eqnarray}
\gamma_2 : 0.5 \mu_{Greece} + 0.5 \mu_{Italy} - 1 \mu_{Norway}  = 0 
\end{eqnarray}


In SPSS, we can code this in the following way $(0.5 \ 0.5 -1)$\footnote{Note that for the SPSS  \textbf{country} variable, the first group is Greece, the second is Italy and the third is Norway. The contrast code should reflect the same order.}. We use the following syntax:

\begin{verbatim}
UNIANOVA height BY country
/DESIGN=country
/CONTRAST(country)=SPECIAL(0.5  0.5  -1).
\end{verbatim}


Now let's compare this analysis to an analysis where we use a dummy variable \textbf{NrthrnEr}, where we code 1s for Norwegians and 0s for Greeks and Italians. We then run an ordinary linear model on the data using this numerical dummy variable:


\begin{verbatim}
UNIANOVA height WITH NrthrnEr
/PRINT=PARAMETER
/DESIGN=NrthrnEr.
\end{verbatim}

From the contrast analysis we obtain $F(1,147)=10.56, MSE=14.77, p=0.001$, which is equivalent to $t(147)=3.25, p=0.001$. From the dummy variable analysis, we obtain $t(148)= 3.24, p=0.001$. So in this case, the results are very close: the degrees of freedom is larger for the dummy variable analysis, but the $t$-value is lower. All in all, we gain nothing much, and that is because in the dummy analysis we also use all of the data: we put the Greeks and Italians in one group (N=100), and the Norwegians in another group (N=50). The degrees of freedom differs because we only need to estimate 2 means in the dummy analysis, but we need to estimate 3 means in the contrast analysis.

In general: if you run a model where you compare various groups, AND you have very specific hypotheses that you'd like to test, it is generally advised to run contrast analyses (including a small number of them), and then report only the tests and $p$-values of those contrasts. Do not then also report the $p$-values of the parameters of your model. Actually, your contrasts are a re-specification of your model: either report the contrasts or the parameters, but not both, since they contain the same information. If you report too many $p$-values, the probability that you make a Type I error (concluding that you have a significant difference while there is really no difference) becomes too large. That is also the reason why you should report not more contrasts than the number of your parameters for your variable. For example, if you compare 5 groups, you will have 4 parameters for these groups. In that case specify no more than 4 contrasts. SPSS has a number of pre-specified sets of contrasts, like Helmert, Deviation, Difference, etcetera. Check out the SPSS manual for more details. If you want something more specific, use the SPECIAL option as indicated above. 

% Also very important: only go for contrasts when the overall ANOVA is significant. If the group means are not significantly different from eachother in a general sense, it is generally not advised to test specific contrasts.

\section{Testing more than one contrast}

In some cases you have a number of research hypotheses about group differences. For instance, you might have the a priori hypothesis that people in Northern countries are taller than in Southern countries, and another a priori hypothesis that people in Western countries are taller than in Eastern countries. So, a priori you have the hypothesis that the mean height in Norway is different from the mean heights in Greece and Italy combined. Second, you expect that the mean height in Italy is higher than in Greece.

You could test these two hypotheses at once in SPSS by specifying a $K$ matrix, like so:

\begin{verbatim}
UNIANOVA height BY country
/DESIGN=country
/CONTRAST(country)=SPECIAL(-0.5  -0.5  1
                            1    -1   0).
\end{verbatim}

Remember that the countries were coded like 1=Greece, 2=Italy, and 3= Norway. So the first null-hypothesis that is tested is that Norway has the same mean as the average of Greece and Italy. The second null-hypothesis is that Greece and Italy have the same mean.

This set of contrasts is said to be \textit{orthogonal}: whether or not we find a significant result for the first contrast has nothing to do with whether we find a significant result for the second contrast. Why this is the case can be seen from the $K$ matrix: if we take the first elements of the first and second row and multiply them we get $-0.5 \times 1 = -0.5$. If we take the second elements of the first and second row and multiply them we get $-0.5 \times -1 = 0.5$. If we take the third elements of the first and second row and multiply them we get $1 \times 0 = 0$. If we add these numbers we get $-0.5 + 0.5 +0= 0 $. Here we get a total of 0, which indicates that the contrasts are orthogonal, implying that the statistical results for contrast 1 and 2 are independent of each other. If the sum is unequal to 0, the contrasts are said to be \textit{dependent}. 
Here's an example of a non-orthogonal set of contrasts:

\begin{verbatim}
UNIANOVA height BY country
/DESIGN=country
/CONTRAST(country)=SPECIAL( 1    0   -1
                            1   -1   0).
\end{verbatim}

Here the sum of the products equals $1\times 1 +0\times -1 + -1 \times 0 =1$. This means that the set of statistical results is not independent of each other, so if the null-hypothesis if the first contrast is significant, this yields some information about the probability of obtaining a significant result for the second contrast. This you do not want, of course. So generally you would want to use independent sets of contrast. However, research questions are always more important: if you have good theoretical reasons to specify a set of non-orthogonal contrast, just go for it (Stevens, ).

As stated earlier, SPSS has a number of pre-specified sets of contrasts. One of them is the Helmert set of contrasts. In Helmert contrasts, the first group is contrasted with the average of all later groups, the second group is compared to the average of the later groups (ignoring group 1), the third group is compared with the average of the later groups (ignoring groups 1 and 2), etcetera. For a five country analysis, the syntax would be like


\begin{verbatim}
UNIANOVA height BY country
/DESIGN=country
/CONTRAST(country)=SPECIAL(1 -0.25 -0.25 -0.25 -0.25
                            0 1 -0.33 -0.33 -0.33
                            0 0 1 -0.5 -0.5
                            0 0 0 1 -1).
\end{verbatim}

and this is equivalent to the syntax 

\begin{verbatim}
UNIANOVA height BY country
/DESIGN=country
/CONTRAST(country)=HELMERT.
\end{verbatim}


This set of 4 contrasts is also completely orthogonal: all pairs of contrasts are orthogonal.





\section{Post-hoc comparisons}

In some cases, you compare 3 or more groups, and you find some interesting differences. For instance, in the above example, when you look at the box plot of the  differences between Greece, Italy and Norway, you might wonder whether there is a real difference in Italy and Greece. Or perhaps there is a difference between Norway and Italy, or even between Norway and Greece. There might be all kinds of interesting things to find out from these data.

In this case note, we now formulate these hypotheses \textit{after} looking at the differences in our data. In this case, suppose that there was no specific hypothesis before collecting our data, and we merely wanted to find out whether there are differences between mean heights across these countries. So, our null-hypothesis before looking at the data was that there were no differences in mean height across Greek, Italian and Norwegian populations. To test this, we perform a regular linear model analysis, with \textbf{height} as the dependent variable and a categorical variable \textbf{country} as independent variable. We want SPSS to make dummy variables automatically, so we use the following syntax using BY:

\begin{verbatim}
UNIANOVA height BY country
/PRINT=PARAMETER
/DESIGN=country.
\end{verbatim}



\begin{figure}[h]
    \begin{center}
       \includegraphics[scale=1,trim={0cm 23cm 0cm 0cm}, clip,width=0.9\linewidth]{/Users/stephanievandenberg/Dropbox/Statistiek_Onderwijs/Data" "Analysis/spss" "examples" "linear" "model/posthoc" "comparisons/Ftest}
    \end{center}
    \caption{Output of a regular linear model.}
    \label{fig:Ftest}
\end{figure}


From the output in Figure \ref{fig:Ftest} we use the $F$-test to test the overall hypothesis about equality of means, and report a significant difference between the three means, $F(2, 147)=6.14 , MSE=14.77, p<0.05$. Now \textit{given} that we have this rejection of the null-hypothesis, we might be very interested where this significance comes from: is it that Norway is very different from the other two countries? Perhaps there are no differences between Greece and Italy? Perhaps there is only a real difference between Norway
and Italy, but no real difference between Greece and Italy. And so on, and so forth. Note that here you could make 3 pair-wise simple comparisons: Greece vs Italy, Greece vs Norway and Italy vs Norway. You or anyone else interested in your research would like to know if these pair-wise differences are significant. In that case you can report so-called \textit{post hoc} pairwise comparisons. 

Note that we do not perform planned comparisons using contrasts here. Planned comparisons are very powerful tools that are only allowed for hypotheses that are specified \textit{a priori}, that is, \textit{before} doing any analysis, and preferably before any data collection. \textit{Post hoc} comparisons are done \textit{after} the fact: after testing the research hypothesis and after having looked at the data (seeing means or box plots!), you can test extra hypotheses that are of secondary interest. First we will show you how to do it, and second we will explain why we do it like that. 

\begin{verbatim}
UNIANOVA height BY country
/PRINT=PARAMETER
/EMMEANS=TABLES(country) compare(country) ADJ(BONFERRONI).
\end{verbatim}

So we state that we wish to see comparisons (\textit{compare}) for the variable \textbf{country} and that we want to make an adjustment (ADJ). In parentheses we indicate that the type of adjustment we want is \textit{Bonferroni}. This adjustment and Bonferroni we will explain below.

\begin{figure}[h]
    \begin{center}
       \includegraphics[scale=1,trim={0cm 22cm 0cm 2cm}, clip,width=0.9\linewidth]{/Users/stephanievandenberg/Dropbox/Statistiek_Onderwijs/Data" "Analysis/spss" "examples" "linear" "model/posthoc" "comparisons/Posthoctest}
    \end{center}
    \caption{Output of post hoc comparisons.}
        \label{fig:posthoctest}
\end{figure}

In the SPSS output we look for a table called Pairwise Comparisons, see Figure \ref{fig:posthoctest}. There we see $p$-values that are adjusted for multiple comparisons, using a Bonferroni correction. From Figure \ref{fig:posthoctest} we see that there are 6 comparisons, but by closer inspection we see that all three possible comparisons are reported twice. We see a significant difference between Italy and Norway, $p<0.002$, and that the other two comparisons are not significant. So average height in Greece is not different from the average height in Italy, $p>0.575$, nor from that in Norway, $p>0.097$.
Now note the difference from the Planned comparisons analysis. There we found a $p$-value of 0.19 for the hypothesis that Greece has the same average height as Italy. This contrast had a somewhat higher $p$-value than a simple dummy analysis for Greece and Italy, ignoring the Norwegian data, $p=0.17$. And now we see a much higher $p$-value of 0.58. The reason is that a correction has been applied to the $p$-values. This correction is needed because otherwise we too easily conclude that there are true differences between Greece and Italy.

Remember that the probability of a Type I error is very often chosen to be $5\%$. If we have one null-hypothesis that we want to test, the probability that we incorrectly conclude that there \textit{is} a difference in means (but there is really \textit{no} difference at population level!) is $5\%$. But suppose we have 10 hypotheses that we wish to test. Then what is the probability of finding at least 1 significant result while there is no difference? Well, it could be that our first hypothesis is falsely rejected, or our second, or our third, or perhaps even both our second and fourth hypothesis, and so on and so forth. With 10 hypotheses to test, there will be a high probability that \textit{at least} 1 will be falsely rejected. If \textit{each} hypothesis has a probability of a Type I error of 5\%, the probability that none of the hypotheses is falsely rejected equals $0.95^{10}=0.60$ (if we assume that all probabilities are independent). So the probability that at least one is falsely rejected is the complement of that, so $1-0.60=0.40$. If we carry out such research with 10 hypotheses, each using a significance level of $5\%$, we actually have a probability of 40\% of making at least one Type I error! That is awful, we don't want that. That's why in research, with a lot of hypotheses to be tested, we generally adjust the $p$-value in order to be more careful rejecting null-hypotheses. Theoretically, the $p$-value of our post hoc comparison of Greece and Italy should be equal to 0.19, corresponding to the simple dummy variable analysis ignoring Norway, but we report 0.575, because we also test two other hypotheses here. Actually, the $p$-value of the simple analysis of 0.19 is multiplied by the total number of tests, which is 3.


% \subsection{Posthoc tests for complex contrasts}



\section{Fishing expeditions}
The practice of testing a lot of hypotheses is often described as a fishing expedition. Just set out with large nets, throw them out, and catch whatever you can. In some extreme cases, in genetics for example, researchers test thousands or even millions of hypotheses on the basis of only one data set. Imagine that you collect height data on 70 countries and you want to know what countries differ from what other countries. The total number of pairs of countries equals 70 over 2, which is equal to 2415. So with 2415 $p$-values, what is the a priori probability of a significant result? If in reality there ARE no differences in means, and a fixed significance level of 0.05, 5\% of the $p$-values will be significant! So with such a data set, there will be at least $0.05 \times 2415 = 121$ significant $p$-values. At least, because there might be some true ones too. So in that scenario, it is impossible to know which $p$-values are to be trusted: many of them will involve false rejections.

For this reason, always be very specific about the null-hypothesis that you want to test with your data. If you have a very specific hypothesis about the differences in means, following a specific pattern, then always use a planned contrasts analysis. If after your analysis, there are some secondary hypotheses that you'd like to check (but for which you had no specific expectation) then report post hoc tests. The Bonferroni post hoc test is a good choice, as it is very conservative: it is very unlikely that you will falsely reject a hypothesis. Alternatively, there are some other post hoc tests, for further reading see the SPSS manual.

In general do a contrast analysis (planned comparisons) if:


\begin{itemize}
\item the overall test for the equality of all means is significant
\item the comparisons are chosen \textit{before} looking at the results (means, plots, statistical tests): they should be planned ahead!
\item the number of planned contrasts should not exceed the degrees of freedom, that is, the number of groups minus 1. 
\end{itemize}


Otherwise, do post hoc analyses, or better still, perform as few tests as possible! Only do post hoc tests if you are in an exploratory mood (you're not having a specific hypothesis but you would like to get some new ideas for future research) or when your supervisor asks for them.





% \section{Exercises}
% 
% You compare 4 groups. You'd like to know whether the averages observed in groups 1 and 2 differ from the averages observed in groups 3 and 4. 
% 
% \begin{itemize}
% \item State the null hypothesis
% \item Define the contrast
% \item Provide the SPSS syntax for this contrast
% \end{itemize}
% 
% answers: 
% \begin{itemize}
% \item $H_0: \frac{\mu_1 + \mu_2}{2} - \frac{\mu_3 + \mu_4}{2} = 0  $
% \item (0.5 0.5 -0.5 -0.5)
% \item \begin{verbatim}
% UNIANOVA height BY group
% /DESIGN=country
% /CONTRAST(group)=SPECIAL(0.5 0.5 -0.5 -0.5).
% \end{verbatim}
% \end{itemize}
% 
% 
% 
% 
% You compare 5 groups. You'd like to know whether the average observed in group 1 differs from the averages observed in groups 3, 4 and 5. 
% 
% \begin{itemize}
% \item State the null hypothesis
% \item Define the contrast
% \item Provide the SPSS syntax for this contrast
% \end{itemize}
% 
% answers: 
% \begin{itemize}
% \item $H_0: \frac{\mu_1}{1} - \frac{\mu_3 + \mu_4 + \mu_5}{3} = 0  $
% \item (1 0 -0.33 -0.33 -0.33)
% \item \begin{verbatim}
% UNIANOVA height BY group
% /DESIGN=group
% /CONTRAST(group)=SPECIAL((1 0 -0.33 -0.33 -0.33)).
% \end{verbatim}
% \end{itemize}
% 
% 
% A student has run the following SPSS syntax:
% 
% \begin{verbatim}
% UNIANOVA score BY school
% /DESIGN=school
% /CONTRAST(school)=SPECIAL((0 0 1 -0.5 -0.5)).
% \end{verbatim}
% 
% What null-hypothesis is tested using this syntax?
% \\
% answer:
% The hypothesis that the average score in school 3 is the same as the mean average score in schools 4 and 5
% or 
% $H_0: \mu_3 = \frac{\mu_4 + \mu_5}{2}$
% \\
% \\
% A student has tested the research hypothesis that height is different in the Benelux countries: The Netherlands, Belgium and Luxemburg, and finds a significant result. His supervisor asks then where the differences come from: is it that height is different in the Netherlands, or is it perhaps Luxemburg that deviates from the other two countries? She would like to have more specific information where the differences are between these three countries. What would you advise this student to do?
% \\
% \\
% answer: the supervisor does not seem to have any clearcut hypthesis about height differences in the Benelux countries. You therefore advise to carry a number of posthoc tests, that take into account the increase in the probability of a Type I error by adjusting $p$-values.
% \\
% \\
% A student has tested the research hypothesis that height is different in the Benelux countries: The Netherlands, Belgium and Luxemburg, and finds a significant result. His supervisor then says that the student is not finished yet. She would like to know whether the theory is correct that the larger the country, the taller the people. She would therefore like to know whether the average height in small country Luxemburg is different from the height averages in Belgium and The Netherlands.
% \\
% \\
% answer: the supervisor has a clearcut hypothesis about height differences in the Benelux countries. You therefore advise to carry out a planned comparison (a contrast analysis), that specifically tests the null hypothesis that the average in Luxemburg is the same as the mean average of Belgium and the Netherlands together.
% 
% 
% 
