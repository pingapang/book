
\documentclass[]{report}
\usepackage[english]{babel}
\usepackage{graphicx}





% Title Page
\title{Analyzing data using linear models}

\author{St\'ephanie van den Berg}
\date{Versie 0.1 \\ (\today)}



\begin{document}
\maketitle

<<libraries, echo=FALSE,  warning=FALSE, message=F>>=
library(ggplot2)
library(foreign)
library(dplyr)
library(lme4)
library(tidyr)
library(xtable)
library(scales)
library(DAAG)
@


\begin{abstract}
This book is intended to be of use to bachelor students in social sciences that want to learn how to analyze their data, with the specific aim to answer research questions. The book has a practical take on data analysis: how to do it, how to interpret the results, and how to report the results. All techniques are presented within the framework of linear models: this includes simple regression models, to linear mixed models, and generalized linear models. All methods can be carried out within one supermodel: the generalized linear mixed model. This approach is illustrated using SPSS.
\end{abstract}


\tableofcontents


\Sexpr{knit_child('chapter_1.Rnw')} % exploring your data, descriptive statistics

\chapter{Linear modelling: introduction FULYA}
\section{Linear relationships}
\section{Pearson correlation}
\section{Simple regression with a continuous predictor}
\section{Predicting the dependent variable}






\chapter{Inference I: random samples, standard errors and confidence intervals}

In the previous chapters on simple and multiple regression we have seen how a linear equation can describe a data set: the linear equation describes the behaviour of one variable, the dependent variable, on the basis of one other variable, the independent variable. Sometimes we are indeed interested in the relationship between two variables in one given data set. For instance, a teacher wants to know whether her exam gradings in her class of last year predict how well her students do in a second course a year later.

<<inf_0, fig.height=4, echo=FALSE, fig.align='center', fig.cap='The relationship between temperature and volume in a sample of 200 bottles.'>>=
set.seed(1234)
bottles <- data.frame(ID=1:800000,
                      volume= round(rnorm(800000, 30,1 ),2),
                      temperature=  round(runif(800000, 18,21 ),2)                 )
bottles1 <- bottles[sample(1:80000,200),]
out.sample <-  lm(volume~temperature, bottles1 )


bottles1  %>%  ggplot(aes(temperature, volume)) + geom_point() +xlim(c(17,22)) + geom_smooth(method="lm", se=F) + xlab("Temperature in degrees centigrade") + ylab("Volume in centiliters")
@


But very often, researchers are not interested in the relationships between variables in one data set, but interested in the relationship between variables in general, not limited to only the observed data. For example, a researcher would like to know what the relationship is between the temperature in a brewery and the volume of beer that goes into one bottle. In order to study the effect of temperature on volume, the researcher measures the volume of beer in 200 bottles at 20 degrees centigrade and determines from log files the temperature in the factory during production for each measured bottle. The researcher might find a small effect of temperature ($t$) on the volume of beer in the 200 produced bottles. The linear equation might be $volume = \Sexpr{out.sample$coef[1]} \Sexpr{out.sample$coef[2]} \times t + e$, see Figure \ref{fig:inf_0}. But the question is what the effect of temperature is in \textit{all} bottles produced in the same factory.



In other words, we might have data on a sample of bottles, but we might really be interested to know whether there is an effect \textit{had we been able to measure the volume in all bottles}.


\section{Population data and sample data}

In the beer bottle example above, the volume of beer was measured in a total of 200 bottles. Let's do a thought experiment. Suppose we could have access to volume data about all bottles of beer on all days where the factory was operating, including information about the temperature for each day of production. Suppose that the total number of bottles produced is 80,000 bottles. When we plot the volume of each bottle against the temperature of the factory we get the scatter plot in Figure \ref{fig:inf_1}.


<<inf_1, fig.height=4, echo=FALSE, fig.align='center', fig.cap='The relationship between temperature and volume in all 80,000 bottles.'>>=
out.population <- lm(volume~ temperature, bottles)
bottles[sample(1:80000,18000),]  %>%  ggplot(aes(temperature, volume)) + geom_point() +xlim(c(17,22)) + geom_smooth(method = "lm" , se=F)+ xlab("Temperature in degrees centigrade") + ylab("Volume in centiliters")

@


In our thought experiment, we could determine the regression equation using all bottles that were produced: all 80,000 of them. We then find the blue regression line displayed in Figure \ref{fig:inf_1}. Its equation is $Volume = \Sexpr{out.population$coef[1]} + \Sexpr{out.population$coef[2]} \times t$.


However, in the data example above, data was only collected on 200 bottles. These bottles were randomly selected: there were many more bottles but we could measure only a limited number of them. This explains why the regression equation based on the sample differed from the regression equation based on all bottles: we only see part of the data.

Here we see a discrepency between the regression equation based on the sample, and the regresssion equation based on the population. Here, the \textit{population} is the collection of all bottles produced in the factory. The \textit{sample} is the collection of 200 randomly selected bottles. Here we have a slope of \Sexpr{out.population$coef[2]} in the population, and we see a slope of \Sexpr{out.sample$coef[2]} in the sample. Also the intercepts differ. To distinguish between the coefficients of the population and coefficients of the sample, the population coefficient is often denoted by the Greek letter $\beta$ and the sample coefficient by the Roman letter $b$.



\begin{eqnarray}
Population: Volume &=& \Sexpr{out.population$coef[1]} + \Sexpr{out.population$coef[2]} \times t  \nonumber\\
Sample: Volume &=&  \Sexpr{out.sample$coef[1]}  \Sexpr{out.sample$coef[2]} \times t \nonumber
\end{eqnarray}

The discrependency between the two equations is simply the result of chance: had we selected another sample of 200 bottles, we probably would have found a different sample equation with a different slope and a different intercept. The intercept and slope based on sample data, are the result of chance. The population intercept and slope (the true ones) are fixed, but unknown. If we want to know something about the population intercept and slope, we only have the sample equation to go on. Our best guess for the population equation is the sample equation, but how certain can we be about how close the sample intercept and slope are to the population intercept and slope?


\section{Random sampling and the standard error}


In order to know how close the intercept and slope in a sample are to their values in the population, we do another thought experiment. Let's see what happens if we take more than one random sample of 200 bottlees. With random, we mean that every bottle has the same chance of being picked.

We put the 200 bottles that we selected earlier back into the population and we again blindly pick a new collection of 200 bottles. We then measure for each bottle the volume of beer it contains and we determine the temperature of the factory on the day of its production. We then apply a regression analysis and determine the intercept and the slope. Next, we put these bottles back into the population and draw a next random sample of 200 bottles.

You can probably imagine that if we repeat this procedure of randomly picking 200 bottles from a large population of 80,000, each time we find a different intercept and a different slope. Let's carry out this procedure 100 times by a computer. If we then plot the 100 sample intercepts and sample slopes we get the picture in Figure \ref{fig:inf_3}.



<<inf_3, fig.height=4, echo=FALSE, fig.align='center', message=F,fig.cap='Distribution of the sample mean when population variance is 225 and sample size equals 200.'>>=
set.seed(1234)
sample.intercept <- c()
sample.slope <- c()
for (i in 1:100)
{
        sample <- bottles[sample(1:80000,200),]
        out <- lm(volume~temperature, sample)
        sample.intercept[i] <- out$coef[1]
        sample.slope[i] <- out$coef[2]
}
data.frame(x=c(sample.intercept, sample.slope), fill=rep(c("intercept","slope"), each=100)) %>% ggplot(aes(x=x)) +geom_histogram(binwidth = 0.1) + facet_wrap(~fill)
@

We see a large variation in the intercepts that we find, and only a small variation in the slopes (all values very close to 0).


For now, let's focus on the slope; this because we are mostly interested to know whether there is a relationship between volume and temperature, but everything that follows also applies to the intercept. In Figure \ref{fig:inf_5} we see the histogram of the slopes if we carry out the random sampling 1000 times.

<<inf_5, fig.height=4, echo=FALSE, fig.align='center', message=F, fig.cap='Distribution of the sample mean when population variance is 225 and sample size equals 200.'>>=
set.seed(1234)
sample.intercept <- c()
sample.slope <- c()
for (i in 1:1000)
{
        sample <- bottles[sample(1:80000,200),]
        out <- lm(volume~temperature, sample)
        sample.intercept[i] <- out$coef[1]
        sample.slope[i] <- out$coef[2]
}
data.frame(sample.slope) %>% ggplot(aes(x=sample.slope)) +geom_histogram(binwidth = 0.01) + xlim(c(-0.3,0.3))
se<- sd(sample.slope)
@

If we look at the distribution of the 1000 sample slopes in Figure \ref{fig:inf_5}, we see that on average the sample slope is around $\Sexpr{out.population$coef[2]}$, which is the population slope (the slope if we analyse all bottles). But there is variation around that mean of 0: the standard deviation of all 1000 sample slopes turns out to be \Sexpr{se}.

The standard deviation of the sample mean is called the \textit{standard error}. Had the population slope been 110 or -40, the sample slopes would cluster around 110 or -40, but the standard deviation of the sample slopes, the standard error, would be the same.

The standard error for a sample slope represents the uncertainty about the population slope. If the standard error is large, it means that if we would draw many different random samples from the same population data, we would get very different sample slopes. If the standard error is small, it means that if we would draw many different random samples from the same population data, we would get sample slopes that are very close to one another, and very close to the population slope.


\subsection{Standard error and sample size}

It turns out that the standard error for a sample slope depends on many things, but the most important factor is the \textit{sample size}: how many bottles there were in each random sample. In the above example, the sample size is 200 bottles.

% In the above bottle example, the standard deviation of all 80,000 volumes was \Sexpr{sd(bottles$volume)}, where most of the volumes (roughly 95\%) lie between 28 and 32 cl. The variance is the square of the standard deviation so the variance is \Sexpr{var(bottles$volume)}. Now imagine that we have another population, say bottles from a different brand, where we see a much smaller variation in volumes: suppose the average volume is also 30, but the standard deviation is 0.5, so that roughly 95\% of the scores lie between 29 and 31. If we then take 1000 samples from this distribution of bottles from this other brand, we get the distribution in Figure \ref{fig:inf_6}.

% <<inf_6 ,fig.height=4, echo=FALSE, fig.align='center', fig.cap='Distribution of the sample mean when population variance is 25 and sample size equals 200.'>>=
% set.seed(1234)
% bottles <- data.frame(ID=1:800000,
%                       volume= round(rnorm(800000, 30, 0.5 ),2),
%                       temperature=  round(runif(800000, 18,21 ),2)                 )
% sample.intercept <- c()
% sample.slope <- c()
% for (i in 1:1000)
% {
%         sample <- bottles[sample(1:80000,200),]
%         out <- lm(volume~temperature, sample)
%         sample.intercept[i] <- out$coef[1]
%         sample.slope[i] <- out$coef[2]
% }
% data.frame(sample.slope) %>% ggplot(aes(x=sample.slope)) +geom_histogram(binwidth = 0.01) +  xlim(c(-0.3,0.3))
% @

% Now we see that the sample slopes cluster much closer around the value of 0. The standard deviation of this distribution, that is, the standard error, is now much smaller: \Sexpr{sd(sample.slope)}. This makes sense: the larger the variation at population level, the higher the probability that you find extreme values in your sample that influence the sample slope upwards or downwards. The smaller the variation at population level, the higher the proportion of data points in your sample that are very close to the population slope, so that the sample intercept will be very close to the population slope In sum: the higher the population variance, the larger the standard error, the larger the uncertainty about the population slope.

Imagine that you draw only 2 bottles from a population of bottles. Then there is quite some probability that by sheer luck you find one bottle with a low temperature and a small volume, and another bottle with a high temperature and a large volume. This would yield a sample slope that is quite large and positive. But there is also an equally high probability that you get one bottle with a low temperature with a large volume, and another bottle with a high temperature and a small volume. Then based on these two other bottles, the sample slope will be large and negative. In case of a sample size of only 2, you see that there will be quite a lot of variation in the sample slope if we draw various random samples. This large variation in sample slopes is then captured by the standard error, that will be large. With only 2 bottles per sample, the uncertainty about the population slope will then also be large.

Now imagine that your sample size is 20. Then the probability that the 20 bottles will result in a large variation of slopes will be smaller: it would be very unlikely that \textit{all} 20 bottles have either a high volume and a high temperature, or a low volume and a low temperature. If there happen to be a few of such bottles in the sample, the other bottles will average these effects out. Because of this averaging effect, the slope based on 20 bottles will then be closer to the population slope. The standard error therefore decreases with increasing sample size.

In Figure \ref{fig:inf_7} we see the distributions of the sample slope where the sample size is either 2 (left panel) or 20 (right panel). We see quite a lot of variation in sample slopes with sample size equal to 2, and considerably less variation in sample slopes if sample size is 20. This shows that the larger the sample size, the smaller the standard error, the larger the certainty about the population slope.


<<inf_7,fig.height=4, echo=FALSE, fig.align='center', warning=F, fig.cap='Distribution of the sample slope when sample size is 2 and when sample size is 20.'>>=

sample.slope <- c()
for (i in 1:1000)
{
        sample <- bottles[sample(1:80000,2),]
        out <- lm(volume~temperature, sample)

        sample.slope[i] <- out$coef[2]

}
for (i in 1001:2000)
{
        sample <- bottles[sample(1:80000,20),]
        out <- lm(volume~temperature, sample)

        sample.slope[i] <- out$coef[2]

}
panel <- rep(c("sample size 2","sample size 20"), each=1000)

data.frame(sample.slope, panel) %>% ggplot(aes(x=sample.slope)) + geom_histogram(binwidth =0.5)  + facet_wrap(~ panel) + xlim(c(-20,20)) + xlab("sample slope")
 @


\subsection{From sample slope to population slope}

So if we have a small standard error, we can be relatively certain that our sample slope is close to the population slope. Above we've done a thought experiment where we knew everything about the population intercept and slope, and we drew 1000 samples from this population. In reality, we don't know anything about the population: we only have the sample data to go on. So suppose we draw a sample of 200 from an unknown population of bottles, and we find a slope of 1, we have to look at the standard error to know how close that sample slope is to the population slope.

For example, suppose we find a sample slope of 1 and the standard error is equal to 0.1. Then we know that the population slope is more likely to be in the neighbourhood of values like 0.9, 1, or 1.1 than in the neighbourhood of 10 or -10.

Now suppose we find a sample slope of 1 and the standard error is equal to 10. Then we know that the sample slope is more likely to be somewhere in the neighbourhood of values like -9, 1 or 11, than around values in the neighbourhood of -100 or +100. However, values like -9, 1 and 11 are quite far apart, so actually we have no idea where the population slope is; we don't even know whether the population slope is positive or negative! The standard error is simply too large.


As we have seen, the standard error depends very much on sample size. Apart from sample size, the standard error for a slope also depends on the variance of the independent variable, the variance of the dependent variable, and the correlations between the independent variable and other independent variables in the equation (in case of multiple regression). We will not bore you with the complicated formula for the standard error for regression coefficients \footnote{See https://www3.nd.edu/~rwilliam/stats1/x91.pdf for the formula. In this pdf, 'IV' means independent variable}. Instead, we look at the standard error that SPSS or other computer packages compute for us.



% % \begin{equation}
% % \sigma_{\bar{y}} = \frac{\sigma}{\sqrt{n}}
% % \end{equation}
% %
% % where $\sigma$ is the population standard deviation and $n$ is sample size. Sample size we know, this is 100, but how about the population standard deviation? We don't know anything about the population, that's the whole reason that we took a sample. But we do know the standard deviation in the sample data. It turns out the \textit{sample standard deviation} $s$ is a rough approximation of the population variance. Therefore we often see the following formula for a standard error
% %
% % \begin{equation}
% % \sigma_{\bar{y}} = \frac{s}{\sqrt{n}}
% % \end{equation}
% %
% %
% % where $s$ represents an approximation of the population standard deviation using the sample data, more specifically the sums of squares (SS, see Chapter 1).
% % \begin{equation}
% % s = \sqrt{\frac{SS}{n-1}}
% % \end{equation}
% %
% % Note the similarity between the standard deviation of a particular set of values, $\sigma=\sqrt{\frac{SS}{n}}$ and the formula for $s$: if you're interested in the standard deviation for a specific set of values, then you use $\sigma=\sqrt{\frac{SS}{n}}$, if you're interested in the standard deviation of the population that a set of numbers is a random sample of, then you use $s=\sqrt{\frac{SS}{n-1}}$.\footnote{Confusingly, $s$ is often called the $sample standard deviation$, while it is really an approximation of the population standard deviation based on the sample data.}
% %
% % Suppose in the Paris data we find an $s$ of 12, then we know that the standard error is equal to $\frac{12}{\sqrt{100}}=\Sexpr{12/10}$.



\section{$t$-distributions}

Above we saw that if there is a large collection of data points (population) with a particular slope that describes the relationship between two variables, and if you then take random samples out of this collection, each time you find a different value for the slope in the sample, the sample slope. We saw that the standard deviation of the distribution of all such slopes is called the standard error. The standard error gives us information about how certain we can be that a slope in the sample is close to the slope in the population. The smaller the standard error the more certain that the population slope has a value that is in the neighboorhood of the value for the sample slope.



<<inf_8,fig.height=4, echo=FALSE, fig.align='center', fig.cap='Difference in the shapes of a normal distribution and a t-distribution'>>=
x=seq(-3,3,0.1)
data.frame(x)  %>%  ggplot(aes(x=x)) +
  stat_function(fun = dnorm, args = list(mean = 0, sd = 1), aes(colour = "normal"))  +
  stat_function(fun = dt, args = list( df= 3), aes(colour = "t")) +
  scale_color_manual("Distribution", values = c("blue", "red")) +
        ylab("density") + xlim(c(-5,5)) +xlab(" ")

@



When we look at the distribution of the sample slope, for instance in Figure \ref{fig:inf_5}, we notice that the distribution looks very much like a normal distribution. Well, actually it isn't quite a normal distribution. In reality it has the shape of a $t$-distribution. Figure \ref{fig:inf_8} shows the difference between a $t$-distribution (in red) and a normal distribution (in blue). In this figure, the means are equal (0) and the areas under the curve are equal (1), but the shapes are clearly different. Compared to the $t$-distribution, the normal distribution has more observed values close to the mean (the distribution is more peaked). The $t$-distribution has relatively more observations in the tails of the distribution (heavy tails).



Actually, the shape of the distribution of sample slopes depends on the size of the samples, the sample size. In Figure \ref{fig:inf_9} we see what the distribution would look like if all samples would be of size 4 (the red line) and what the distribution would like if sample size would be 200 (the blue line). Remember: we are talking here only about the \textit{shape} of the distribution. If sample size is large, like for instance 200 (the blue line), the shape looks extremely close to the normal distribution.


In summary, when we draw many samples from a population, the standard deviation of the sample slopes (the standard error) will be smaller. In addition, the shape of the distribution of sample slopes is that of a $t$-distribution. The shape of the $t$-distribution also depends on sample size. The larger the sample size, the more the shape of the $t$-distribution looks like a normal distribution. Thus, for large sample sizes, the distribution of sample slopes shows very little variance with a shape more closely resembling a normal distribution. 


<<inf_9,fig.height=4, echo=FALSE, fig.align='center', fig.cap='The shape of the distribution of sample slopes depends on sample size.'>>=
set.seed(1234)


data.frame(x)  %>%  ggplot(aes(x=x)) +
  stat_function(fun = dt, args = list(df=199), aes(colour = "sample size 200"))  +
  stat_function(fun = dt, args = list( df= 3), aes(colour = "sample size 4")) +
  #       stat_function(fun = dnorm, args = list( mean=0, sd=1.005708), aes(colour = "sample size 200")) +
  # stat_function(fun = dnorm, args = list( mean=0, sd=1.70675), aes(colour = "sample size 4")) +
  scale_color_manual("t-distribution", values = c("blue", "red")) +
        ylab("density") + xlim(c(-4,4))

# rt(10000, df=199) %>% sd()
@





\section{$T$-statistics}


Above we saw that sample slopes have a $t$-distribution, and that if sample size is large, say larger than 200, the $t$-distribution looks very much like a normal distribution. From the normal distribution, we know that if we standardize the scores by computing $z$-scores, that is, if we subtract the mean and then divide by the standard deviation, $z= \frac{x-\bar{x}}{\sigma}$, then 2.5\% of the $z$-values is smaller than -1.96 and 2.5\% of the $z$-values is larger than +1.96.


Therefore, if for large sample size the $t$-distribution is practically indistinguishable from the normal distribution, we know that if we standardize the sample slope values, we get a similar result. Instead of looking at the raw slope value, we can compute a standardized slope, let's call that standardized result $t$. Then we get:


\begin{equation}
t = \frac{b-\beta}{se}
\end{equation}

In words: we take a particular sample slope $b$ and we subtract the population slope $\beta$. The result we divide by the standard deviation of the sample slopes, which is callled the standard error $se$.


Let's go back to the example of the beer bottles. In our first random sample of 200 bottles, we found a sample slope of \Sexpr{out.sample$coef[2]}. We also happened to know the population slope, which was \Sexpr{out.population$coef[2]}. From our computer experiment, we saw that the standard deviation of the sample slopes with sample size 200 was equal to \Sexpr{se}. Thus, if we fill in the formula for the standardized slope $t$, we get for this particular sample


\begin{equation}
T = \frac{\Sexpr{out.sample$coef[2]}-\Sexpr{out.population$coef[2]}}{\Sexpr{se}}= \Sexpr{(out.sample$coef[2]-out.population$coef[2])  / se }
\end{equation}


Notice that we distinguish between a variable $t$ that has a $t$-distribution, and a $T$-statistic that is based on a computation.


Now, what can we say about this $T$-value? Since with a sample size of 200 the distribution closely resembles a normal distribution, we can use normal tables published online or in computer packages to see how likely a value of $T=\Sexpr{(out.sample$coef[2]-out.population$coef[2])  / se }$ actually is. In normal tables we find that a Z-value of $\Sexpr{(out.sample$coef[2]-out.population$coef[2])  / se }$ is not that strange: in the standard normal distribution, $\Sexpr{100*pnorm((out.sample$coef[2]-out.population$coef[2])  / se) }$\% of the values is smaller than $\Sexpr{(out.sample$coef[2]-out.population$coef[2])  / se }$. The area is shown in Figure \ref{fig:inf_9b}.


<<inf_9b,fig.height=4, echo=FALSE, fig.align='center', fig.cap='The standard normal distribution and the probability of a Z-score lower than -1.06'>>=

df = 198; ncp = 0; limits = c(-5,5)
lb=-20; ub=(out.sample$coef[2]-out.population$coef[2])  / se
    x <- seq(limits[1], limits[2], length.out = 100)
    xmin <- max(lb, limits[1])
    xmax <- min(ub, limits[2])
    areax <- seq(xmin, xmax, length.out = 100)
    area <- data.frame(x = areax, ymin = 0, ymax = dnorm(areax))
    (ggplot()
     + geom_line(data.frame(x = x, y = dnorm(x)),
                 mapping = aes(x = x, y = y))
     + geom_area(data = area, mapping = aes(x = x,  y = ymax))
     + scale_x_continuous(limits = limits, breaks=seq(-5,5,1))
     # + geom_area(data = area, mapping = aes(x = seq(qt(0.975, df=df, ncp=ncp), 5, length.out = 100),  y = dt(seq(qt(0.975, df=df, ncp=ncp), 5, length.out = 100), df=df, ncp=ncp)))
            + geom_vline(xintercept = (out.sample$coef[2]-out.population$coef[2])  / se )  + xlab("T") )
@


When would we say that a certain $T$-value would cause concern? Well, perhaps we could say that if the $T$-value we would find were 3 standard deviations away from the population value, either 3 standard deviations above the population value or 3 standard deviations below the population value. From the normal tables, we know that that happens in only $\Sexpr{2*100*pnorm(-3)}$\% of the time.

Alternatively, we could say that we would perhaps also be worried if the sample slope were 2 standard deviations away from the population slope, corresponding to $T$-value of 2 or -2. We know that the probabilty that that happens is around 5\%, small enough perhaps to raise concern about our knowledge about the population slope.

In this section, when discussing $T$-statistics, we assumed we knew the population slope, that is, the slope of the linear equation based on all 80,000 bottles. In reality, we never know the population slope: the whole reason to look at the sample slope is to have an idea about the population slope. Let's look at some hypothetical population slopes.





\section{Hypothetical population slopes}


Since we don't know the actual value of the population slope, we could ask the personnel in the beer factory what they think is a likely value for the slope. Suppose Mark says he believes that a slope of 2 could be true. Well, let's find out whether that is a reasonable guess. Now we \textit{assume} that the population slope $\beta$ is 2, and we compute the $T$-statistic for our sample slope:



\begin{equation}
T = \frac{\Sexpr{out.sample$coef[2]}-2}{\Sexpr{se}}= \Sexpr{(out.sample$coef[2]-2)  / se }
\end{equation}

From the normal distribution, we know that such a $T$-value is very unlikely: the probability of finding a sample slope 25 standard deviations away form a population slope of 2 is less than 0.00000000000000000000000000001. Because we know that such a $T$-value of 25 is unlikely, we know that a sample slope of \Sexpr{out.sample$coef[2]} is unlikely \textit{if the population slope is equal to 2}.


Now let's ask Martha. She thinks a reasonable value for the population slope is 0, as she doesn't believe there is a linear relationship between temperature and volume. She feels that the fact that we found a sample slope that was not 0 was a pure coincidence. Based on that hypothesis, we compute $T$ again and find:


\begin{equation}
T = \frac{\Sexpr{out.sample$coef[2]}-0}{\Sexpr{se}}= \Sexpr{(out.sample$coef[2])  / se }
\end{equation}

In other words, if we believe Martha, our sample slope is only about 1 standard deviation away from her hypothesized value. That's not a very bad idea, since from the normal distribution we know that a value more than 1.05 standard deviations away from the mean (above or below) is $\Sexpr{100 *round(2*pnorm((out.sample$coef[2]-0)  / se), 4)}$\%. In other words, if the population is truly 0, then our sample slope of $\Sexpr{out.sample$coef[2]}$ is quite a reasonable finding. If we reverse this line of reasoning: if our sample slope is $\Sexpr{out.sample$coef[2]}$, with a standard error of $\Sexpr{se}$, then a population slope of 0 is quite a reasonable guess! It is reasonable, since the difference between the sample slope and the hypothesised value is only $\Sexpr{(out.sample$coef[2])  / se }$ standard errors.

So when do we no longer feel that a value for the population slope is reasonable? Perhaps if the probability of finding a sample slope of a certain size given a certain population slope is so small that we no longer believe that the hypothesised value is reasonable. We might for example choose a small probability like 1\%. We know from the normal distribution that 1\% of the values lie at least $\Sexpr{round(qnorm(0.995),2)}$ standard deviations above and below the mean. So if our sample slope is more than $\Sexpr{round(qnorm(0.995),2)}$ standard errors away from the hypothesised population slope, then that population slope is \textit{not} a reasonable guess. In other words, if the \textit{distance} between the sample slope and the hypothesised population slope is more than 2.58 standard errors, then the hypothesised population slope is no longer reasonable.

This implies that \textit{any} value within the range of $\Sexpr{round(qnorm(0.995),2)}$ standard errors around the sample slope is a collection of reasonable values for the population slope.

Thus, in our example of the 200 bottles, a sample slope of $\Sexpr{out.sample$coef[2]}$ and a standard error of $\Sexpr{se}$, the interval from $\Sexpr{out.sample$coef[2]- 2.32* se}$ to $\Sexpr{out.sample$coef[2]+ 2.32* se}$ contains reasonable values for the population mean. If we would have to guess the value for the population slope, our guess would be that it would lie somewhere between between $\Sexpr{out.sample$coef[2]- 2.58* se}$ and $\Sexpr{out.sample$coef[2]+ 2.58* se}$, \textit{if we feel that 1\% is a small enough probability}.

In data analysis, such an interval that contains reasonable values for the population value, if we only know the sample value, is called a \textit{confidence interval}. Here we've chosen to use $\Sexpr{qnorm(0.995)}$ standard deviations as our cut-off point, because we felt that 1\% would be a small enough probability to dismiss a population value as a reasonable candidate. Such a confidence interval based on this 1\% cut-off point is called a 99\% confidence interval.

One often also sees 95\% confidence intervals, particularly in social and behavioural sciences. Because with the normal distribution, 5\% of the observations lie more than 1.96 standard deviations away from the mean, the 95\% confidence interval is constructed by subtracting/addding 1.96 standard errors from/to the sample value. Thus, in the case of our bottle sample, the 95\% confidence interval for the population slope is from $\Sexpr{out.sample$coef[2]}- 1.96* \Sexpr{se}$ to $\Sexpr{out.sample$coef[2]}+ 1.96* \Sexpr{se}$, so reasonable values for the population slope are those values between $\Sexpr{out.sample$coef[2]- 1.96* se}$ and $\Sexpr{out.sample$coef[2]+ 1.96* se}$. Luckily, this corresponds to the truth, because we happen to know that the population slope is equal to \Sexpr{out.population$coef[2]}. In real life we don't know the population slope and of course it might happen that the true population value is not within the 95\% confidence interval. If you want to make the probability of this being the case smaller, then you can use a 99\% or even a 99.9\% or larger interval. 


\section{Confidence intervals for smaller sample sizes}

In the previous section we used the normal distribution to come up with 95\% and 99\% confidence intervals for the slope coefficient. These were constructed using 1.96 and $\Sexpr{round(qnorm(0.995),2)}$ times the standard error, respectively. However, these numbers 1.96 and $\Sexpr{round(qnorm(0.995),2)}$ can only be used when the sample size is large enough to say that the distribution of the sample slope is very close to a normal distribution. Earlier, we saw that the distribution of the sample slope is actually a $t$-distribution, that doesn't look normal at all for small sample sizes.

So for small sample sizes, we need to know the cut-off points that correspond to 5\% and 1\% probabilities for the $t$-distribution.








% In data analysis, one often uses a \textit{confidence interval} to indicate a range of reasonable values for the population value. Here we found a sample slope of 112. Now imagine that 112 were also the population slope. Then if we would draw many random samples of size 100, we know from the computed standard error of \Sexpr{12/10} that roughly 95\% of the sample means would lie between $112 - 2 \times \Sexpr{12/10} = 109.6$ and $112 + 2 \times \Sexpr{12/10} = 114.4$.

% Now suppose that the true population mean were not 112 but 114.4. In that case, if we draw many samples of size 100, we could reasonably find a value of 112, since 95\% of the sample mean would then lie between $114.4 - 2 \times \Sexpr{12/10} = 112$ and $114.4 + 2 \times \Sexpr{12/10} = 116.8$. So even if the true population mean were 112.4, it's very possible that we could find a sample slope of ?. We cannot neglect the possiblity that the true slope is 114.4. Similarly, we cannot neglect the possibility that the true slope is 109.6, because if the true mean were 109.6, 95\% of the sample means of size 100 would lie between $109.6 - 2 \times \Sexpr{12/10} = 107.2$ and $109.6 + 2 \times \Sexpr{12/10} = 112$. So our range of reasonable values for the population slope would be somewhere between 107.2 and 114.4. This range is referred to as the \textit{95\% confidence interval}. The 95\% confidence interval can be computed by subtracting and adding twice the standard error of the mean to the sample mean.


For large sample sizes we can approximate the $t$-distribution by a normal distribution so that we know that 95\% of the observations lie between -1.96 and +1.96 times the standard deviation. For small sample sizes we have to use a $t$-distribution to construct confidence intervals. For small sample sizes, we need to know the particular shape of the distribution to find out where the middle 95\% of the sample means lie.

Figure \ref{fig:inf_10} shows the case for the situation where the population slope is 0 and the sample size is 4. Suppose the standard error is equal to 1. Then this figure shows that roughly 95\% of the sample slopes lie between $\pm$ 3.18 standard errors below and above the mean (the red lines). In the same figure we also see that if sample size is 200, 95\% of the sample means lie between $\pm$ 1.97 standard errors below and above the mean (the blue line). This is almost the same as for the normal distribution, where 95\% of the observations lie between $\pm$ 1.96 standard deviations below and above the mean.

Because for every sample size, the middle region where 95\% of the observations lie is different, there are tables available where these values can be found. However, these tables are built-in in every statistical package, so it is far easier to let SPSS construct the 95\% confidence intervals for us.


<<table_inf_1, fig.height=4, echo=FALSE, fig.align='center',results='asis' >>=
probs <- c(0.0005, 0.001, 0.005,0.01, 0.025, 0.05, 0.10, 0.90, 0.95, 0.975,0.99, 0.995,0.999, 0.9995)
    norm <- qnorm(probs)
    t198 <- qt(probs, df=198)
    t100 <- qt(probs, df=100)
    t50 <- qt(probs, df=50)
    t10 <- qt(probs, df=10)
    t2 <- qt(probs, df=2)
    data.frame(probs, norm, t198, t100, t50, t10, t2) %>%
        xtable(caption="Quantiles for the normal and several t-distributions.", label="tab:nonparmixed_4", digits=c(0,4,2,2,2,2,2,2)) %>%
        print(include.rownames=F, caption.placement = "top")
 @
But let us look at a few regularities. For several probabilities, the corresponding quantiles are presented in Table \ref{tab:nonparmixed_4} for the standard normal distribution and several $t$-distributions.

The shape of the $t$-distribution is indicated by its \textit{degrees of freedom}. The shape of the distribution of sample slopes when sample size is 200, is a $t$-distribution with 198 degrees of freedom. The shape of the distribution of sample slopes when sample size is 4, is a $t$-distribution with 2 degrees of freedom. In general, the shape of the distribution of sample slopes for sample size $n$, is a $t$-distribution with $n-2$ degrees of freedom. The higher the degrees of freedom, the more the corresponding $t$-distribution looks like a normal distribution. We will come back to degrees of freedom and the $n-2$ rule in the next section.

Table \ref{tab:nonparmixed_4} shows for instance the cut-off points for 2.5\% and 97.5\% for the normal distribution and the $t$-distribution with 198 degrees of freedom: 1.96 and 1.97 standard deviations (standard errors) respectively. For the $t$-distribution with 100 degrees of freedom, the cutoff point is 1.98 standard errors. This would be the appropriate $t$-distribution for a sample size of 102. But for smaller sample sizes, the increase in number of standard errors goes up quickly: with 50 degrees of freedom (sample size 52), the cutoff is 2.01, for 10 degrees of freedom it is 2.23 and for 2 degrees of freedom it becomes even 4.30 standard errors. Thus, if we have a sample size of 4, we construct a 95\% confidence interval of 4.30 standard errors below the sample slope and 4.30 standard errors above the sample slope.

If you want to have the 99\% confidence interval, you look at the cutoff points for 0.005 and 0.995 which are -2.58 and +2.58, respectively, for the normal distribution, but -9.92 and +9.92 for a $t$-distribution with 2 degrees of freedom. Suppose we sample 4 bottles and find a sample slope of 5 with a standard error of 4, then the 99\% confidence for the slope is from $5-9.92\times 4$ to $5+9.92\times 4$, so from -34.68 to 44.68, which is of course a huge interval. On the other hand, a sample of only 4 bottles is of course very small.

In short, we can look up the cutoff points for 95\%, 99\% and other intervals from tables online, in books, or in statistical packages. Generally, the smaller the sample size, the lower the degrees of freedom, the larger the number of standard errors you need to construct your confidence intervals.


<<inf_10,fig.height=4, echo=FALSE, fig.align='center', fig.cap='Two t-distributions when sample size is 4 or 200, with corresponding 95 percent intervals.'>>=
set.seed(1234)


data.frame(x)  %>%  ggplot(aes(x=x)) +
  stat_function(fun = dt, args = list(df=199), aes(colour = "sample size 200"))  + geom_vline(xintercept=qt(c(0.025, 0.975), df=3), colour="red") +
  stat_function(fun = dt, args = list( df= 3), aes(colour = "sample size 4")) +  geom_vline(xintercept=qt(c(0.025, 0.975), df=199), colour="blue") +
  scale_color_manual("Distribution", values = c("blue", "red")) +
        ylab("density") + xlim(c(-7,7))

# rt(10000, df=3) %>% sd()
@




\subsection{Exercises}

\begin{enumerate}


\item Suppose we randomly pick 102 students from the University of Twente and determine the linear equation between age in years (independent variable) and height in cms (dependent variable). Suppose we find a slope coefficient of 0.010, with a standard error of 0.009. Construct the 95\% confidence interval for the slope in the entire population in UT students using table \ref{tab:nonparmixed_4}.

\item What can we say about values within this constructed confidence interval?

\item Suppose a professor believes the true slope is equal to 0: is that a reasonable belief given the finding of a sample slope of 0.010? Motivate your answer using the 95\% confidence interval.


\item Suppose we randomly pick 52 adult inhabitants of Tuvalu and determine the linear equation between age in years (independent variable) and height in cms (dependent variable). Suppose we find an intercept of 168, with a standard error of 0.07. Construct the 99\% confidence interval for the intercept in the entire population of adult inhabitants of Tuvalu using table \ref{tab:nonparmixed_4}.

\item What can we say about values within this constructed confidence interval?

\item Suppose a Swedish diplomat stationed in Tuvalu believes the population intercept is equal to 169 cm: is that a reasonable belief given the finding of a sample intercept of 168? Motivate your answer using the 99\% confidence interval.

\end{enumerate}


Answers:

\begin{enumerate}

\item Sample size is 102, so degrees of freedom for the sample slope is 100. For a 95\% interval, 2.5\% of the observations should be on the left, and 2.5\% of the observations should be on the right. The cut-off quantiles should therefore be 0.025 and 0.975. These cut-off values for the $t$-distribution with 100 degrees of freedom are -1.98 and 1.98. Therefore the 95\% interval ranges from $0.010 - 1.98 \times 0.009$ to $0.010 + 1.98 \times 0.009$, so from -0.008 to 0.028.

\item These values are all reasonable values for the slope in the population of University of Twente students.

\item Yes, the value of 0 lies within the range from -0.008 to 0.028, so 0 is a reasonable value for the population slope.

\item Sample size is 52, so degrees of freedom for the sample slope is 50. For a 99\% interval, 0.5\% of the observations should be on the left, and 0.5\% of the observations should be on the right. The cut-off quantiles should therefore be 0.005 and 0.995. The 99\% cut-off values for the $t$-distribution with 100 degrees of freedom are therefore -2.68 and 2.68. Thus, the 99\% interval ranges from $168 - 2.68 \times 0.07$ to $168 + 2.68 \times 0.07$, so from 167.8124 to 168.1876.

\item These values are all reasonable values for the slope in the population of all adult inhabitants of Tuvalu.

\item No, the value of 169 does not lie within the range from 167.8124 to 168.1876, so 169 is not a reasonable value for the population intercept.


\end{enumerate}
%

\section{Degrees of freedom}


What does the term, "degrees of freedom" mean? It refers to the number of independent pieces of information in a sample of data.

Suppose that we have a sample with four values {4, 2, 6, 8}. There are four separate pieces of information here. There is no particular connection between these values. They are free to take any values, in principle. We could say that there are “four degrees of freedom” associated with this sample of data.

Now, suppose that I tell you that three of the values in the sample are 4, 2, and 6; and I also tell you that the sample average is 5. You can immediately deduce that the fourth value has to be 8. There is no other logical possibility.

So, once I tell you that the sample average is 5, I am effectively introducing a \textit{constraint}. The value of the unknown fourth sample value is implicitly being determined from the other three values, and the constraint. That is, once the constraint is introduced, there are only three logically independent pieces of information in the sample. That is to say, there are only three "degrees of freedom", once the sample average is revealed.

Now let's carry this example to regression analysis. Suppose I have four observations of variables $x$ and $y$. Each value of $y$ is one piece of information. These values could be anything, so we say that we have 4 degrees of freedom. Now suppose I use a linear equation for these data points, and suppose I only use an intercept. Let the intercept be 5. Now the first bit of information, $x$ and $y$ could be anything, say 1 and 2 respectively. The second and third bits of information could also be anything, say 2 and 6, and 4 and 2. Figure \ref{fig:inf_11} shows these bits of information as dots in a scatterplot. Since we know that the intercept is equal to 5, with no slope (slope=0), we can also draw the regression line.

<<inf_11,fig.height=4, echo=FALSE, fig.align='center', fig.cap='Distribution of the sample mean when sample size is 4 or 200'>>=

x <- c(1, 2, 4)
y<- c(2, 6, 2 )
data.frame(x, y) %>% ggplot(aes(x, y)) + geom_point() + geom_hline(yintercept=5)
@

If we compute the residuals, we have residuals -3, 1 and -3 for these data points. When we sum them we get -3. Since we know that all residuals should sum to 0 in a regression analysis, we can derive the fourth residual to be +5, since only then the residuals sum to 0. Therefore, the $y$-value for the fourth data point (for $x=3$, for example) has to be 8, since then the residual is equal to $8-5=3$.

In short, when we do a regression analysis with only an intercept, the degrees of freedom is equal to the number of data points (combinations of $x$ and $y$) minus 1, or in short notation: $n-1$.

Now let's look at the situation where we do a regression analysis with both an intercept and a slope: suppose the intercept is equal to 3 and the slope is equal to 1: $y=3+1 x$. Then suppose we have the same $x$-values as the example above: 1, 2 and 4. When we give these $x$-values corresponding $y$-values, 2, 6, and 3, we get the plot in Figure \ref{fig:inf_12}.

<<inf_12,fig.height=4, echo=FALSE, fig.align='center', fig.cap='Distribution of the sample mean when sample size is 4 or 200'>>=
x <- c(1, 2 ,4)
y<- c(2, 6 ,3)
data.frame(x, y) %>% ggplot(aes(x, y)) + geom_point() + geom_abline(intercept=3, slope=1, size=2) + geom_smooth(method='lm', se=F)+
        ylim(c(0,12.5))
# X<- matrix(c(1,1,1,1,1,2,3,4),4,2)
# XX=(t(X)%*%X)^(-1)
# M= diag(4) -  X %*% XX %*% t(X)
# y_star <- c(2, 6 ,8,3)
# M%*% y_star
@

The black line is the regression line that should be imposed on the data. The blue line is the regression line based on the three data points. Now the question is, is it possible for a fourth data point with $x=3$, to think of a $y$-value such that the regression line based on these four data points is equal to $y=3+1x$?

Figure \ref{fig:inf_13} shows a number of possibilities for the value of $y$ if $x=3$. It can be seen, that it is impossible to pick a value for $y$ such that we get a regression equation $y=3+1x$.

<<inf_13,fig.height=4, echo=FALSE, fig.align='center', fig.cap='Different regression lines for different values of y if x=3.'>>=
x <- rep(c(1, 2 ,3, 4),4)
fill <- c(8,9.5, 11, 12)
y<- rep (c(2, 6 ,0,3),4)

y [seq(3,16, 4) ]<- fill
line = rep(1:4, each=4) %>% as.factor()
data.frame(x, y,line) %>% ggplot(aes(x, y)) + geom_point(aes(col=line)) + geom_abline(intercept=3, slope=1, size=2) + geom_smooth(aes(col=line),method='lm', se=F) +
        ylim(c(0,12.5))
@

So, with 4 data points, we can never freely choose 3 residuals in order to satisfy the constraint that a particular regression equation holds. It turns out, that in this case we can only choose 2 residuals freely, and the remaining residuals are already determined. To prove this requires matrix algebra, but the gist of it is that if you have a regression equation with both an intercept and a slope, the degrees of freedom is equal to the number of data points minus 2: $n-2$.

Generally, these degrees of freedom based on the number of residuals that could be freely chosen, given the constraints of the model, are termed \textit{residual degrees of freedom}. When using regression models, one usually only reports these residual degrees of freedom. Later on in this book, we will see instances where one also should use \textit{model degrees of freedom}. For now, it suffices to know what is meant by residuals degrees of freedom.  





\chapter{Inference II: hypothesis testing, $p$-values and beyond}

\section{The null-hypothesis}

Often data analysis is about finding an answer to the question whether there is a relationship between two variables. In most cases, the question pertains to the population: is there a relationship between variable $y$ and variable $x$ in the population? In many cases, one looks for a linear relationship between two variables.

One common method to answer this question is to analyse a sample of data, apply a linear model, and look at the slope. However, one then knows the slope in the sample, but not the slope in the population. We have seen that the slope in the sample can be very different from the slope in the population. Suppose we find a slope of 1: does that mean there is a slope in the population or that there is no slope in the population?

In inferential data analysis, one often works with two hypotheses: the null-hypothesis and the alternative hypothesis. The null-hypothesis states that the population slope is equal to 0 and the alternative hypothesis states that there is a slope that is different from 0. Remember that if the population slope is equal to 0, that is saying that there is no linear relationship between $x$ and $y$. Therefore, the null-hypothesis states there is no linear relationship between $x$ and $y$ in the population. If there is a slope, whether positive or negative, is the same as saying there is a linear relationship, so the alternative hypothesis states there that is a linear relationship between $x$ and $y$ in the population. The null-hypothesis is often denoted as $H_0$ and the alternative hypothesis is often denoted as $H_1$. In formula form, we have


\begin{eqnarray}
H_0: \beta_{slope}=0 \\
H_1: \beta_{slope} \neq 0
\end{eqnarray}

So the population slope, $\beta_{slope}$, is either 0 or it is not. Our data analysis is then aimed at determining which of these two hypotheses is true. Key is that we do a thought experiment on the null-hypothesis: we wonder what would happen if the population slope would be really 0. In our imagination we draw many samples of a certain size, say 40 data points, and then determine the slope for each sample. Earlier we learned that the many sample slopes would form a histogram in the shape of a $t$-distribution with $n-2=38$ degrees of freedom. For example, suppose we would draw 1000 samples of size 40, then the histogram of the 1000 slopes would be like depicted in Figure \ref{fig:inf_14}

<<inf_14,fig.height=4, echo=FALSE, fig.align='center', fig.cap='Distribution of the sample mean when population variance is 25 and sample size equals 200.'>>=
# sample.intercept <- c()
sample.slope <- c()
for (i in 1:1000)
{
        sample <- bottles[sample(1:80000,40),]
        out <- lm(volume~temperature, sample)
        # sample.intercept[i] <- out$coef[1]
        sample.slope[i] <- out$coef[2]

}

data.frame(sample.slope, panel) %>% ggplot(aes(x=sample.slope)) + geom_histogram(binwidth =0.4*0.19)  + scale_x_continuous(limits=c(-1,1),breaks=seq(-1,1,.1))
 @

From this histogram we see that all observed sample slopes are well between -0.8 and 0.8. This gives us the information we need. Of course, we have only one sample of data, and we don't know anything about the population data. But we \textit{do} know that \textit{if the population slope is equal to 0}, then it is very unlikely to find a sample slope of say 1 or -1. Thus, if we happen to find a sample slope of say -1, we know that this finding is very unlikely \textit{if we hold the null-hypothesis to be true}. In other words, if the population slope is equal to 0, it would be quite improbable to find a sample slope of -1. Therefore, we regard the null-hypothesis to be false, since it does not provide a good explanation of why we found a slope of -1. In that case, we say that \textit{we reject the null-hypothesis}.

% \section{$T$-statistics}
%
% In previous sections we looked at the distribution of the slope based on sample data, if you draw many random samples from a population of data points. We saw that nearly always, the slope based on your sample data is different from the slope in the population data. We learned that the shape of the distribution is that of a $t$-distribution. The particular shape of the distribution depends on the degrees of freedom, and we learned that the degrees of freedom is equal to the number of data points in your sample (sample size $n$) minus the number of parameters/coefficients in your linear equation.
%
% We said that the distribution of the regression slope has the \textit{shape} of a $t$-distribution. In order to get a $t$-distribution, you have to standardize the scores. Similar to standardizing other scores to z-scores, by subtracting the mean and dividing by the standard deviation, we too standardize slope estimates into T-scores.
%
% Similar to the normal distribution. If you know a value is 2, then you know nothing, but if you know that a value is 20 standard deviations away from the mean, that is, a $z$-score of 20, then you know that such a value is rather unlikely.
%
% The same is true for the distribution of sample slopes. Only knowing that the sample slope is 1, says nothing, but that the slope is 30 standard errors away from a particular value is saying that such a value is unlikely.
%
% So similar to $z$-scores, we subtract the mean from the sample slope and divide by the standard deviation. If the null-hypothesis is true, the mean of the sample slopes is 0. We also know that the standard deviation of the sample slopes is the standard error.
%
% This standardized slope is called a $T$-statistic. A statistic is a quantity that is based on a calculation using your sample data. For example, using least squares, you determine the slope parameter $b_{slope}$, and you determine the standard error $se$. Next, you compute the $T$-statistic:
%
% \begin{equation}
% T = \frac{b_{slope}-0}{se} = \frac{b_{slope}}{se}
% \end{equation}
%
%
% Figure \ref{fig:inf_15} shows the $t$-distribution with $40-2=38$ degrees of freedom. This is the distribution for the $T$-statistic if our sample size is equal to 40 and the true population slope is equal to 0.
%
%
% <<inf_15,fig.height=4, echo=FALSE, fig.align='center', fig.cap='Different regression lines for different values of y if x=3.'>>=
% data.frame(x)  %>%  ggplot(aes(x=x)) +
%   stat_function(fun = dt, args = list(df=38))  +
%         ylab("density") + xlim(c(-4,4)) + xlab("T")
% @
%
% Suppose the true value of the slope (the slope in the population data) is equal to exactly 0. Then if you analyse a sample of 40 data points, you might find a slope of $b_{slope}=1$, and the standard error turns out to be 2. If we then compute $T$, we get $T=\frac{b_{slope}}{se}=1/2=0.5$. In other words, our slope is half a standard error away from the hypothesised value of 0. Whether this is a lot, depends on the shape of the distribution. For this $T$ we know that it has a $t$-distribution with 38 degrees of freedom. Figure \ref{fig:inf_16} shows this distribution. The tails that each contain 2.5\% are shaded. Thus, if the true slope is 0, and if we would draw a lot of samples and for each sample determine the slope, then 95\% of those slopes will lie within the non-shaded area. The figure also indicates the value for our T-statistic, 0.5. It can be clearly seen that a value of 0.5 lies well within the middle 95\% of the distribution, in other words, a value of 0.5 is not that strange for a $t$-distribution with 38 degrees of freedom.
%
%
% <<inf_16,fig.height=4, echo=FALSE, fig.align='center', fig.cap='Different regression lines for different values of y if x=3.'>>=
%
% df = 38; ncp = 0; limits = c(-5,5)
% lb=-20; ub=qt(0.025, df=df, ncp=ncp)
%     x <- seq(limits[1], limits[2], length.out = 100)
%     xmin <- max(lb, limits[1])
%     xmax <- min(ub, limits[2])
%     areax <- seq(xmin, xmax, length.out = 100)
%     area <- data.frame(x = areax, ymin = 0, ymax = dt(areax, df=df, ncp=ncp))
%     (ggplot()
%      + geom_line(data.frame(x = x, y = dt(x, df=df, ncp=ncp)),
%                  mapping = aes(x = x, y = y))
%      + geom_area(data = area, mapping = aes(x = x,  y = ymax))
%      + scale_x_continuous(limits = limits, breaks=seq(-5,5,1))
%      + geom_area(data = area, mapping = aes(x = seq(qt(0.975, df=df, ncp=ncp), 5, length.out = 100),  y = dt(seq(qt(0.975, df=df, ncp=ncp), 5, length.out = 100), df=df, ncp=ncp)))
%             + geom_vline(xintercept = 0.5)  + xlab("T") )
% @
%
%
%
%
%
%
%
%
%
% Based on our reasoning in the section on confidence intervals, we can construct an interval of reasonable values for the population $T$ by taking the middle 95\% of the distribution.

\section{The $p$-value}

The $p$-value is a probability. It represents the probability of observing certain events, given that the null-hypothesis is true.

In the previous section we saw that if the population slope is 0, and we drew 1000 samples of size 40, we did not observe a sample slope of -1 or smaller. In other words, the frequency of observing a slope of -1 or smaller was 0. If we would draw more samples, we theoretically could observe a sample slope of -1, but the probability that that happens for any new sample we can estimate at less than 1 in a 1000, so less than 0.001. 

This estimate of the $p$-value was based on 1000 randomly drawn samples of size 40 and then looking at the frequency of certain values in that data set. But there is a short-cut, for we know that the distribution of sample slopes has a $t$-distribution if we divide the sample slopes by the standard error. Therefore we do not have to take 1000 samples and estimate probabilities, but we can look at the $t$-distribution directly, using tables online or in statistical packages. 

Figure \ref{fig:inf_117} shows the $t$-distribution that is the actual distribution of the histogram in Figure \ref{fig:inf_14}. If the standard error is equal to 0.19, and the hypothetical population slope is 0, then the $T$-statistic associated with a slope of -1 is equal to $\frac{-1-0}{0.19}=-5.26$. With this value, we can look up in the tables, how often such a value of -5.26 or smaller occurs in a $t$-distribution with 38 degrees of freedom. In the tables we find that the probability that this occurs is 0.00000294. So, the fact that the $T$-statistic has a $t$-distribution gives us the opportunity to exactly determine certain probabilities, including the $p$-value.  

<<inf_117,fig.height=4, echo=FALSE, fig.align='center', fig.cap='The histogram of 1000 sample slopes and its corresponding theoretical t-distribution with 38 degrees of freedom. The vertical line represents the T-value of -5.56.'>>=

df = 38; ncp = 0; limits = c(-8,8)
lb=-20; ub=qt(0.025, df=df, ncp=ncp)
    x <- seq(limits[1], limits[2], length.out = 100)
    xmin <- max(lb, limits[1])
    xmax <- min(ub, limits[2])
    areax <- seq(xmin, xmax, length.out = 100)
    area <- data.frame(x = areax, ymin = 0, ymax = dt(areax, df=df, ncp=ncp))
    (ggplot()
     + geom_line(data.frame(x = x, y = dt(x, df=df, ncp=ncp)),
                 mapping = aes(x = x, y = y))
     # + geom_area(data = area, mapping = aes(x = x,  y = ymax))
     + scale_x_continuous(limits = limits, breaks=seq(-8,8,1))
            + geom_vline(xintercept = -5.26)  + xlab("T") 
            + geom_histogram(aes(x=sample.slope/0.19,y=..density..),binwidth = 0.4)
            + ylab("density")) 
@

Now let's suppose we have only one sample of 40 bottles, and we find a slope of 0.1 with a standard error of 0.19. Then this value of 0.1 is $0.1/0.19=0.53$ standard errors away from 0. Thus, the $T$-statistic was 0.53. We then look at the $t$-distribution with 38 degrees of freedom, and see that such a T-value of 0.53 is not very strange: it lies well within the middle 95\% of the $t$-distribution.

Let's determine the p-value again for this slope of 0.1: we determine the probability that we obtain such a T-value of 0.53 or larger. Figure \ref{fig:inf_17} shows the area under the curve for values of $T$ that are larger than 0.53. This area under the curve can be seen as a probability. The total area under the curve of the t-distribution amounts to 1. If we know the area of the shaded part of the total area, we can compute the probability of finding T-values larger than 0.53.

<<inf_17,fig.height=4, echo=FALSE, fig.align='center', fig.cap='Different regression lines for different values of y if x=3.'>>=

df = 38; ncp = 0; limits = c(-5,5)
lb=-20; ub=qt(0.025, df=df, ncp=ncp)
    x <- seq(limits[1], limits[2], length.out = 100)
    xmin <- max(lb, limits[1])
    xmax <- min(ub, limits[2])
    areax <- seq(xmin, xmax, length.out = 100)
    area <- data.frame(x = areax, ymin = 0, ymax = dt(areax, df=df, ncp=ncp))
    (ggplot()
     + geom_line(data.frame(x = x, y = dt(x, df=df, ncp=ncp)),
                 mapping = aes(x = x, y = y))
     # + geom_area(data = area, mapping = aes(x = x,  y = ymax))
     + scale_x_continuous(limits = limits, breaks=seq(-5,5,1))
     + geom_area(data = area, mapping = aes(x = seq(0.53, 5, length.out = 100),  y = dt(seq(0.53, 5, length.out = 100), df=df, ncp=ncp)), alpha=0.5)
            + geom_vline(xintercept = 0.53)  + xlab("T") )
@

In tables online, in books, or available in statistical packages, we can look up how large this area is. It turns out to be \Sexpr{round(pt(-0.53, df=38),2)}. So, if the population slope is equal to 0 and we draw an infinite number of samples of size 40 and compute the sample slopes, then 30\% of them will be larger than our sample slope of 0.1. The proportion of the shaded area is what we call a \textit{one-sided} $p$-value. We call it one-sided, because we only look at one side of the $t$-distribution: we only look at values that are larger than our T-value of 0.53.

We conclude that a slope value of 0.1 is not that strange to find if the population slope is 0. By the same token, it would also have been probable to find a slope of -0.1, corresponding to a $T$-value of -0.53. Since the $t$-distribution is symmetrical, the probability of finding a $T$-value of less than -0.53 is depicted in Figure \ref{fig:inf_18}, and of course this probability is also \Sexpr{round(pt(-0.53, df=38),2)}.

<<inf_18,fig.height=4, echo=FALSE, fig.align='center', fig.cap='Different regression lines for different values of y if x=3.'>>=

df = 38; ncp = 0; limits = c(-5,5)
lb=-20; ub=qt(0.025, df=df, ncp=ncp)
    x <- seq(limits[1], limits[2], length.out = 100)
    xmin <- max(lb, limits[1])
    xmax <- min(ub, limits[2])
    areax <- seq(xmin, xmax, length.out = 100)
    area <- data.frame(x = areax, ymin = 0, ymax = dt(areax, df=df, ncp=ncp))
    (ggplot()
     + geom_line(data.frame(x = x, y = dt(x, df=df, ncp=ncp)),
                 mapping = aes(x = x, y = y))
     # + geom_area(data = area, mapping = aes(x = x,  y = ymax))
     + scale_x_continuous(limits = limits, breaks=seq(-5,5,1))
     + geom_area(data = area, mapping = aes(x = seq(-5, -0.53, length.out = 100),  y = dt(seq(-5, -0.53, length.out = 100), df=df, ncp=ncp)), alpha=0.5)
            + geom_vline(xintercept = -0.53)  + xlab("T") )
@

Remember that the null-hypothesis is that the population slope is 0, and the alternative hypothesis is that the population slope is \textit{not} 0. We should therefore conclude that if we find a very large positive \textit{or} negative slope, large in the sense of the number of standard errors away from 0, that the null-hypothesis is unlikely to be true. Therefore, if we find a slope of 0.1 or -0.1, then we should determine the probability of finding a $T$-value that is larger than 0.53 or smaller than -0.53. This probability is depicted in Figure \ref{fig:inf_19} and is equal to twice the one-side $p$-value, $2 \times \Sexpr{pt(-0.5, df=38)}=\Sexpr{2*pt(-0.5, df=38)}$.

<<inf_19,fig.height=4, echo=FALSE, fig.align='center', fig.cap='The vertical line represents a T-value of 0.53. The shaded area represents the two-sided p-value: the probability of obtaining a T-value smaller than -0.53 or larger than 0.53.'>>=

df = 38; ncp = 0; limits = c(-5,5)
lb=-20; ub=-0.53
    x <- seq(limits[1], limits[2], length.out = 100)
    xmin <- max(lb, limits[1])
    xmax <- min(ub, limits[2])
    areax <- seq(xmin, xmax, length.out = 100)
    area <- data.frame(x = areax, ymin = 0, ymax = dt(areax, df=df, ncp=ncp))
    (ggplot()
     + geom_line(data.frame(x = x, y = dt(x, df=df, ncp=ncp)),
                 mapping = aes(x = x, y = y))
     + geom_area(data = area, mapping = aes(x = x,  y = ymax), alpha=0.5)
     + scale_x_continuous(limits = limits, breaks=seq(-5,5,1))
     + geom_area(data = area, mapping = aes(x = seq(0.53, 5, length.out = 100),  y = dt(seq(0.53, 5, length.out = 100), df=df, ncp=ncp)), alpha=0.5)
            + geom_vline(xintercept = 0.53)  + xlab("T") )
@

This probability is called the \textit{two-sided} $p$-value. This is the one that should always be used, since the alternative hypothesis is also two-sided: the population slope can be positive or negative. The question now is: is a sample slope of 0.1 enough evidence to reject the null-hypothesis? To determine that, we determine how many standard errors away from 0 the sample slope is and we look up in tables how often that happens. Thus in our case, we found a slope that is 0.53 standard errors away from 0 and the tables told us that the probability of finding a slope that is at least 0.53 standard deviations away from 0 (positive or negative) is equal to \Sexpr{2*pt(-0.53, df=38)}. We find this probability rather large, so we decide that we \textit{do not reject the null-hypothesis}.


\section{Hypothesis testing}

In the previous section, we found a p-value of 0.00000294 and more or less concluded that this probability was rather small. Next we determined the p-value associated with a slope of 0.1 and found a p-value of 0.60. This probability we found was rather large, and decided not to reject the null-hypothesis. In other words, the probability was so large that we thought that the hypothesis that the population slope is 0 should not be rejected based on our findings. 

When should we think the $p$-value is small enough to conclude that the null-hypothesis can be rejected? When can we conclude that the hypothesis that the population slope is 0 is not supported by our sample data? This was a question posed to the founding father of statistical hypothesis testing, Sir Ronald Fischer. In his book \textit{Statistical Methods for Research Workers} (1925), Fisher proposed a probability of 5\%. He advocated 5\% as a standard level for concluding that there is evidence against the null-hypothesis. However, he did not see it as an absolute rule: "If P is between .1 and .9 there is certainly no reason to suspect the hypothesis tested. If it is below .02 it is strongly indicated that the hypothesis fails to account for the whole of the facts. We shall not often be astray if we draw a conventional line at .05...". So Fisher saw the $p$-value as an informal index to be used as a measure of discrepancy between the data and the null-hypothesis: “The null hypothesis is never proved or established, but is possibly disproved”.


Later, Neyman and Pearson saw the $p$-value as an instrument in decision making: is the null-hypothesis true, or is the alternative hypothesis true? You either reject the null-hypothesis or you don't, there is nothing in between. This view to data-analysis is still rather popular in the social and behavioural sciences, but also in particle physics. In order to make such a black-and-white decision, you decide before-hand, that is, before collecting data, what \textit{level of significance} you choose for your $p$-value to decide whether to reject the null-hypothesis. For example, as your significance level, you might want to choose 1\%. Let's call this chosen significance level $\alpha$. Then you collect your data, you apply your linear model to the data, and find that the $p$-value associated with the slope effect equals $p$. If $p$ is smaller than or equal to $\alpha$ you \textit{reject the null-hypothesis}, and if $p$ is larger than $\alpha$ then you \textit{do not reject the null-hypothesis}. A coefficient with a $p \leq \alpha$ is said to be \textit{significant}, and a coefficient with a $p > \alpha$ is said to be \textit{non-significant}. If the slope is significant, then one should reject the null-hypothesis and say there is a slope in the population different from zero. If the slope is not significant, then one should not reject the null-hypothesis and say there is no slope in the population (i.e., the slope is 0).



\subsection{Exercises}

\begin{enumerate}

\item Suppose you test the null-hypothesis that in a linear equation describing the relationship between the mass of a planet and its volume the slope equals 0:

\begin{equation}
mass = \beta_0 + \beta_1 volume + \epsilon
\end{equation}

State the null-hypothesis.


\item You set your significance level to 1\%, so $\alpha=0.01$. Next, you measure 52 planets and you find a sample slope of $b_1=6$, with a standard error of 2.24. Determine the $T$-statistic with which you test the null-hypothesis. 

\item Determine the $p$-value on the basis of Table \ref{tab:nonparmixed_4}.

\item Do you reject or or do you not reject the null-hypothesis? What does this mean?

\item A car manufacturer wants to build safe cars. One of the engineers conducts colision experiments: cars with a certain velocity are directed towards a still-standing car. Both the velocity and the deepness of the dent in the still-standing car is measured. She expects to see that high velocity creates deeper dents and she applies a regression model. 

\begin{equation}
deepness = \beta_0 + \beta_1 velocity + \epsilon
\end{equation}

State the null-hypothesis. 

\item The engineer sets her significance level to 5\%, so $\alpha=0.05$. Next, she measures 4 cars with speeds between 90 mph and 92 mph and she finds a sample slope of $b_1=2$, with a standard error of 1.5. Determine the $T$-statistic with which you test the null-hypothesis. 

\item For her $T$-statistic with 2 degrees of freedom, she finds a $p$-value of $\Sexpr{2*(1-pt(2/1.5,2))}$. Is the effect of velocity on deepness of the dent significant?

\item Should the engineer reject or not reject the null-hypothesis? What does this mean?

\item Could you think of possible reasons why the engineer does not find an effect of velocity on the deepness of the dent?

\end{enumerate}


Answers: 

\begin{enumerate}

\item 

\begin{equation}
H_0: \beta_1 = 0 
\end{equation}



\item $T= \frac{6 - 0}{2.24}= 2.68 $

\item 

Degrees of freedom is $52-2=50$. From the table for a t-distribution with 50 degrees of freedom, we see that a $T$-value of 2.68 is the 0.995 quantile. Thus, half a percent of the $T$-values are larger than 2.68. Because of symmetry, half a percent of the $T$-values is smaller than -2.68. So in total, 1\% of the $T$-values are at least 2.68 away from the mean (both directions). Therefore, the two-sided $p$-value is 0.01.  

\item Our $p$-value of 0.01 is equal to our $\alpha$ and we therefore reject the null-hypothesis. This means that we conclude that the slope coefficient in all planets in the universe is not 0. There is a relationship between the volume of a planet and its mass.

\item 

\begin{equation}
H_0: \beta_1= 0
\end{equation}

\item
$T= (2-0)/1.5=2/1.5=1.33$

\item
The $p$-value $\Sexpr{2*(1-pt(2/1.5,2))}$ is larger than her $\alpha$, so the effect of velocity is not significant. 

\item
 The effect is not significant so she should not reject the null-hypothesis. This means that the conclusion is that there is no relationship between the velocity of the incoming car and the deepness of the dent in the receiving car. 
 
\item First of all, there were only 4 cars tested. A small sample size results in a relatively large standard error, so a relatively small $T$-statistic. The higher the $T$-value the lower the $p$-value. Second, there was hardly any variation in the speed of the incoming car: if you want to find an effect, there should be cars with both high and low velocities, otherwise you won't see any differences in the dents. 

\end{enumerate}







\section{Type I and Type II errors in decision making}


Since data-analysis is about probabilities, there is always a chance that you make the wrong decision: you can wrongfully reject the null-hypothesis, or you can wrongfully accept the null-hypothesis. Pearson and Neyman distinguished between two kinds of error: one could reject the null-hypothesis while it is actually true (error of the first kind, or type I error) and one could accept the null-hypothesis while it is not true (error of the second kind, or type II error). 

To illustrate the difference between type I and type II errors, let's recall the famous fable by Aesop about the boy who cried wolf. The tale concerns a shepherd boy who repeatedly tricks other people into thinking a wolf is attacking his flock of sheep. The first time he cries "There is a wolf!", the men working in an adjoining field come to help him. But when they repeatedly find there is no wolf to be seen, they realise they are being fooled by the boy. One day, when a wolf \textit{does} appear and the boy again calls for help, the men believe that it is another false alarm and the sheep are eaten by the wolf.

In this fable, we can think of the null-hypothesis as the hypothesis that there is no wolf. The alternative hypothesis is that there is a wolf. Now, when the boy cries wolf the first time, there is in fact no wolf. The men from the adjoining field make a type I error: they think there is a wolf while there isn't. Later, when they are fed up with the annoying shepherd boy, they don't react when the boy cries "There is a wolf!". Now they make a type II error: they think there is no wolf, while there actually is a wolf. 

Let's return to regression analysis. Suppose you want to determine the slope for the effect of age on height in children. The null-hypothesis is that the effect is 0 in the population of all children. You might study a sample of children and you might find a value for the slope. You might decide that if the $p$-value is lower than a critical value you conclude that the null-hypothesis is not true. Suppose you think a probability of 10\% is small enough to reject the null-hypothesis as true. In other words, if $p \leq 0.10$ then we no longer think 0 is a reasonable value for the population slope. In this case, we have fixed our $\alpha$ or type I error rate to be $\alpha=0.10$. This means that if we study a random sample of children, we look at the slope and find a $p$-value of 0.11, then we do not reject the null-hypothesis. If we find a $p$-value of 0.10, then we reject the null-hypothesis.

Note that the probability of a type I error is the same as our $\alpha$ for the significance level. Suppose we set our $\alpha=0.05$. Then for any $p$-value equal or smaller than 0.05, we reject the null-hypothesis. Suppose the null-hypothesis is true, how often do we then find a $p$-value smaller than 0.05? We find a $p$-value smaller than 0.05 if we find a $T$-value that is above a certain threshold. For instance, for the $t$-distribution with 198 degrees of freedom, the critical value is $\pm 1.97$, \textit{because only in 5\% of the cases we find a $T$-value of $\pm 1.97$ or more if the null-hypothesis is true}! Thus, if the null-hypothesis is true, we see a $T$-value of at least $\pm 1.97$ in 5\% of the cases. Therefore, we see a signficicant $p$-value in 5\% of the cases if the null-hypothesis is true. This is exactly the definition of a Type I error: the probability that we reject the null-hypothesis (finding a significant $p$-value), given that the null-hypothesis is true. So we call our $\alpha$-value the type I error rate. 

Suppose 100 researchers are studying a particular slope. They each draw a random sample from the population and test whether their sample slope is significantly different from 0. Suppose they all use different sample sizes, but they all use the same $\alpha$ of 0.05. Then we can expect that about 5 researchers will reject the null-hypothesis (finding a $p$-value less than or smaller than 0.05) and about 95 will not reject the null-hypothesis (finding a $p$-value of more than 0.05). 

Fixing the type I error rate should always be done before data collection. How willing are you to take a risk of a type I error? You are free to make a choice about $\alpha$, as long as you report it. 

If $\alpha$ represents the probability of making a type I error, then we can use $\beta$ to represent the probability of not rejecting the null-hypothesis while it is not true (type II error). However, setting the $\beta$ value prior to data collection is a bit trickier than choosing your $\alpha$. It is not possible to compute the probability that we find a non-significant effect $(p < \alpha)$, given that the alternative hypothesis is true, because the alternative hypothesis is only saying that the slope is not equal to 0. In order to compute $\beta$, we need to think first of a reasonable size of the slope that we expect. For example, suppose we believe that a slope of 1 is quite reasonable, given what we know about growth in children. Let that be our alternative hypothesis: 


\begin{eqnarray}
H_0: \beta_1 =0 \nonumber \\
H_1: \beta_1 = 1\nonumber
\end{eqnarray}


Next, we determine the distribution of sample slopes under the assumption that the population slope is 1. We know that this distribution has a mean of 1 and a standard deviation equal to the standard error. We also know it has the shape of a $t$-distribution. Let sample size be equal to 102 and the standard error 2. If we standardize the slopes by dividing by the standard error, we get the two $t$-distributions in Figure \ref{fig:inf_20}: one distribution of $T$-values if the population slope is 0, and one distribution of $T$-values if the population slope is 1.

The shaded areas represent the area where $p < \alpha$: for all values of $T$ smaller than $\Sexpr{qt(0.1, df=df, ncp=0)}$ and larger than $\Sexpr{qt(0.9, df=df, ncp=0)}$, we reject the null-hypothesis. The probability that this happens, \textit{if the null-hypothesis is true}, is equal to $\alpha$ which is 0.10 in this example. The probability that this happens \textit{if the alternative hypothesis is true} (here, population slope is 1), is depicted in Figure \ref{fig:inf_21}.


<<inf_20,fig.height=4, echo=FALSE, fig.align='center', fig.cap='Different t-distributions of the sample slope if the population slope equals 0 (left curve), and if the population slope equals 1 (right curve). Grey area depicts the probability that we find a p-value value smaller than 0.10 if the population slope is 0.'>>=

df = 100; ncp = 0; limits = c(-5,5)
lb=-20; ub=qt(0.05, df=df)
    x <- seq(limits[1], limits[2], length.out = 100)
    xmin <- max(lb, limits[1])
    xmax <- min(ub, limits[2])
    areax <- seq(xmin, xmax, length.out = 100)
    area <- data.frame(x = areax, ymin = 0, ymax = dt(areax, df=df, ncp=ncp))
    (ggplot()
     + geom_line(data.frame(x = x, y = dt(x, df=df, ncp=ncp)),
                 mapping = aes(x = x, y = y), col="blue",show.legend = F)
     + geom_line(data.frame(x = x, y = dt(x, df=df, ncp=0.5)),
                 mapping = aes(x = x, y = y), col="red",show.legend = F)
     + geom_area(data = area, mapping = aes(x = x,  y = ymax, alpha=0.5), fill="blue",show.legend = F)
     + scale_x_continuous(limits = limits, breaks=c(-1.66, 1.66))
     + geom_area(data = area, mapping = aes(x = seq(qt(0.95, df=df) , 5, length.out = 100),  y = dt(seq(qt(0.95, df=df), 5, length.out = 100), df=df, ncp=ncp), alpha=0.5), fill="blue",show.legend = F) 
     + xlab("T") 
            + geom_text(mapping=aes(x=-2.1,y=0.3,label="null hypothesis"),col="blue" )
            + geom_text(mapping=aes(x=3.1,y=0.3,label="alternative hypothesis"),col="red" , show.legend = F)
            +ylab("density"))
@

<<inf_21,fig.height=4, echo=FALSE, fig.align='center', fig.cap='Different t-distributions of the sample slope if the population slope equals 0 (left curve), and if the population slope equals 1 (right curve). Grey area depicts the probability that we find a p-value value smaller than 0.10 if the population slope is 1.'>>=

df = 100; ncp = 0; limits = c(-5,5)
lb=-20; ub=qt(0.05, df=df)
    x <- seq(limits[1], limits[2], length.out = 100)
    xmin <- max(lb, limits[1])
    xmax <- min(ub, limits[2])
    areax <- seq(xmin, xmax, length.out = 100)
    area <- data.frame(x = areax, ymin = 0, ymax = dt(areax, df=df, ncp=ncp))
    (ggplot()
     + geom_line(data.frame(x = x, y = dt(x, df=df, ncp=ncp)),
                 mapping = aes(x = x, y = y), col="blue",show.legend = F)
            + geom_line(data.frame(x = x, y = dt(x, df=df, ncp=0.5)),
                 mapping = aes(x = x, y = y), col="red",show.legend = F)
     + geom_area(data = area, mapping = aes(x = x,  y = dt(x, df=df, ncp=0.5), alpha=0.5), fill="red",show.legend = F)
     + scale_x_continuous(limits = limits, breaks=c(-1.66, 1.66))
     + geom_area(data = area, mapping = aes(x = seq(qt(0.95, df=df, ncp=0) , 5, length.out = 100),  y = dt(seq(qt(0.95, df=df, ncp=0), 5, length.out = 100), df=df, ncp=0.5), alpha=0.5), fill="red",show.legend = F)
            +xlab("T")  + geom_text(mapping=aes(x=-2.1,y=0.3,label="null hypothesis"),col="blue" )
            + geom_text(mapping=aes(x=3.1,y=0.3,label="alternative hypothesis"),col="red" , show.legend = F)
            +ylab("density"))
@

The shaded area in Figure \ref{fig:inf_21} turns out to be $\Sexpr{1-pt(qt(0.95, df=df, ncp=0), df=100, ncp=0.5) + pt(qt(0.05, df=df, ncp=0),df=100,ncp=0.5)}$. This represents the probability that we find a significant effect, \textit{if the population slope is 1}. This is actually the \textit{complement} of the probability to find a non-significant effect, \textit{if the population slope is 1}, which is $\beta$ or the probability of a type II error. Therefore, Figure \ref{fig:inf_21} represents $1- \beta$: the probability of finding a significant $p$-value, if the population slope is 1. In this example, $1-\beta$ is equal to $\Sexpr{1-pt(qt(0.95, df=df, ncp=0), df=100, ncp=0.5) + pt(qt(0.05, df=df, ncp=0),df=100,ncp=0.5)}$, so $\beta$ is equal to its complement, $1- \Sexpr{1-pt(qt(0.95, df=df, ncp=0), df=100, ncp=0.5) + pt(qt(0.05, df=df, ncp=0),df=100,ncp=0.5)} = \Sexpr{1- (1-pt(qt(0.95, df=df, ncp=0), df=100, ncp=0.5) + pt(qt(0.05, df=df, ncp=0),df=100,ncp=0.5))}$.

In sum, in this example with an $\alpha$ of 0.10 and assuming a population slope of 1, we find that the probability of a type II error is 0.86: if there is a slope of 1, then we have a 86\% chance of wrongly concluding that the slope is 0.
% 
Type I and II error rates $\alpha$ and $\beta$ are closely related. If we feel that a significance level of $\alpha=0.10$ is too high, we could choose a level of 0.01. This ensures that we are less likely to reject the null-hypothesis when it is true. The critical value for our $T$-statistic is then equal to $\pm  \Sexpr{qt(0.995,df=df,ncp=0)}$, see Figure \ref{fig:inf_22}. In Figure \ref{fig:inf_23} we see that if we change $\alpha$, we also get a different value for $1-\beta$.

Thus, if we use smaller values for $\alpha$, we get smaller values for $1-\beta$, so we get larger values for $\beta$. This means that if we lower the probability of rejecting the null-hypothesis given that it is true (type I error) by choosing a lower value for $\alpha$, we inadvertently increase the probability of failing to reject the null-hypothesis given that it is not true (type II error). One should therefore always strike a balance between the two types of errors. One should consider how bad it is to think that the slope is not 0 while it is, and how bad it is to think that the slope is 0, while it is not. If you feel that the first mistake is worse than the second one, then make sure $\alpha$ is really small, and if you feel that the second mistake is worse, then make $\alpha$ not too small. Another option, and a better one, to avoid type II errors, is to increase sample size, as we will see in the next section.

<<inf_22,fig.height=4, echo=FALSE, fig.align='center', fig.cap='Different t-distributions of the sample slope if the population slope equals 0 (left curve), and if the population slope equals 1 (right curve). Grey area depicts the probability that we find a p-value value smaller than 0.01 if the population slope is 0.'>>=

df = 100; ncp = 0; limits = c(-5,5)
lb=-20; ub=qt(0.005,df=df,ncp=0)
    x <- seq(limits[1], limits[2], length.out = 100)
    xmin <- max(lb, limits[1])
    xmax <- min(ub, limits[2])
    areax <- seq(xmin, xmax, length.out = 100)
    area <- data.frame(x = areax, ymin = 0, ymax = dt(areax, df=df, ncp=ncp))
    (ggplot()
     + geom_line(data.frame(x = x, y = dt(x, df=df, ncp=ncp)),
                 mapping = aes(x = x, y = y), col="blue", show.legend = F)
            + geom_line(data.frame(x = x, y = dt(x, df=df, ncp=0.5)),
                 mapping = aes(x = x, y = y),  col="red", show.legend = F)
     + geom_area(data = area, mapping = aes(x = x,  y = ymax, alpha=0.5), fill="blue", show.legend = F)
     + scale_x_continuous(limits = limits, breaks=c(qt(0.005,df=df,ncp=0), qt(0.995,df=df,ncp=0)))
     + geom_area(data = area, mapping = aes(x = seq(qt(0.995,df=df,ncp=0) , 5, length.out = 100),  y = dt(seq(qt(0.995,df=df,ncp=0), 5, length.out = 100), df=df, ncp=ncp), alpha=0.5), fill="blue",show.legend = F)  
            + geom_text(mapping=aes(x=-2.1,y=0.3,label="null hypothesis"),col="blue" )
            + geom_text(mapping=aes(x=3.1,y=0.3,label="alternative hypothesis"),col="red" , show.legend = F)
            +ylab("density")
            + xlab("T"))
@

<<inf_23,fig.height=4, echo=FALSE, fig.align='center', fig.cap='Different t-distributions of the sample slope if the population slope equals 0 (left curve), and if the population slope equals 1 (right curve). Grey area depicts the probability that we find a p-value value smaller than 0.01 if the population slope is 1.'>>=

df = 100; ncp = 0; limits = c(-5,5)
lb=-20; ub=qt(0.005,df=df,ncp=0)
    x <- seq(limits[1], limits[2], length.out = 100)
    xmin <- max(lb, limits[1])
    xmax <- min(ub, limits[2])
    areax <- seq(xmin, xmax, length.out = 100)
    area <- data.frame(x = areax, ymin = 0, ymax = dt(areax, df=df, ncp=ncp))
    (ggplot()
     + geom_line(data.frame(x = x, y = dt(x, df=df, ncp=ncp)),
                 mapping = aes(x = x, y = y), col="blue")
            + geom_line(data.frame(x = x, y = dt(x, df=df, ncp=0.5)),
                 mapping = aes(x = x, y = y), col="red")
     + geom_area(data = area, mapping = aes(x = x,  y = dt(x, df=df, ncp=0.5), alpha=0.5), fill="red", show.legend = F)
     + scale_x_continuous(limits = limits, breaks=c(qt(0.005,df=df,ncp=0), qt(0.995,df=df,ncp=0)))
     + geom_area(data = area, mapping = aes(x = seq(qt(0.995,df=df,ncp=0) , 5, length.out = 100),  y = dt(seq(qt(0.995,df=df,ncp=0), 5, length.out = 100), df=df, ncp=0.5), alpha=0.5), fill="red", show.legend = F)
            +xlab("T")
            + geom_text(mapping=aes(x=-2.1,y=0.3,label="null hypothesis"),col="blue" )
            + geom_text(mapping=aes(x=3.1,y=0.3,label="alternative hypothesis"),col="red" , show.legend = F)
            +ylab("density"))
@


\subsection{Exercises}

\begin{enumerate}

\item When we talk about decision making in data analysis, what do we mean by $\beta$?

\item What do we mean by $1-\beta$?

\item What doe we mean by $\alpha$?

\item What do we mean by making a type I error?

\item What do we mean by making a type II error?

\item What do we mean by $1-\alpha$?


\end{enumerate}



Answers:

\begin{enumerate}

\item The type II error rate, or the probability of not rejecting the null-hypothesis while the null-hypothesis is not true. 

\item The power of test: the probability of finding a significant effect if the null-hypothesis is not true.

\item The type I error rate, or the probability of rejecting while the null-hypothesis is true

\item Wrongly concluding that the null-hypothesis is not true.

\item Wrongly concluding that the null-hypothesis is true.

\item The probability of not rejecting the null-hypothesis while the null-hypothesis is true. 


\end{enumerate}


\section{Statistical power}

Null-hypothesis testing only involves the null-hypothesis: we look at the sample slope, compute the $T$-statistic and then see how often such a $T$-value and larger values occur given that the population slope is 0. Then we look at the $p$-value and if that $p$-value is smaller than $\alpha$, the we reject the null-hypothesis. Therefore, null-hypothesis testing does not involve testing the alternative hypothesis. We can decide what value we choose for our $\alpha$, but not our $\beta$. 

As stated in the previous section, we can compute $\beta$ only if we have a more specific idea of an alternative value for the population slope. We saw that we needed to think of a reasonable value for the population slope that we might be interested in. Suppose we have the intuition that a slope of 1 could well be the case. Then, we would like to find a $p$-value of less than $\alpha$ if indeed the slope were 1. We hope that the probability that this happens is very high: the conditional probability that we find a $T$-value large enough to reject the null-hypothesis, given that the population slope is 1. This probability is actually the \textit{complement} of $\beta$, $1-\beta$: the probability that we reject the null-hypothesis, given that the null-hypothesis is not true. When we think again about the boy cried wolf: the power is the probability that the villagers come to the rescue if there is indeed a wolf attacking the sheep. 

So in order to get a large value for $1-\beta$, we should have large $T$-values in our data-analysis. There are two ways in which we can increase the value of the $T$-statistic. Since $T=B/se$, we can get large values for $T$ if we use large values for the alternative population slope $B$, and if we have a small standard error, $se$. We get a small standard error if we have a large sample size. 

If we go back to the example of the previous section where we had a sample size of 102 children and our alternative hypothesis was that the population slope was 1, we found that the t-distribution for the alternative hypothesis was centered at 0.5, because the standard error was 2. Suppose that we would increase sample size to 1200 children, then our standard error might be 0.2, so that our t-distribution for the alternative hypothesis is centerd at 5. This is shown in Figure \ref{fig:inf_24}. 








<<inf_24,fig.height=4, echo=FALSE, fig.align='center', fig.cap='Different t-distributions of the sample slope if the population slope equals 0 (left curve), and if the population slope equals 1 (right curve). Now for a larger sample size. Grey area depicts the probability that we find a p-value value smaller than 0.01 if the population slope is 1.'>>=

df = 1198; ncp = 0; limits = c(-5,10)
lb=-20; ub=qt(0.005,df=df,ncp=0)
    x <- seq(limits[1], limits[2], length.out = 100)
    xmin <- max(lb, limits[1])
    xmax <- min(ub, limits[2])
    areax <- seq(xmin, xmax, length.out = 100)
    area <- data.frame(x = areax, ymin = 0, ymax = dt(areax, df=df, ncp=ncp))
    (ggplot()
     + geom_line(data.frame(x = x, y = dt(x, df=df, ncp=ncp)),
                 mapping = aes(x = x, y = y), col="blue")
            + geom_line(data.frame(x = x, y = dt(x, df=df, ncp=5)),
                 mapping = aes(x = x, y = y), col="red", show.legend = F)
     + geom_area(data = area, mapping = aes(x = x,  y = dt(x, df=df, ncp=5), alpha=0.5 ),fill="red", show.legend = F)
     + scale_x_continuous(limits = limits, breaks=c(qt(0.005,df=df,ncp=0), qt(0.995,df=df,ncp=0)))
     + geom_area(data = area, mapping = aes(x = seq(qt(0.995,df=df,ncp=0) , 10, length.out = 100),  y = dt(seq(qt(0.995,df=df,ncp=0), 10, length.out = 100), df=df, ncp=5), alpha=0.5), fill="red", show.legend = F)
            + xlab("T")
            + geom_text(mapping=aes(x=-2.1,y=0.3,label="null hypothesis"),col="blue" )
            + geom_text(mapping=aes(x=8.1,y=0.3,label="alternative hypothesis"),col="red" , show.legend = F)
            + ylab("density"))
@


We see from the shaded area that if the population slope is really 1, there is a very high chance that the $T$-value for the sample slope will be larger than 2.57, the cutoff point for an alpha of 0.01. The probability of rejecting the null-hypothesis while it is not true, is therefore very large. This is our $1-\beta$ and we call this probability of making the correct decision to reject the null-hypothesis while it is not true the $power$ of the null-hypothesis test. We see that with increasing sample size, the power to find a significant $T$-value increases too. 

By the same token, the power also increases with increasing population slope. If the population slope were 20, and we only had a sample of 102 children (resulting in a standard error of 2), the $t$-distribution for the alternative hypothesis that the population slope is 10 is centered around $B/se=20/2=5$, resulting in the same plot as in Figure \ref{fig:inf_24}, with a large value for $1-\beta$.


In sum: the statistical power of a test is the probability that a null-hypothesis is rejected, given that it is not true. This probability is equal to $1-\beta$. The statistical power of a test increases with sample size, and depends on the actual population slope. The further away the population slope is from 0 (positive or negative), the larger the statistical power. Earlier we also saw that $1-\beta$ decreases with increasing alpha: the smaller $\alpha$, the lower the power. 


\section{Power analysis}
Because of these relationships between statistical power, $\alpha$, sample size and the actual population slope, we can compute the statistical power for any combination of $\alpha$, sample size and hypothetical population slope. 

Serious researchers carry out a power analysis prior to collecting data in order to find out how large their sample size should be. If you ever want to do your own research and you want to collect data, you should also think very carefully about how large your sample size should be. Luckily, you can find many tools online that can help you with that \footnote{e.g., http://www.gpower.hhu.de/}.

Suppose you want to minimize the probability of a type I error, so you choose an $\alpha=0.01$. Next, you think of what kind of population slope you would like to find, if it indeed has that value. Suppose that you feel that if the population slope is 2, you would really  like to find it. Next, you have to specify how badly you want to find such a value of 2. Suppose that the population slope is really 2, then you would like to have a high probability to find a $T$-value large enough to reject the null-hypothesis. This is of course the power of the test, $1-\beta$. Let's say you want to have a power of 0.90. Now you have enough information to calculate how large your sample size should be. 

Let's look at G*power, an application that can be downloaded from http://www.gpower.hhu.de/. If we start the app, we can ask for the sample size required for a slope of 0.15, an $\alpha$ of 0.01, a power ($1-\beta$) of 0.90. Let the standard deviation of our dependent variable (y=height) be 3 and the standard deviation of our independent variable (x=age) be 2. Then we get the input as displayed in Figure \ref{fig:gpower}. Note that you should use 2-sided tests, so tails=2. From the output we see that the required sample size is 1477 children.  


\begin{figure}[h]
    \begin{center}
       \includegraphics[scale=0.5,trim={14cm 0cm 0cm 0cm}]{/Users/stephanievandenberg/SURFdrive/Werk/Onderwijs/statistiek/book/linear models/book/gpower.png}
    \end{center}
    \caption{G*power output for a simple regression analysis.}
    \label{fig:gpower}
\end{figure}



\section{Criticism on null-hypothesis testing and $p$-values}






% In its turn, the $p$-value, as we have seen, depends on the $T$-statistic and the degrees of freedom. The degrees of freedom in turn depends on sample size. The $T$-statistic also depends on sample size, as it is partly based on the standard error.
% 
% If the alternative hypothesis is true, that is, if the population slope is not 0, then the probability of getting a $p$-value larger than 0.1, is equal to $\beta$. This is because by definition $\beta$ is the probablity of a type II error: the error that we \textit{do not reject the null-hypothesis, while the null-hypothesis is not true}. For example, suppose the population slope is 0.01. In a sample we find a slope of 1, with a $T$-statistic of 2.50 with 45 degrees of freedom. The associated $p$-value is equal to $\Sexpr{round(2*pt(-2.5,45),3)}$. If $\alpha=0.01$ then we conclude that this slope of 1 is not significantly different from zero. However, since the population slope is actually different from 0, namely 0.01, we draw the wrong conclusion. The conditional probability\footnote{$\alpha$, $\beta$ and the $p$-value are conditional probabilities. For the distinction between a probability and a conditional probability, see \dots. In short, suppose that in the whole world, 51\% of the people are at most 17 years old. However, suppose that in the Netherlands that proportion is only 20\%. Then if we pick a random person, the probability that that person is at most 17 years old is 0.51. However, if we happen to know that the person was picked from the Dutch population, then we know better: we know that the probability has decreased to 20\%. Thus the conditional probablity that a person is under age, given that the person is Dutch, equals 0.20. The conditional probabiilty that a person is under age, given that the person is \textit{not} Dutch, equals more than 0.51.} that we find a non-significant slope (we reject the null-hypothesis), given that the population slope differs from 0 (the null-hypothesis is not true) is equal to $\beta$.
% 
% 
% Of course we'd like to have a small $\beta$: we don't like making mistakes. So if indeed the null-hypothesis is false, we want the probability that we reject the null-hypothesis as large as possible. In order to achieve that, we need to have a $T$-value as large as possible. Since $T=b/se$, this can be achieved by having a standard error as small as possible, and this happens when our sample size is as large as possible.
% 


%%%%%%%%%%


In later editions, Fisher explicitly contrasted the use of the p-value for statistical inference in science with the Neyman–Pearson method, which he terms "Acceptance Procedures".[19] Fisher emphasizes that while fixed levels such as 5\%, 2\%, and 1\% are convenient, the exact p-value can be used, and the strength of evidence can and will be revised with further experimentation. In contrast, decision procedures require a clear-cut decision, yielding an irreversible action, and the procedure is based on costs of error, which, he argues, are inapplicable to scientific research.

There is widespread agreement that p-values are often misused and misinterpreted.[21][22][23] One practice that has been particularly criticized is accepting the alternative hypothesis for any p-value nominally less than .05 without other supporting evidence. Although p-values are helpful in assessing how incompatible the data are with a specified statistical model, contextual factors must also be considered, such as "the design of a study, the quality of the measurements, the external evidence for the phenomenon under study, and the validity of assumptions that underlie the data analysis".[23] Another concern is that the p-value is often misunderstood as being the probability that the null hypothesis is true.[23][24] Some statisticians have proposed replacing p-values with alternative measures of evidence,[23] such as confidence intervals,[25][26] likelihood ratios,[27][28] or Bayes factors,[29][30][31] but there is heated debate on the feasibility of these alternatives.[32][33] Others have suggested to remove fixed significance thresholds and to interpret p-values as continuous indices of the strength of evidence against the null hypothesis.[34][35]
%%%%%%%%%%%%%




In 2016, the American Statistical Association published a statement on $p$-values, saying that "the widespread use of 'statistical significance' (generally interpreted as '$p \leq 0.05$') as a license for making a claim of a scientific finding (or implied truth) leads to considerable distortion of the scientific process". Science is cumulative: no study is definitive. More data, alternative theories can change the interpretation of earlier results. 

p-hacking. Higgs particle


principles:


Inference about a population slope or intercept can be made on the basis of sample data, but only in probabilistic terms. This means a simple statement like the value of the population is definitely not zero cannot be made. Only statements like A population slope of 0 is not very likely given the sample data can be made.

science is cumulative. No study is definitive. Effects should be replicated by independent researchers.

always report your regression slope or intercept, with the standard error and the sample size. Based on these, the t-values can be computed with the degrees of freedom. Then if several other researchers have done the same research, the results can be combined in a meta-analysis, so that a stronger statement about the population can be made, based on a larger total sample size. 




\section{Relationship between $p$-value and confidence intervals}
We could have also come to the same conclusion using the 95\% confidence interval. If we find a sample slope of 1, and we know that the standard error is equal to 2, then we can find the 95\% confidence interval for the $T$-statistic (0.5) if we use a $t$-distribution with 38 degrees of freedom. From tables we can deduce that with a $t$-distribution of 38 degrees of freeedom, 2.5\% of the area is left of \Sexpr{qt(0.025, df=38)} and 2.5\% of the area is right of \Sexpr{qt(0.975, df=38)}. This way we know that the confidence interval for the $T$-value is from $1  \Sexpr{qt(0.025, df=38)}\times 2$ to $1 + \Sexpr{qt(0.975, df=38)}\times 2$, so from $\Sexpr{1+2*qt(0.025, df=38)}$ to \Sexpr{1+qt(0.025, df=38)*2}.

We see that the value 0 is within this range, so 0 is a reasonable value for the population slope. From this we know that the $p$-value for the null-hypothesis is less than 5\%.


<<inf_25,fig.height=4, echo=FALSE, fig.align='center', fig.cap='Different regression lines for different values of y if x=3.'>>=

df = 38; ncp = 0; limits = c(-5,5)
lb=-20; ub=-2.02
    x <- seq(limits[1], limits[2], length.out = 100)
    xmin <- max(lb, limits[1])
    xmax <- min(ub, limits[2])
    areax <- seq(xmin, xmax, length.out = 100)
    area <- data.frame(x = areax, ymin = 0, ymax = dt(areax, df=df, ncp=ncp))
    (ggplot()
     + geom_line(data.frame(x = x, y = dt(x, df=df, ncp=ncp)),
                 mapping = aes(x = x, y = y))
     + geom_area(data = area, mapping = aes(x = x,  y = ymax))
     + scale_x_continuous(limits = limits, breaks=seq(-5,5,1))
     + geom_area(data = area, mapping = aes(x = seq(2.02, 5, length.out = 100),  y = dt(seq(2.02, 5, length.out = 100), df=df, ncp=ncp)))
            + geom_vline(xintercept = 0.5)  + xlab("T")
            +geom_vline(xintercept=(0.5-2.02), col=2) + geom_vline(xintercept=(0.5+2.02), col=2))
@


<<inf_26,fig.height=4, echo=FALSE, fig.align='center', fig.cap='Different regression lines for different values of y if x=3.'>>=

df = 38; ncp = 0; limits = c(-5,5)
lb=-20; ub=-2.02
    x <- seq(limits[1], limits[2], length.out = 100)
    xmin <- max(lb, limits[1])
    xmax <- min(ub, limits[2])
    areax <- seq(xmin, xmax, length.out = 100)
    area <- data.frame(x = areax, ymin = 0, ymax = dt(areax, df=df, ncp=ncp))
    (ggplot()
     + geom_line(data.frame(x = x, y = dt(x, df=df, ncp=ncp)),
                 mapping = aes(x = x, y = y))
     + geom_area(data = area, mapping = aes(x = x,  y = ymax))
     + scale_x_continuous(limits = limits, breaks=seq(-5,5,1))
     + geom_area(data = area, mapping = aes(x = seq(2.02, 5, length.out = 100),  y = dt(seq(2.02, 5, length.out = 100), df=df, ncp=ncp)))
            + geom_vline(xintercept = 2.02)  + xlab("T")
            +geom_vline(xintercept=(2.02-2.02), col=2) + geom_vline(xintercept=(2.02+2.02), col=2))
@






\section{Inference: from sample to population}


Remember, we're usually not interested in whether there is a linear relationship between x and y in a sample. The probability that the sample slope will be exactly 0 is practically zero, so the answer will be usually no.
We're usually interested in the population: is there a linear relationship between x and y in the population?

But when we think about it, it is often ridiculous to think that the population slope will be 0 exactly! Why not 0.00000111 or -0.000000008827100? Why is a slope of exactly 0 very improbable for a sample size of 200 bottles, but less improbable in the population of 80,000 bottles? Perhaps a more interesting question is how large is the population slope?

So, instead of asking research questions like Is there a linear relationship between x and y? you might ask: How large is the linear effect of x on y? Instead of a question like Is there an effect of the intervention? it might be more interesting to ask: How large is the effect of the intervention?



% \Sexpr{knit_child('chapter_3.Rnw')} % multiple regression
% 
% \Sexpr{knit_child('chapter_5.Rnw')} % dummy variables and categorical predictors
% \Sexpr{knit_child('chapter_6.Rnw')} % moderation
% \Sexpr{knit_child('chapter_7.Rnw')} % assumptions
% \Sexpr{knit_child('chapter_8.Rnw')} % advanced topics linear models
% 
% %% contrasts en post hoc zijn nog te lastig te volgen, en zorg dat data niet 1 2 3 is, maar met betere labels, strings. niet te veel stapjes met contrast equations.
% 
% \Sexpr{knit_child('chapter_9.Rnw')} % nonparametric alternatives linear models
% \Sexpr{knit_child('chapter_10.Rnw')} % introduction linear mixed models
% \Sexpr{knit_child('chapter_11.Rnw')} % nonparametrics for within designs
% %
% 
% \Sexpr{knit_child('chapter_12.Rnw')} % generalized linear models: logistic regression
% % 
% \Sexpr{knit_child('chapter_13.Rnw')} % generalized linear models: poisson models

\end{document}