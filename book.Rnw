
\documentclass[]{report}
\usepackage[english]{babel}
\usepackage{graphicx}





% Title Page
\title{Analyzing data using linear models}

\author{Stephanie van den Berg}
\date{Versie 0.1 \\ (\today)}



\begin{document}
\maketitle

<<libraries, echo=FALSE,  warning=FALSE, message=F>>=
library(ggplot2)
library(foreign)
library(dplyr)
library(lme4)
library(tidyr)
library(xtable)
@


\begin{abstract}
This book is intended to be of use to bachelor students in social sciences that want to learn how to analyze their data, with the specific aim to answer research questions. The book has a practical take on data analysis: how to do it, how to interpret the results, and how to report the results. All techniques are presented within the framework of linear models: this includes simple regression models, to linear mixed models, and generalized linear models. All methods can be carried out within one supermodel: the generalized linear mixed model. This approach is illustrated using SPSS.
\end{abstract}


\tableofcontents


\Sexpr{knit_child('chapter_1.Rnw')} % exploring your data


\chapter{Linear modelling: introduction FULYA}
\section{Linear relationships}
\section{Pearson correlation}
\section{Simple regression with a continuous predictor}
\section{Predicting the dependent variable}


\chapter{Multivariate regression}
\section{R-squared}

\chapter{Inference MARIAN}
\section{Random sampling and confidence intervals}
\section{$t$-statistics, null-hypothesis testing and $p$-values}
\section{Inference: from sample to population}



\Sexpr{knit_child('chapter_5.Rnw')} % dummy variables and categorical predictors
\Sexpr{knit_child('chapter_6.Rnw')} % moderation
\Sexpr{knit_child('chapter_7.Rnw')} % assumptions
\Sexpr{knit_child('chapter_8.Rnw')} % advanced topices linear models
\Sexpr{knit_child('chapter_9.Rnw')} % nonparametric alternatives linear models
\Sexpr{knit_child('chapter_10.Rnw')} % introduction linear mixed models



\chapter{Non-parametric alternatives for linear mixed models}


\section{Checking assumptions}

In previous chapters we have discussed the assumptions of linear models and linear mixed models: linearity (in parameters), homoscedasticity (equal variance), normal distribution of residuals, normal distribution of random effects (relevant for linear mixed models only), and independence (no clustering unaccounted for). 




The problem of nonlinearity can be solved by introducing quadratic terms, for instance by replacing a linear model $Y = b_0 + b_1 X + e$ by another linear model $Y = b_0 + b_1 X + b_2 X^2 + e$.

If we have nonindependence, then you can introduce either an extra fixed effect or a random effect for this clustering. For example, if you see that cars owned by low income families have much more mileage than cars owned by high income families, you can account for this by adding a fixed effect of an income variable as predictor. If you see that average milage is rather similar within municipality but that average mileage can vary quite a lot across municipalities, you can introduce a random effect for municipality (if you have data say from 30 different municipalities). 

Unequal variance of residuals and nonnormal distribution of residuals are harder to tackle. Unequal variance can be tackled sometimes by using linear models, but with more advanced options, or by making corrections to $p$-values that make inference more robust against model violations. Violations of normality are even a bigger problem. Nonnormality can sometimes be solved by using generalized linear models (see next chapter). A combination of nonnormality and unequal variance can sometimes be solved by using a transformation of the data, for instance not analyzing $Y = b_0 + b_1 X + e$ but analyzing $log(Y)=  b_0 + b_1 X + e$ or $\sqrt{Y}=  b_0 + b_1 X + e$.

If these data transformations or advanced options don't work (or if you're not acquainted with them), and your data show nonequal variance and/or nonnormally distributed residuals, there are nonparametric alternatives.  Here we discuss two: Friedman's test and Wilcoxon's signed rank test. We explain them using an imaginary data set on speedskating.
\\
\\
Suppose we have data on 12 speedskaters that participate on the 10 kilometers distance in three separate championships in 2017-2018: the European Championships, the Winter Olympics and the World Championships. Your friend expects that speedskaters will perform best at the Olympic games, so there she expects the fastest times. So you decide to test the null-hypothesis that average times are the same at the three occasions. In Figure \ref{fig:nonparmixed_1} we see a boxplot of the data.

% H_0: $\mu_{EC}=\mu_{WC}_\mu{WO}$


<<nonparmixed_1, fig.height=4, echo=FALSE, fig.align='center', fig.cap='Boxplot of the imaginary speed skating data.'>>=
set.seed(01234)
athlete <- rep (seq(1:12), 3)  %>% as.factor()
occasion <- rep(c("EuropeanChampionships","WorldChampionships","Olympics"), each=6) %>% as.factor()
time <- rnorm(36, log(1017), .002 ) %>%  exp() %>%   round(2)  -1000
time <- time + rep( c(-0.2, 1, 0.2), each=12   )
time[  which(time==max(time))   ] <- 28
time[  which(time==min(time))   ] <- 27.9
datalong <- data.frame(athlete, occasion, time) %>% dplyr::arrange(athlete)
datawide <- datalong %>% tidyr::spread(occasion, time) %>% dplyr::arrange(athlete)
# datawide <- datawide[, c(1, 2,4,3)]
# names(datawide) <- c('patient','group','pre','post')
source('/Users/stephanievandenberg/Dropbox/Statistiek_Onderwijs/Data Analysis/writeForeignCode.R')
write.foreign(datalong,
              '/Users/stephanievandenberg/Dropbox/Statistiek_Onderwijs/Data Analysis/spss examples nonparmixed/nonparmixedspeedskate.sav',
              '/Users/stephanievandenberg/Dropbox/Statistiek_Onderwijs/Data Analysis/spss examples nonparmixed/nonparmixedspeedskate.sps',
              package = c("SPSS"))
datalong %>% ggplot( aes(x=occasion, y=time)  )  + geom_boxplot() + ylab("Time in minutes") + xlab("Occasion")
@

In order to test this null-hypothesis, we run a linear mixed model with dependent variable time, and independent variable occasion. We use random effects for the differences in speed across skaters. In Figure \ref{fig:nonparmixed_2} we see the residuals:

<<nonparmixed_2, fig.height=4, echo=FALSE, fig.align='center', warning=F, fig.cap='Residuals of the speedskating data with a linear mixed model.'>>=
res <- datalong %>%  lmer( time ~ occasion +  (1|athlete), data=. ) %>%
        resid()
datalong <- cbind(datalong, res)
datalong %>% ggplot( aes(x=occasion, y=res)    ) + geom_point() + ylab("Residual") + xlab("Occasion")
 @

From this plot we clearly see that the assumption of equal variance (homogeneity of variance) is violated: the variance of the residuals in the Worldchampionships condition is clearly smaller than the variance of the European championships condition. From the histogram of the residuals in Figure \ref{fig:nonparmixed_3} we also see that the distribution of the residuals is not bell-shaped: it is positively skewed (skewed to the right).




<<nonparmixed_3, fig.height=4, echo=FALSE, fig.align='center', warning=F, message=FALSE, fig.cap='Histogram of the residuals of the speedskating data with a linear mixed model.'>>=
datalong %>% ggplot( aes( x=res )  ) + geom_histogram() + xlab("Residual") + ylab("Count")
 @
% \\
% \\
Since the assumptions of homogeneity of variance and of normally distributed residuals are violated\footnote{Remember that assumptions relate to the population not samples: oftentimes your data set is too small to say anything about assumptions at the populationlevel. Residuals for a data set of 8 persons might show very nonnormal residuals, or very different variances for two subgroups of 4 persons each, but that might just be a coincidence, a random result because of the small sample size. If in doubt, it is best to use nonparametric methods.}, the results from the linear mixed model cannot be trusted. In order to answer our research question, we therefore have to resort to another kind of test. Here we discuss Friedman's test, a non-parametric test, for testing the null-hypothesis that the \textit{medians} of the three groups of data are the same. This Friedman test can be used in all situations where you have at least 2 levels of the within variable. In other words, you can use this test when you have data from three occasions, but also when you have data from 10 occassions or only 2. In the following section the Wilcoxon signed ranks test is discussed. This test is often used in social and behavioural sciences. The downside of this test is that it can only handle data sets with 2 levels of the within variable. In other words, it can only be used when we have data from two occassions. Friedman's test is therefore more generally applicable than Wilcoxon's. We therefore advise to always go with the Friedman test, but for the sake of completeness, we will also explain the Wilcoxon test.





\section{Friedman's test for $k$ measures}


Similar to many other nonparametric tests for testing the equality of medians, Friedman's test is based on ranks. Figure \ref{fig:nonparmixed_4} shows the speedskating data in wide format.


<<nonparmixed_4, fig.height=4, echo=FALSE, fig.align='center', results='asis'>>=
datawide %>%
        xtable(caption="The speedskating data in wide format.", label="tab:nonparmixed_4") %>%
        print(include.rownames=F, caption.placement = "top")
 @

We rank all of these time measures by determining the fastest time, then the next to fastest time, etcetera, until the slowest time. But because the data in each row belong together (we compare individuals with themselves), we do the ranking \textit{row-wise}. For each athlete separately, we determine the fastest time (1), the next fastest time (2), and the slowest time (3) and put the ranks in a table.


<<nonparmixed_5, fig.height=4, echo=FALSE, fig.align='center', results='asis'>>=
ranks <- apply(datawide[,2:4], 1,  function(x) rank(x) )
datawideranks<- datawide
datawideranks[,2:4] <- t(ranks)
datawideranks %>%
        xtable(caption="Row-wise ranks of the speedskating data.", label="tab:nonparmixed_5") %>%
        print(include.rownames=F, caption.placement = "top")
 @

From this table we see for example that athlete 1 had the fastest time on the European Championships (14.35, rank 1) and the slowest at the Olympics (16.42, rank 3).


Next we compute the sum of the ranks column-wise: the sum of the ranks for the European Championships data is \Sexpr{sum(datawideranks[2]) }, for the Olympic data it's \Sexpr{sum(datawideranks[3])} and for the World Championships data it is \Sexpr{sum(datawideranks[4])}.

From these sums we can gather that in general, these athletes showed their best times (many rank 1s) at the World Championships, as the sum of the ranks is lowest. We also see that in general these athletes showed their worst times (many rank 2s and 3s) at the European Championships, as the relevant column showed the highest sum of ranks.

In order to know whether these sums of ranks are significantly different from eachother, we may compute an $F_r$-value based on the following formula:


\begin{equation}
F_r = \left[  \frac{12}{Nk(k+1)} \Sigma^k_{j=1} S_j^2      \right] - 3N (k+1)
\end{equation}


In this formula, $N$ stands for the number of rows (12 athletes), $k$ stands for the number of columns (3 occasions), and $S_j^2$ stands for the squared sum of column $j$ ($31^2$, $26^2$ and $15^2$). If we fill in these numbers, we get:

\begin{eqnarray}
F_r &=& \left[  \frac{12}{12 \times  3(3+1)} \times (\Sexpr{sum(datawideranks[2]) }^2 + \Sexpr{sum(datawideranks[3]) }^2 + \Sexpr{sum(datawideranks[4]) }^2)      \right] - 3 \times 12 (3+1) \nonumber \\
  &=&   \left[  \frac{12}{144} \times  1862      \right] - 144 = 11.17  \nonumber
\end{eqnarray}



What can we tell from this $F_r$-statistic? In order to say something about significance, we have to know what values are to be expected under the null-hypothesis that there are no differences across the three groups of data. Suppose we randomly mixed up the data by taking all the speedskating times and randomly assigning them to the three contests and the twelve athletes, until we have a newly filled datamatrix in Table \ref{tab:nonparmixed_26}:

<<nonparmixed_26, fig.height=4, echo=FALSE, fig.align='center', results='asis'>>=
set.seed(234)
r <- sample ( 1:    (dim(datawide[-1])[1]*dim(datawide[-1])[2]), replace=F)
datawide_random1 <- datawide
datawide_random1[-1] <- matrix(  as.matrix(datawide[-1])[r] , dim(datawide[-1])[1], dim(datawide[-1])[2]) %>%  as.data.frame()
datawide_random1  %>%
        xtable(caption="The raw skating data in random order.", label="tab:nonparmixed_26") %>%
        print(include.rownames=F, caption.placement = "top")
 @

If we then compute $F_r$ for these mixed up data, we get another value. If we do this say 1000 times, we get the following values for $F_r$, summarized in the  histogram in Figure \ref{fig:nonparmixed_36}.

%fig.cap='Histogram of 1000 possible values for F_r given that the null-hypothesis is true, for 12 speedskaters.'

<<nonparmixed_36, fig.height=4, echo=FALSE, fig.align='center', warning=F, message=F,  fig.cap='Histogram of 1000 possible values for Fr given that the null-hypothesis is true, for 12 speedskaters.' >>=
set.seed(234)
F_r <- c()
for (i in 1:1000)
{
r <- sample ( 1:    (dim(datawide[-1])[1]*dim(datawide[-1])[2]), replace=F)
datawide_random1 <- datawide
datawide_random1[-1] <- matrix(  as.matrix(datawide[-1])[r] , dim(datawide[-1])[1], dim(datawide[-1])[2]) %>%  as.data.frame()

ranks <- apply(datawide_random1[,2:4], 1,  function(x) rank(x) )
datawideranks<- datawide_random1
datawideranks[,2:4] <- t(ranks)
sums <- apply(datawideranks[,2:4],2, sum )
squaredsums <- sums^2
SUM <- sum(squaredsums)
F_r[i] <- 12 / (12*3*4) *SUM - 3*12 *4
}
data <- as.data.frame(F_r)
data %>% ggplot( aes(F_r)  ) + geom_histogram()
@

So if the data is just randomly distributed over the three columns in the data matrix, we expect no systematic differences and so the null-hypothesis is true. So now we know what the distribution of $F_r$ looks like when the null-hypothesis is true. Remember that for the true data that we actually gathered, we found an $F_r$-value of 11.17. From the histogram, we see that only very few values of 11.17 or larger are observed when the null-hypothesis is true. If we look more closely, we find that only \Sexpr{table(F_r>11.17)[2]/length(F_r)*100}\% of the values are larger than 11.17, so we have a $p$-value of 0.004. The 95th percentile of these 1000 $F_r$-values is \Sexpr{quantile(F_r, 0.95)}, meaning that of the 1000 values for $F_r$, 5\% are larger than \Sexpr{quantile(F_r, 0.95)}. So if we use a signficance level of 5\%, our observed value of 11.17 is larger than the critical value for $F_r$, and we conclude that the null-hypothesis can be rejected.

Now this $p$-value of 0.004 and the critical value of \Sexpr{quantile(F_r, 0.95)} are based on our own computations. Actually there are better ways. One is to look up critical values of $F_r$ in tables, for instance in Kendall M.G. (1970) \textit{Rank correlation methods}. (fourth edition). The $p$-value corresponding to this $F_r$-value depends on $k$, the number of groups of data (here 3 columns) and $N$, the number of rows (12 individuals). If we look up that table, we find that for $k=3$ and $N=12$ the critical value of $F_r$ for a type I error rate of 0.05 equals 6.17. Our observed $F_r$-value of 11.17 is larger than that, therefore we can reject the null-hypothesis that the median skating times are the same at the three different championships. So we have to tell your friend that there are general differences in skating times at different contests, $F_r=11.17, p < 0.05$, but it is not the case that the fastest times were observed at the Olympics.

Another way is to make an approximation of the distribution of $F_r$. Note that the distribution in the histogram is very strangely shaped. The reason is that the data set is quite limited. Suppose we have not data on 12 speedskaters, but on 120. If we then randomly mix up data again and compute 1000 different values for $F_r$, we get the histogram in Figure \ref{fig:nonparmixed_46}.


<<nonparmixed_46, fig.height=4, echo=FALSE, fig.align='center', warning=F, message=F, fig.cap='Histogram of 1000 possible values for Fr given that the null-hypothesis is true, for 120 speedskaters.'>>=

set.seed(01234)
athlete2 <- rep (seq(1:120), 3)  %>% as.factor()
occasion2 <- rep(c("EuropeanChampionships","WorldChampionships","Olympics"), each=6) %>% as.factor()
time2 <- rnorm(360, log(1017), .002 ) %>%  exp() %>%   round(2)  -1000
time2 <- time2 + rep( c(-0.2, 1, 0.2), each=120   )
time2[  which(time2==max(time2))   ] <- 28
time2[  which(time2==min(time2))   ] <- 27.9
datalong2 <- data.frame(athlete2, occasion2, time2) %>% dplyr::arrange(athlete2)
datawide2 <- datalong2 %>% tidyr::spread(occasion2, time2) %>% dplyr::arrange(athlete2)


F_r <- c()
for (i in 1:1000)
{
r <- sample ( 1:    (dim(datawide2[-1])[1]*dim(datawide2[-1])[2]), replace=F)
datawide_random12 <- datawide2
datawide_random12[-1] <- matrix(  as.matrix(datawide2[-1])[r] , dim(datawide2[-1])[1], dim(datawide2[-1])[2]) %>%  as.data.frame()

ranks2 <- apply(datawide_random12[,2:4], 1,  function(x) rank(x) )
datawideranks2<- datawide_random12
datawideranks2[,2:4] <- t(ranks2)
sums2 <- apply(datawideranks2[,2:4],2, sum )
squaredsums2 <- sums2^2
SUM2 <- sum(squaredsums2)
F_r[i] <- 12 / (120*3*4) *SUM2 - 3*120 *4
}
data <- as.data.frame(F_r)
data %>% ggplot( aes(F_r)  ) + geom_histogram()
@

The shape becomes more regular. It also starts to resemble another distribution, that of the $\chi^2$ (chi-square). It can be shown that the distribution of the $F_r$ for a large number of rows in the data matrix, and at least 6 columns, approaches the shape of the $\chi^2$-distribution with $k-1$ degrees of freedom. This is shown in Figure \ref{fig:nonparmixed_56}.

<<nonparmixed_56, fig.height=4, echo=FALSE, fig.align='center', warning=F, message=F, fig.cap='The distrbution of Fr under the null-hypothesis, overlain with a chi-square distribution with 2 degrees of freedom.'>>=
data <- dplyr::data_frame(
  F_r = F_r,
  chi = dchisq(F_r, df=2)
)

data %>% ggplot( aes(x=F_r)  ) + geom_histogram(aes(y=..density..)) + geom_line(aes(y = chi))
 @

The line of the $\chi^2$-distribution with 2 degrees of freedom approaches the histogram quite well, but not perfectly. In general, for large $N$ and $k>5$, the approximation is good enough. In that way it gets easier to look up $p$-values for certain $F_r$-values, because the $\chi^2$-distribution is well-known\footnote{The $\chi^2$-distribution is based on the normal distribution: the $\chi^2$-distribution with $k$ degrees of freedom is the distribution of a sum of the squares of $k$ independent standard normal random variables.}, so we don't have to look up critical values for $F_r$ in old tables. For a significance level of 5\%, the critical value of a $\chi^2$ with 2 degrees of freedom is \Sexpr{round(qchisq(df=2, p=0.95), 3)}. This is close to the value in the table for $F_r$ in old books: 6.17. The part of the $\chi^2$-distribution with 2 degrees of freedom that is larger than the observed 11.17 is \Sexpr{pchisq(11.17,df=2, lower.tail=F)}, so our approximate $p$-value for our null-hypothesis is \Sexpr{round(pchisq(11.17,df=2, lower.tail=F),3)}.


\section{How to perform Friedman's test in SPSS}

First of all, you need data in wide format. If your data happens to be in long format, use the CASETOVARS procedure to get the data in wide format. CASETOVARS requires your data to be ordered, so use the SORT CASE BY procedure before CASETOVARS. Suppose your data is in long format, as in Table \ref{tab:nonparmixed_6}.

<<nonparmixed_6, fig.height=4, echo=FALSE, fig.align='center', warning=F, results="asis">>=
occasion_old<- datalong$occasion
datalong$occasion <-  datalong$occasion %>% as.numeric()
head(datalong[1:3]) %>%
        xtable(caption="The raw skating data in long data format.", label="tab:nonparmixed_6") %>%
        print(include.rownames=F, caption.placement = "top")
 @


Then the following syntax turns the data into wide format:


\begin{verbatim}
SORT CASES BY athlete occasion.
CASESTOVARS
  /ID=athlete
  /INDEX=occasion
  /GROUPBY=VARIABLE
 /SEPARATOR = "_".
\end{verbatim}


This creates the wide format data matrix in Table \ref{tab:nonparmixed_7}:


<<nonparmixed_7, fig.height=4, echo=FALSE, fig.align='center',results='asis' >>=
names(datawide)[2:4] <- c("time_1.00", "time_2.00","time_3.00")
datawide %>%
        xtable(caption="The raw skating data in wide data format after CASETOVARS", label="tab:nonparmixed_7") %>%
        print(include.rownames=F, caption.placement = "top")
@

Note the variable names: they start with the dependent variable time and are then indexed by the number of the occasion, 1.00, 2.00 and 3.00, that relate to European Championships, Olympic Games and World Championships, respectively.

We can then specify that we want Friedman's test by using the NPAR TESTS procedure with the FRIEDMAN subcommand and indicating which variables we want to use:

\begin{verbatim}
NPAR TESTS
/FRIEDMAN=time_1.00 time_2.00 time_3.00.
\end{verbatim}

\begin{figure}[h]
    \begin{center}
       \includegraphics[scale=0.8, trim={0cm 22cm 0cm 0cm}]{/Users/stephanievandenberg/Dropbox/Statistiek_Onderwijs/Data" "Analysis/spss" "examples" "nonparmixed/friedman1.pdf}
    \end{center}
     \caption{SPSS output of the Friedman test.}
    \label{fig:friedman1}
\end{figure}


In the output in Figure \ref{fig:friedman1} you first see the mean ranks. Note that if you multiply these by 12 (the number of rows), you get the sum of the ranks per column that we also computed above. Next you see a chi-square statistic, degrees of freedom, and an asymptotic $p$-value (Asymp. Sig.). Why don't we see an $F_r$-statistic?

The reason is, as discussed in the previous section, that for large number of measurements (columns) and a large number of individuals (rows), the $F_r$ statistic tends to behave like a chi-square, $\chi^2$, with $k-1$ degrees of freedom. So what we are looking at in this output is really an $F_r$-value of 11.17 (exactly the same value as we computed by hand in the previous section). In order to approximate the $p$-value, this value of 11.17 is interpreted as a chi-square ($\chi^2$), which with 2 degrees of freedom has a $p$-value of 0.004.


This asymptotic (approximated) $p$-value is the correct $p$-value if you have a lot of rows (large $N$) and at least 6 variables ($k>5$). If you do not have that, as we have here, this asymptotic $p$-value is only what it is: an approximation. If you want to have the exact $p$-value, then do

\begin{verbatim}
NPAR TESTS
/FRIEDMAN=time_1.00 time_2.00 time_3.00
/METHOD=EXACT.
\end{verbatim}

and then use the $p$-value under $exact sign.$, in this case 0.002, see Figure \ref{fig:friedman2}.

\begin{figure}[h]
    \begin{center}
       \includegraphics[scale=0.8, trim={0cm 22cm 0cm 0cm}]{/Users/stephanievandenberg/Dropbox/Statistiek_Onderwijs/Data" "Analysis/spss" "examples" "nonparmixed/friedman2.pdf}
    \end{center}
    \caption{SPSS output of the Friedman test with the exact p-value.}
    \label{fig:friedman2}
\end{figure}


Thus, a Friedman's test of equal medians showed that speedskaters show significantly different median times on the 10 kilometer distance at the three types of contests, $F_r=11.17, p=0.002$.



\section{Wilcoxon's signed ranks test for 2 measures}

Friedman's test can be used for 2 measures, 3 measures or even 10 measures. As stated earlier, the well-known Wilcoxon's test can only be used for 2 measures. For completeness, we also discuss that test here.
\\
\\
For each athlete, we take the difference in skating times and call it $d$, see Table \ref{tab:nonparmixed_77}. Next we rank these $d$-values, irrespective of sign, and call these ranks $rank_d$. From the table \ref{nonparmixed_77} we see that athlete 12 shows the smallest difference in skating times ($d$= 0.06, rank = 1) and athlete 2 the largest difference.

<<nonparmixed_77, fig.height=4, echo=FALSE, fig.align='center',results='asis'>>=
names(datawide)[2:4] <- c("EuropeanChampionsips", "Olympics","WorldChampionships")
datawilcoxon <-
        datawide[,c(1,3:4)] %>%  mutate(d = Olympics-WorldChampionships  )   %>%
        mutate(rank_d =  rank(abs(d))   )  %>%
        mutate(ranksign =  ifelse(d>0, rank_d,-rank_d   ))
datawilcoxon %>%
        xtable(caption="The raw skating data and the computations for Wilcoxon signed ranks test", label="tab:nonparmixed_77") %>%
         print(include.rownames=F, caption.placement = "top")
 @

Next we indicate for each rank whether it belongs to a positive or a negative difference $d$ and call that variable \textbf{ranksign}.

Under the null-hypothesis, we expect that some of the larger $d$-values are positive and some of them negative, in a fairly equal amount. If we sum the ranks having plus-signs and sum the ranks having minus-signs, we would expect that these two sums are about equal, but only if the null-hypothesis is true. If the sums are very different, then we should reject this null-hypothesis. In order to see if the difference in sums is too large, we compute them as follows:


\begin{eqnarray}
T^+ &=& 5+ 12 + 8 +10+6+11+4 +2 +7 +1 = \Sexpr{ sum (   datawilcoxon$ranksign[datawilcoxon$ranksign>0])     } \nonumber \\
T^- &=& 3 + 9= \Sexpr{ -1 * sum (   datawilcoxon$ranksign[datawilcoxon$ranksign<0])     } \nonumber
\end{eqnarray}



To know whether $T^+$ is significantly larger than $T^-$, the value of $T^+$ can be looked up in a table, for instance in Siegel \& Castellan (1988). There we see that for $T^+$, with 12 rows, the probability of obtaining a $T^+$ of at least 66 is 0.0171. For a two-sided test (if we would have switched the columns of the two championships, we would have gotten a $T^-$ of 66 and a $T^+$ of 12!), we have to double this probability. So we end up with a $p$-value of $2 \times 0.0171=\Sexpr{2*0.0171}$.


In the table we find no critical values for large sample size $N$, but fortunately, similar to the Friedman test, we use an approximation using the normal distribution. It can be shown that for large sample sizes, the statistic $T^+$ is approximately normally distributed with mean


\begin{equation}
\mu = \frac{N(N+1)}{4}
\end{equation}

and variance:

\begin{equation}
\sigma^2= \frac {N(N+1)(2N+1)  }  {24}
\end{equation}


If we therefore standardize the $T^+$ by subtracting the $\mu$ and then dividing by the square root of the variance $\sqrt(\sigma^2)=\sigma$, we get a $Z$-value with mean 0 and standard deviation 1. To do that, we use the following formula:

\begin{equation}
Z = \frac{T^+ - \mu}{\sigma} =  \frac  { T^+ - N(N+1)/4} {\sqrt{N(N+1)(2N+1)/24}}
\end{equation}


Here $T^+$ is 66 and $N$ equals 12, so if we fill in the formula we get $Z= \Sexpr{ (66 - 39) / sqrt(162.5)   }$. From the standard normal distribution we know that 5\% of the observations lie above 1.96 and below -1.96. So a value for $Z$ larger than 1.96 or smaller than -1.96 is enough evidence to reject the null-hypothesis. Here our $Z$-statistic is larger than 1.96, therefore we reject the null-hypothesis that the median skating times are the same at the World Championships and the Olympics. The $p$-value associated with a $Z$-score of \Sexpr{ (66 - 39) / sqrt(162.5)} is \Sexpr{ round(pnorm((66 - 39) / sqrt(162.5), lower.tail=F)*2, 3)}.





\section{How to perform Wilcoxon's signed ranks test in SPSS}

If you want to use the Wilcoxon test, then use the following syntax:

\begin{verbatim}
NPAR TESTS
/WILCOXON=time_2.00 time_3.00
/METHOD=EXACT.
\end{verbatim}


\begin{figure}[h]
    \begin{center}
       \includegraphics[scale=0.8, trim={0cm 18cm 0cm 0cm}]{/Users/stephanievandenberg/Dropbox/Statistiek_Onderwijs/Data" "Analysis/spss" "examples" "nonparmixed/wilcoxon1.pdf}
    \end{center}
    \caption{SPSS output of the Wilcoxon test.}
    \label{fig:wilcoxon1}
\end{figure}

In the output in Figure \ref{fig:wilcoxon1} we see a $Z$-statistic, an asymptotic $p$-value, and two exact $p$-values. The reason that we see a $Z$-statistic is that the Wilcoxon $T^+$ statistic approaches a normal distribution in case we have a large number of observations (many rows). If $N>15$, the approximation is good enough so that the statistic can be interpreted as a $z$-score (standardized score with a normal distribution). That means that a $z$-score of 1.96 or larger or -1.96 or smaller can be regarded as significant at the 5\% significance level. Since the standard normal distribution is only an approximation, and we have $N=12$, we have to look at the exact significance level, which is in this case 0.034. We see that the exact $p$-value is in this case equal to the approximate $p$-value. Note that we use a two-sided test, to allow for the fact that random sampling could lead to a higher median for the Olympic Games or a higher median for the World Championships. We just want to know whether the null-hypothesis that the two medians differ can be rejected (in whatever direction) or not.
\\
\\


Let's compare the output with the Friedman test, but then only use the relevant variables in your syntax:

\begin{verbatim}
NPAR TESTS
/FRIEDMAN=  time_2.00 time_3.00
/METHOD=EXACT.
\end{verbatim}


In the output in Figure \ref{fig:friedman3} we see that the null-hypothesis of equal medians at the World Championships and the Olympic Games can be rejected, with a $p$-value of 0.039.

\begin{figure}[h]
    \begin{center}
       \includegraphics[scale=0.8, trim={0cm 22cm 0cm 0cm}]{/Users/stephanievandenberg/Dropbox/Statistiek_Onderwijs/Data" "Analysis/spss" "examples" "nonparmixed/friedman3.pdf}
    \end{center}
    \caption{SPSS output of the Friedman test for two measures.}
    \label{fig:friedman3}
\end{figure}



Note that both the Friedman and Wilcoxon tests come up with very similar $p$-values. Their rationales are very similar: Friedman's test is based on ranks and Wilcoxon's test is based on positive and negative differences between measures 1 and 2, so in fact ranks 1 and 2 for each row in the data matrix. Both can therefore be used in the case you have two measures. We recommend to use the Friedman test, since that test can be used in all situations where you have 2 or more measures per row. Wilcoxon's test can only be used if you have 2 measures per row.
\\
\\
In sum, we can report in two ways on our hypothesis regarding similar skating times at the World Championships and at the Olympics:

\begin{enumerate}

\item

\begin{quotation}
A Friedman test showed a significant difference between the 10km skating times at the World Championships and at the Olympics, $F_r = 5.33, p=0.04$. Athletes more often show their fastest times at the World Championships than can be expected due to chance.
\end{quotation}

\item

\begin{quotation}
A Wilcoxon signed ranks test showed a significant difference between the 10km skating times at the World Championships and at the Olympics, $Z = -2.12, p=0.03$. Athletes more often show their fastest times at the World Championships than can be expected due to chance.
\end{quotation}

\end{enumerate}

How do we know that the fastest times were at the World Championships? If we look at raw data above, that does not seem that obvious. But this conlusion is based on the sum of ranks: we saw a sum of ranks of 26 for the Olympics and 15 for the World Championships. So the average rank is lower at the World Championships.


\section{Ties}

Many nonparametric tests are based on ranks. For example, if we have the data sequence {0.1, 0.4, 0.5, 0.2}, we give these values the ranks {1, 3, 4, 2}, respectively. But in may data cases, data sequences cannot be ranked unequivocally. Let's look at the sequence {0.1, 0.4, 0.4, 0.2}. Here we have 2 values that are exactly the same. We say then that we have \textit{ties}. If we have ties in our data like the 0.4 in this case, one very often used option is to arbitrarily choose one of the 0.4 values as smaller than the other, and then average the ranks. Thus, we rank the data into {1, 3, 4, 2} and then average the tied observations: {1, 3.5, 3.5, 2}. As another example, suppose we have the sequence {23, 54, 54, 54, 19}, we turn this into ranks {2, 3, 4, 5, 1} and take the average of the ranks of the tied observations of 54: {2, 4, 4, 4, 1}. These ranks corrected for ties can then be used to compute the test statistic, for instance Friedman's $F_r$ or Wilcoxon's $Z$. However, in many cases, because of these corrections, a slightly different formula is to be used. So the formulas become a little bit different. This is all done in SPSS automatically. If you want to know more, see Siegel and Castellan (1988).




\section{Exercises}


A researcher is interested in the relationship between mood and day of the week: are people generally moodier on Monday than on Wednesday or Friday?

Below we see the data on 4 people that rated their mood from 1 (very moody) to 10 (not moody at all) on three separate days in a week in February: Day 1 is Monday, day 2 is Wednesday and day 3 is Friday:

<<nonparmixed_8, fig.height=4, echo=FALSE, fig.align='center'>>=
set.seed(1234565910)
ID <- rep(c(1,2,3,4), each=3)
Day <- rep (c(1, 2, 3), 4)
Mood <- rpois(12, 5)
data <- data.frame(ID, Day, Mood)
data %>% kable()
source('/Users/stephanievandenberg/Dropbox/Statistiek_Onderwijs/Data Analysis/writeForeignCode.R')
write.foreign(data,
              '/Users/stephanievandenberg/Dropbox/Statistiek_Onderwijs/Data Analysis/spss examples nonparmixed/nonparmixedmood.sav',
              '/Users/stephanievandenberg/Dropbox/Statistiek_Onderwijs/Data Analysis/spss examples nonparmixed/nonparmixedmood.sps',
              package = c("SPSS"))
@

\begin{enumerate}

\item Put the data into wide format, and think of appropriate variable names
\\
 \\
 \begin{tabular}{llrrrr}
   & \dots & \dots  & \dots & \dots  & \dots  \\ \hline
  & \dots & \dots  & \dots & \dots  & \dots  \\
  & \dots & \dots  & \dots & \dots  & \dots  \\
  & \dots & \dots  & \dots & \dots  & \dots  \\
  & \dots & \dots  & \dots & \dots  & \dots  \\
  & \dots & \dots  & \dots & \dots  & \dots  \\
  & \dots & \dots  & \dots & \dots  & \dots  \\
  & \dots & \dots  & \dots & \dots  & \dots  \\
  & \dots & \dots  & \dots & \dots  & \dots  \\
  & \dots & \dots  & \dots & \dots  & \dots  \\
  & \dots & \dots  & \dots & \dots  & \dots  \\
  & \dots & \dots  & \dots & \dots  & \dots  \\
  & \dots & \dots  & \dots & \dots  & \dots  \\
 \end{tabular}
\\
\\
\item Rank these data row-wise: for each row determine the lowest mood (1), the second lowest mood (2) and the highest mood score (3)
\\
 \\
 \begin{tabular}{llrrrr}
   & \dots & \dots  & \dots & \dots  & \dots  \\ \hline
  & \dots & \dots  & \dots & \dots  & \dots  \\
  & \dots & \dots  & \dots & \dots  & \dots  \\
  & \dots & \dots  & \dots & \dots  & \dots  \\
  & \dots & \dots  & \dots & \dots  & \dots  \\
  & \dots & \dots  & \dots & \dots  & \dots  \\
  & \dots & \dots  & \dots & \dots  & \dots  \\
  & \dots & \dots  & \dots & \dots  & \dots  \\
  & \dots & \dots  & \dots & \dots  & \dots  \\
  & \dots & \dots  & \dots & \dots  & \dots  \\
  & \dots & \dots  & \dots & \dots  & \dots  \\
  & \dots & \dots  & \dots & \dots  & \dots  \\
  & \dots & \dots  & \dots & \dots  & \dots  \\
 \end{tabular}
\\
\\
\item Determine the column sums: the sum of the ranks for Monday, Wednesday and Friday.
\item How many rows do you have ($N$) and how many columns of data do you have ($k$)?
\item Compute $F_r$.
\item Copy the data into SPSS and run a Friedman's test. Should you ask for an exact $p$-value? Provide the syntax.
\item Suppose you get the SPSS output in Figure \label{ref:friedmanmood1}. What would your conclusion be regarding the research question about the relationship between moodiness and the day of the week?

\begin{figure}[h]
    \begin{center}
       \includegraphics[scale=0.8, trim={0cm 20cm 0cm 0cm}]{/Users/stephanievandenberg/Dropbox/Statistiek_Onderwijs/Data" "Analysis/spss" "examples" "nonparmixed/friedmanmood1.pdf}
    \end{center}
    \caption{SPSS output of a Friedman test.}
    \label{fig:friedmanmood1}
\end{figure}

\item
In this data set, for which day did we observe the personal best mood? How many of the individuals showed their best mood on that day?


\item
A linear mixed model was run on this data set. When checking model assumptions, we saw the following graphs in Figures \ref{fig:nonparmixed_11a} and \ref{fig:nonparmixed_11b}.

<<nonparmixed_11a, fig.height=4, echo=FALSE, fig.align='center', warning=F, message=FALSE, fig.cap='Residual plot after a linear mixed model analysis.'>>=
res <- data %>%  lmer( Mood ~ as.factor(Day) +  (1|ID), data=. ) %>%
        resid()
datalong <- cbind(data, res)
datalong %>% ggplot( aes(x=as.factor(Day), y=res)    ) + geom_point()  + xlab("Day") + ylab("Residual")
datalong %>% ggplot( aes( x=res )  ) + geom_histogram() + xlab("Residual")
 @

<<nonparmixed_11b, fig.height=4, echo=FALSE, fig.align='center', warning=F, message=FALSE, fig.cap='Histogram of residuals after a linear mixed model analysis.'>>=
datalong %>% ggplot( aes( x=res )  ) + geom_histogram() + xlab("Residual")
 @


Would you prefer to stick to the Friedman's test for this data set, or would you prefer to report a linear mixed model? Explain your answer.

\item Could you have performed a Wilcoxon test on these data? Why, or why not?

\end{enumerate}


Answers:
\begin{enumerate}

\item
The raw data in wide format:
<<nonparmixed_9, fig.height=4, echo=FALSE, fig.align='center'>>=
data <- data %>% dplyr::arrange(ID)
datawide <- data %>% tidyr::spread(Day, Mood) %>% dplyr::arrange(ID)
names(datawide) <- c("ID", "Mood_1", "Mood_2", "Mood_3")
datawide %>% kable()
@

\item
The row-wise ranked data:
<<nonparmixed_10, fig.height=4, echo=FALSE, fig.align='center'>>=
ranks <- apply(datawide[,2:4], 1,  function(x) rank(x) )
datawideranks<- datawide
datawideranks[,2:4] <- t(ranks)
datawideranks %>% kable()
 @
\item Day 1: \Sexpr{sum(datawideranks[2]) }, Day 2: \Sexpr{sum(datawideranks[3])} and Day3: \Sexpr{sum(datawideranks[4])}.
\item $N=4$ and $k=3$
\item

\begin{eqnarray}
F_r &=& \left[  \frac{12}{4 \times  3(3+1)} \times (\Sexpr{sum(datawideranks[2]) }^2 + \Sexpr{sum(datawideranks[3]) }^2 + \Sexpr{sum(datawideranks[4]) }^2)      \right] - 3 \times 4 (3+1) \nonumber \\
  &=&   \left[  \frac{12}{48} \times  \Sexpr{sum(datawideranks[2])^2 +sum(datawideranks[3])^2+sum(datawideranks[4])^2  }      \right] - 48 = 1.50  \nonumber
\end{eqnarray}

\item

\begin{verbatim}
NPAR TESTS
/FRIEDMAN=  Mood_1   Mood_2    Mood_3
/METHOD=Exact.
\end{verbatim}

\item
\begin{quotation}
We found no significant effect of day of the week on mood, $F_r=1.50, p=0.65$, so the null-hypothesis of equal mood during the week is not rejected. Note however that the sample size was extremely small (12 data points), so even if there is a real relationship between mood and day of the week, there was little chance to find evidence of that in this data set.
\end{quotation}

\item The highest column sum of the ranks was found for day 2, which was Wednesday. So in this data set we saw that the four individuals generally showed their personal highest mood score on Wednesday. Actually, 2 persons out of 4 showed their highest score (rank 3) on Wednesday (ID=2 and ID=3).

\item The plots suggests that the variance of the residuals is very small for the second day, compared to the other two days. The distribution is also hardly normal. But it is hard to tell whether the assumptions are reasonable, since there are so few data points. It would therefore be safest to report a Friedman test.

\item A Wilcoxon test can only be performed on two measures, say Monday and Wednesday data, or Monday and Friday data. You could not test the null-hypothesis of the same moods on three days with a Wilcoxon test.

\end{enumerate}









\chapter{Generalized linear models}

\section{Introduction}
In previous chapters we were introduced to the linear model, with its basic form


\begin{eqnarray}
y = b_0 + b_1 X_1 + \dots + b_n X_n + e \\
e \sim N(0, \sigma_e^2)
\end{eqnarray}

The basic assumptions of this model is the linearity in the parameters, and the normally distributed residual $e$. Linearity in the parameters means that the effects of intercept and the independent variables $X_1 \dots X_n$ are additive: the assumption is that you can sum these effects to come to a predicted value for $y$. So that is also true when we include interaction effects to account for moderation effects, 

\begin{eqnarray}
y = b_0 + b_1 X_1 +  b_2 X_2 + b_3 X_1 X_2 + e \\
e \sim N(0, \sigma_e^2)
\end{eqnarray}


or when we use a quadratic term to account for other types of nonlinearity in the data:


\begin{eqnarray}
y = b_0 + b_1 X_1 +  b_2 X_1 X_1 + e \\
e \sim N(0, \sigma_e^2)
\end{eqnarray}

In all these models, the assumption is that the effects of the parameters can be added to one another. 

The other major assumption of linear (mixed) models is the normal distribution of the residuals. As we have seen in for instance the previous chapter, sometimes the residuals are not normally distributed. Remember that with a normal distribution $N(0,\sigma^2)$, in principle all values between $-\infty$ and $+\infty$ are possible, but they tend to concentrate around the value of 0, in the shape of the bell-curve. Figure \ref{fig:gen_1} shows the normal distribution $N(0,\sigma^2=4)$: it is centered around 0 and has variance 4. Note that the inflection point, that is the point where the decrease in density tends to decelerate, is exactly at the values -2 and +2. These are equal to the square root of the variance, which is the standard deviation, $+\sigma$ and $-\sigma$.


<<gen_1, fig.height=4, echo=FALSE, fig.align='center', fig.cap='Density function of the normal distribution, with mean 0 and variance 4 (standard deviation 2). Inflection points are positioned at residual values of minus 1 standard deviation and plus 1 standard deviation.'>>=
residual <- seq (-6,6,0.1)
density <- dnorm(residual, 0, 2)
data <- data.frame(residual, density)
data %>% ggplot(aes(x=residual, y=density)) + geom_line() + 
        geom_vline(xintercept = -2, col='blue') +geom_vline(xintercept = 2, col='blue') +
        geom_label(  aes(x=5, y=dnorm(2,0,2)), col='black', label='Inflection point', show.legend = F) +
        geom_segment(aes(x=3.5, xend=2.1, y=dnorm(2,0,2), yend=dnorm(2,0,2)), size = 1,
               arrow = arrow(length = unit(0.1, "cm")))+
        geom_label(  aes(x=-5, y=dnorm(2,0,2)), col='black', label='Inflection point', show.legend = F) +
        geom_segment(aes(x=-3.5, xend=-2.1, y=dnorm(2,0,2), yend=dnorm(2,0,2)), size = 1,
               arrow = arrow(length = unit(0.1, "cm"))) + scale_x_continuous(breaks=seq (-6,6,1))
        @

A normal distribution is suitable for continuous data: for example a variable that can take all possible values between -1 and 0. For many data sets this is not true. Think for example of temperature measures: if the thermometer gives degrees centigrade with a precision of only 1 decimal, we can never have values of say 10.07 or -56.789. Our data will in fact be \textit{discrete}, showing rounded values like 10.1, 10.2, 10.3, but no values in between. 

Nevertheless, the normal distribution can still be used in many such cases. Take for instance a data set where the temperature in Amsterdam in summer was predicted on the basis of a linear model. Fig \ref{fig:gen_2} shows the distribution of the residuals of that model:

<<gen_2, fig.height=4, echo=FALSE, fig.align='center', message=F, fig.cap='Even if residuals are really discrete, the normal distribution can be a good approximation of their distribution.'>>=
set.seed(1234)
residual <- rnorm(1000, 0, 3) %>%  round(1)
density <- dnorm(residual, 0, 3)
data <- data.frame(residual, density)
data %>% ggplot(aes(x=residual, y=density)) + geom_histogram(aes(y=..density..))  + geom_line(aes(y=density))
@


The temperature measures were discrete with a precicsion of one tenth of a degree centigrade, but the distribution seems well approximated by a normal curve. 


But let's look at an example where the discreteness is more prominent. In Figure \ref{fig:gen_3} we see the residuals of an analysis of exam results. Students had to do an asssignment that had to meet 4 criteria: 1) originality, 2) language, 3) structure, and 4) literature review. Each criterion was scored as either fulfilled (1) or not fulfilled (0). The score for the assignment was given on the basis of \textit{the number of criteria} that were met, so the scores could be 0, 1, 2, 3 or 4. The score was predicted on the basis of the average exam score on previous assignments using a linear model. 


<<gen_3, fig.height=4, echo=FALSE, fig.align='center', message=F, fig.cap='Count data example where the normal distribution is not a good approximation of the distribution of the residuals.'>>=
set.seed(1234)
score <- rnorm(100, 1, 3) %>%  round(0) 
score [score>4] <- 4
score [score<0] <- 0
x = rnorm(100, 0, 1)
data=data.frame(score, x)
residual <-  lm(score ~ x, data=data   )$res
density <- dnorm(residual, mean(residual), sd(residual))
data <- data.frame(residual, density)
data %>% ggplot(aes(x=residual, y=density)) + geom_histogram(aes(y=..density..))  + geom_line(aes(y=density))
@


Figure \ref{fig:gen_3} shows that the residuals are very discrete, and that the continous normal distribution is a very bad approximation of the histogram. We often seen this phenomenon when our data consists of \textit{counts} with a limited maximum number. 

An even more extreme case we observe when our dependent variable consists of whether or not students passed the assignment: only those assignments that fulfilled all 4 criteria are regarded as sufficient. If we score all students with a sufficient assignment as passed (1) and all students with an insufficient assignment as failed (0) and we predict this again by the average exam score on previous assignments using a linear model, we get the residuals displayed in Figure \ref{fig:gen_4}.


<<gen_4, fig.height=4, echo=FALSE, fig.align='center', message=F, fig.cap='Dichotomous data example where the normal distribution is not a good approximation of the distribution of the residuals.'>>=
set.seed(1234)
score <- rnorm(100, 1, 3) %>%  round(0) 
score [score<=4] <- 0
score [score>3] <- 1
x = rnorm(100, 0, 1)
data=data.frame(score, x)
residual <-  lm(score ~ x, data=data   )$res
density <- dnorm(residual, mean(residual), sd(residual))
data <- data.frame(residual, density)
data %>% ggplot(aes(x=residual, y=density)) + geom_histogram(aes(y=..density..))  + geom_line(aes(y=density))
@


Here it is definitely evident that a normal approximation of the residuals will not do. When the dependent variable has only 2 possible values, a linear model will never work because the residuals can never have a distribution that is even remotely looking normal. 

In this chapter we will discuss how generalized linear models can be used to analyze data sets where the assumption of normally distributed residuals is not tenable. First we discuss the case where the dependent variable has only 2 possible values (dichotomous dependent variables like yes/no or pass/fail, heads/tails, 1/0). Next, we will discuss the case where the dependent variable consists of counts ($1, 2, 3, 4, \dots$).


\section{Logistic regression}

Imagine that we analyze results on an exam for third grade children. These children are usually either 6 or 7 years old, dependending on what month they were born in. The exam is on February 1st. A researcher wants to know whether the age of the child can explain why some children pass the test and others fail. She computes the age of the child in months. Each child that passes the exam gets a score 1 and all the others get a score 0. Figure \ref{fig:gen_5} plots the data.

<<gen_5, fig.height=4, echo=FALSE, fig.align='center', message=F, fig.cap='Data example: Exam outcome (score) as a function of age, where 1 means pass and 0 means fail.'>>=
set.seed(12345)
age = rnorm(100, 78, 3) %>% round(1)
logoddsscore <- 0.51*(age-78) %>%  round(0)
score <- exp(logoddsscore)/ (1+logoddsscore)
score [score<=4] <- 0
score [score>3] <- 1
data.exam <- data.frame(score, age)
data.exam %>% ggplot(aes(x=age, y=score)) + geom_point() + xlab("age in months")
@


She wants to use the following linear model:

\begin{eqnarray}
score = b_0 + b_1 age  + e \\
e \sim N(0, \sigma_e^2)
\end{eqnarray}

Figure \ref{fig:gen_6} shows the estimated regression line and Figure \ref{fig:gen_7} shows the distribution of the residuals as a function of age.

<<gen_6, fig.height=4, echo=FALSE, fig.align='center', message=F, , fig.cap='Example exam data with a linear regression line.'>>=
data.exam %>% ggplot(aes(x=age, y=score)) + geom_point() + xlab("age in months") + geom_smooth(formula=y~x, method="lm", se=F)
@


<<gen_7, fig.height=4, echo=FALSE, fig.align='center', message=F, fig.cap='Residuals as a function of age, after a linear regression analysis of the exam data.'>>=
residual <-  lm(score ~ age, data=data.exam   )$res
data.exam <- data.frame(residual, age, score)
data.exam %>% ggplot(aes(x=age, y=residual)) + geom_point() + xlab("age in months")
outglm <- glm(score~ age, data=data.exam)
@

Clearly a linear model is not appropriate. Here, the assumption that the dependent variable, score in this case, is scattered randomly around the predicted value with a normal distribution is not reasonable. The main problem is that the dependent variable score can only have 2 values, 0 and 1.


\subsection{Bernoulli distribution}

Rather than using a normal distribution, we could try a Bernoulli distributiuon. The Bernoulli distribution is the distribution of a coin flip. For example, if the probability of heads is 0.1, we can expect that if we flip the coin, on average we expect to see $0.1$ times heads and 0.9 times tails. Our best bet then is that the outcome is tails. However, if we actually flip the coin, we might see heads anyway. There is some randomness to be expected. Let $y$ be the outcome of a coin flip: heads or tails. If we have a Bernoulli distribution for variable $y$ with probability $p$ for heads, we \textit{expect} to see heads $p$ times, but we actually \textit{observe} heads or tails.

\begin{equation}
y \sim Bern(n, p)
\end{equation}

The same is true for the normal distribution in the linear model case: we \textit{expect} that the observed value of $y$ is exactly equal to its predicted value ($b_0 + b_1 X$), but we always \textit{observe} that it is different.

\begin{equation}
y \sim N(\mu= b_0 + b_1 X, \sigma^2_e)
\end{equation}

In our example, the pass rate could also be conceived as the outcome of a coin flip: pass instead of heads and fail instead of tails. So would it be an idea to predict the \textit{probability} of success on the basis of age? And then for every predicted probability, we allow for the fact that actually the observed success can differ. Our linear model could then look like this:


\begin{eqnarray}
p_i = b_0 + b_1 age_i \\
score_i \sim Bern(p_i)
\end{eqnarray}

So for each child $i$, we predict the probability of success, $p_i$, on the basis of her/his age. Next, the randomness in the data comes from the fact that a probability is only a probability, so that the observed success of a child $score_i$, is like a coin toss with probability of $p_i$ for success. 

For example, suppose that we have a child with an age of 80 months, and we have $b_0=-3.8$ and $b_1=0.05$. Then the predicted probability $p_i$ is equal to $-3.8 + 0.05 \times 80 = 0.20$. The best bet for such a child would be that it fails the exam. But 0.20 is only a probability, so by chance the child could pass the exam. This model also means that if we would have 100 children of age 80 months, we would \textit{expect} that 20 of these children would pass the test and 80 would fail.  But we can't make predictions for one individual alone: we don't know which child exactly will pass and which child won't. Note that this is similar to the normally distributed residual in the linear model: in the linear model we expect a child to have a certain value for $y$, but we know that there will be a deviation from this predicted value: the residual. For a whole group of children with the same predicted value for $y$, we know that the whole group will show residuals that have a normal distribution. But we're not sure what the residual will be for each individual child.

Unfortunately, this model for probabilities is not very helpful. If we use a linear model for the probability, this means that we can predict probability values of less than 0 and more than 1, and this is not possible for probabilities. If we use the above values of $b_0=-3.8$ and $b_1=0.05$, we predict a probability of -.3 for a child of 70 months and a probability of 1.2 for a child of 100 months. Those values are meaningless!

\subsection{Odds and odds ratios}
Instead of predicting probabilities, we could predict \textit{odds}, or rather \textit{odds ratios}. The nice property of odds ratios is that they can have very large values, much larger than 1. 

What are odds again? Odds are a different way of talking about probability. Suppose the probability of winning the lottery is 1\%. Then the probability of loosing is $99\%$. This is equal to saying that the odds of winning against loosing are 1 to 99, or $1:99$, because the probability of success is 99 times smaller than the probability of loosing.

As another example, suppose the probability of being alive tomorrow is equal to 0.9999. Then the probability of not being alive tomorrow is $1-0.9999=0.0001$. Then the probability of being alive tomorrow is $0.9999/0.0001=9999$ times larger than the the probability of not being alive. Therefore the odds of being alive tomorrow against being dead is 9999 to 1 (9999:1).

If we have a slightly biased coin, the probability of heads might be 0.6. The probability of tails is then 0.4. So the probability of heads is then 1.5 times bigger than the probability of heads (0.6/0.4=1.5). So the odds of heads against tails is then 1.5 to 1. Odds are often multiplied by a constant to get nice integers, so we can also say the odds of heads aganst tails are 3 to 2. Similarly, if the probablity of heads were 0.61, the odds of heads against tails would be 0.61 to 0.39, which can be modified into 61 to 39.

Now that we know how to go from probability statements to statements about odds, how do we go from odds to probability? If someone says the odds of heads against tails is 10 to 1, this means that for every 10 heads, there will be 1 tails. In other words, if there were 11 coin tosses, 10 would be heads and 1 would be tails. We can therefore transform odds back to probabilities by noting that 10 out of 11 toin tosses is heads, so $10/11 = 0.91$, and 1 out of 11 is tails, so $1/11=0.09$. 


If someones says the odds of winning a gold medal at the Olympics is a thousand to one (1000:1), this means that if there were $1000+1=1001$ opportunities, there would be a gold medal in 1000 cases and failure in only one. This corresponds to a probability of 1000/1001 for winning and 1/1001 for failure. 

As a last example, if at the horse races, the odds of Bruno winning against Sacha are four to five (4:5), this means that for every 4 winnings by Bruno, there would be 5 winnings by Sacha. So out of a total of 9 winnings, 4 will be by Bruno and 5 will be by Sacha. The probability of Bruno outrunning Sacha is then $4/9=0.44$.
\\
\\
If we would summarize the odds by doing the division, we have an \textit{odds ratio}. Odds ratios have values that can be larger than 1. For instance, the odds 1:1 can be summarized as an odds ratio of 1, the odds 3:2 can be summarized as an odds ratio of 1.5, the odds 61:39 can be summarized as 1.564, and the odds 1000/1 can be summarized as an odds ratio of 1000.

However, note that odds ratios can never be negative: a very small odds is 1 to a 100000. This can be summarized into an odds ratio of 0.00001, but that is still larger than 0.

Mathematicians have therefore proposed to use the \textit{natural logarithm}\footnote{The natural logarithm of a number is its logarithm to the base of the constant $e$, where $e$ is approximately equal to 2.7. The natural logarithm of $x$ is generally written as 
$ln x$ or $log^e x$. The natural logarithm of $x$ is the power to which $e$ needs to be raised to equal $x$. For example, $ln(2)$ is 0.69, because $e^{0.69} = 2$, and $ln(0.2)=-1.6$ because $e^{-1.6}=0.2$. The natural logarithm of $e$ itself, $ln(e)$, is 1, because $e^1 = e$, while the natural logarithm of 1, $ln(1)$, is 0, since $e^0 = 1$.} of the odds ratio as the preferred transformation of probabilities. For example, suppose we have the probability of heads of 0.42. This can be transformed into an odds by noting that in 100 coin tosses, we would expect 42 times heads and 58 times tails. So the odds are 42:58. The odds ratio is then $\frac{42}{58}=\Sexpr{42/58}$. The \textit{natural} logarithm of \Sexpr{42/58} equals \Sexpr{log(42/58)} (use the $ln$ button on your calculator!). If we have a value between 0 and 1 and we take the logarithm of that value, we always get a value smaller than 0.

Figure \ref{fig:gen_8} shows the relationship between a probability (with values between 0 and 1) and the natural logarithm of the corresponding odds ratio.


<<gen_8, fig.height=4, echo=FALSE, fig.align='center', message=F, fig.cap='The relationship between a probability and the natural logarithm of the corresponding odds ratio.'>>=
probability <-  seq(0.001,0.999, 0.001)
logodds <- log( probability/(1-probability)  )
data <- data.frame(probability, logodds)
data %>% ggplot(aes(x=probability, y=logodds)) + geom_line() + ylab("natural logarithm of the odds ratio")
@

The result is a mirrored S-shaped curve on its side. For large probabilities close to one, the equivalent odds ratio becomes infinitely positive, and for very small probabilities close to zero, the equivalent odds ratio becomes infinitely negative. An odds ratio of 0 is equal to a probability of 0.5.
\\
\\
In summary, if we use a linear model to predict probabilities, we have the problem of predicted probabilities smaller than 0 and larger than 1 that are meaningless. If we use a linear model to predict odds ratios we have the problem of predicted odds ratios smaller than 0 that are meaningless. If on the other hand we use a linear model to predict \textit{the natural logarithm of odds ratios}, we have no problem whatsoever. We therefore propose to use a linear model to predict \textit{logoddsratios}.
\\
\\
Returning back to our example of the children passing the exam, suppose we have the following linear equation for the relationship between age and the logarithm of the odds of passing the exam


\begin{eqnarray}
logoddsratio=\Sexpr{round(outglm$coef[1],2)} + \Sexpr{round(outglm$coef[2],2)} age, \nonumber
\end{eqnarray}


This equation predicts that a child aged 70 months has a logoddsratio of $\Sexpr{round(outglm$coef[1],2)} + \Sexpr{round(outglm$coef[2],2)} \times 70 =\Sexpr{round(predict.glm(outglm, newdata=data.frame(age=70)),2)}$. In order to transform that logoddsratio back to a probability, we first have to take the exponential of the logoddsratio\footnote{If we know $ln(x)=60$, we have to infer that $x$ equals $e^{60}$, because $ln(e^{60})=60$ by definition of the natural logarithm, see previous footnote. Therefore, if we know that $ln(x)=c$, we know that $x$ equals $e^c$. The exponent of $c$, $e^c$, is often written as $exp(c)$. So if we know that the logarithm of the odds ratio equals $c$, $logoddsratio=ln(oddsratio)=c$, then the odds ratio is equal to $exp(c)$.} to get the odds ratio:


\begin{eqnarray}
oddsratio = exp(logoddsratio)= e^{logoddsratio}=e^{\Sexpr{round(predict.glm(outglm, newdata=data.frame(age=70)),2)}}=\Sexpr{round(exp(predict.glm(outglm, newdata=data.frame(age=70))),2)} \nonumber
\end{eqnarray}

An oddsratio of \Sexpr{round(exp(predict.glm(outglm, newdata=data.frame(age=70))),2)} means that the odds of passing the exam is \Sexpr{round(exp(predict.glm(outglm, newdata=data.frame(age=70))),2)} to 1 (\Sexpr{round(exp(predict.glm(outglm, newdata=data.frame(age=70))),2)}:1). So out of $1 + \Sexpr{round(exp(predict.glm(outglm, newdata=data.frame(age=70))),2)}= \Sexpr{round(1+exp(predict.glm(outglm, newdata=data.frame(age=70))),2)}$ times, we expect \Sexpr{round(exp(predict.glm(outglm, newdata=data.frame(age=70))),2)} successes and 1 failure. The probability of success is therefore $\frac{\Sexpr{round(exp(predict.glm(outglm, newdata=data.frame(age=70))),2)}}{1+\Sexpr{round(exp(predict.glm(outglm, newdata=data.frame(age=70))),2)}} = \Sexpr{round(exp(predict.glm(outglm, newdata=data.frame(age=70)))/ (1+exp(predict.glm(outglm, newdata=data.frame(age=70)))),2)}$. Thus, based on this equation, the expected probability of passing the exam for a child of 70 months equals 0.42.


\subsection{Exercises}

From probability to logoddsratios:

Given:
In the Netherlands, 51\% of the inhabitants is female.
\begin{enumerate}

\item
If we randomly pick someone from this Dutch population, what is the probability that that that person is female?


\item
If we randomly pick someone from this Dutch population, what are the odds that that that person is female?

\item
If we randomly pick someone from this Dutch population, what are the odds that that that person is male?

\item
What is the oddsratio of randomly picking an inhabitant that is female?

\item
What is the oddsratio of randomly picking an inhabitant that is male?


\item
What is the logoddsratio of randomly picking an inhabitant that is female?

\item
What is the logoddsratio of randomly picking an inhabitant that is male?


\end{enumerate}

Answers:

\begin{enumerate}

\item
0.51


\item
51 to 49 (51:49).

\item
49:51.

\item
51/49=1.04

\item
49/51=0.96


\item
ln(51/49)= ln(1.04)=0.04

\item
ln(49/51)= ln(0.96)=-0.04


\end{enumerate}

From logoddsratios to probabilities:

Given:
In the Netherlands, 51\% of the inhabitants are female. Females tend to get older than males, so if we predict sex by age, we should expect a higher probability of a female for older ages. Suppose we have the following linear model for the relationship between age (in years) and the logoddsratio of being female:


\begin{eqnarray}
logoddsratio_{female}=-0.01 + 0.01 \times age, \nonumber
\end{eqnarray}

\begin{enumerate}

\item
What is the predicted logoddsratio of being female for a person of age 20?

\item
What is the predicted logoddsratio of being female for a person of age 90?

\item
What is the predicted oddsratio of being female for a person of age 20?

\item
What is the predicted oddsratio of being female for a person of age 90?

\item
What are the predicted odds of being female for a person of age 20?

\item
What are the predicted odds of being female for a person of age 90?

\item
What is the predicted probability of being female for a person of age 20?

\item
What is the predicted probability of being female for a person of age 90?

\item
What is the predicted probability of being MALE for a person of age 90?


\end{enumerate}

Answers:

\begin{enumerate}

\item
$-0.01 + 0.01 \times 20 = 0.19$

\item
$-0.01 + 0.01 \times 90 = 0.89$

\item
$exp(0.19)=1.21$

\item
$exp(0.89)=2.44$

\item
1.21 to 1, or 1.21:1

\item
2.44 to 1, or 2.44:1

\item
1.21/ (1.21 + 1)= 0.55

\item
2.44 / (2.44 + 1)= 0.71

\item
1 - 0.71 = 0.29


\end{enumerate}


A big data analyst constructs a model that predicts whether an account on Twitter belongs to either a real person or organisation, or to a bot.

\begin{enumerate}

\item
For one account, a user of this model finds an logoddsratio of 4.5 that the account belongs to a bot. What is the corresponding probability that the twitter account belongs to a bot? Give the calculation.

\item
For a short tweet with only a hyperlink, the probability that it comes from a bot is only 10\%. What is the logoddsratio that corresponds to this probability? Give the calculation.


\end{enumerate}



Answers:
\begin{enumerate}

\item The logoddsratio is 4.5, so the oddsratio is exp(4.5)=90.0.
The odds ratio of being a bot is then 90:1.
The probability of being a bot is 90/ (90+1)= 0.99

\item
Out of 100 tweets with only a hyperlink, 10 are by bots and 90 are by real persons or organisations. So the odds of coming from a bot are 10:90. The odds ratio is therefore 10/90 = 0.11. When we take the natural logarithm of this odds ratio, we get the logoddsratio: ln(0.11) = -2.21.

\end{enumerate}



\subsection{Link functions}

In previous pages we have seen that logoddsratios have the nice property of having meaningful values between $-\infty$ and $+\infty$. This makes them suitable for linear models. In essence, our linear model for our exam data in children might then look like this:


\begin{eqnarray}
logoddsratio_{pass}= b_0 + b_1 age\\
y \sim Bern(p_{pass})
\end{eqnarray}

Note that we can write the odds ratio as $p/(1-p)$. So the logoddsratio that corresponds to the probability of passing the exam, $p_{pass}$, can be written as $ln\frac{p_{pass}}{1- p_{pass}}$, so that we have


\begin{eqnarray}
ln\frac{p_{pass}}{1- p_{pass}}= b_0 + b_1 age \\
y \sim Bern(p_{pass})
\end{eqnarray}

Note that we do not have a residual anymore: the randomness around the predicted values is no longer modelled using a residual $e$ that is normally distributed, but is now modelled by a $y$-variable with a Bernoulli distribution.
Also note the strange relationship between the probability parameter $p_{pass}$ for the Bernoulli distribition, and the dependent variable for the linear equation $b_0+b_1 age$. The linear model predicts the logoddsratio, but for the Bernoulli distribution, we use the probability. But it turns out that this model is very flexible and useful in many real-life problems. This model is often called a \textit{logit} model: one often writes that the \textit{logit of the probability} is predicted by a linear model.

\begin{eqnarray}
logit(p_{pass}) = b_0 + b_1 age \\
y \sim Bern(p_{pass})
\end{eqnarray}

In essence, the logit function transforms a $p$-value into a logoddsratio:

\begin{equation}
logit(p)= ln( \frac{p}{1-p} ) \nonumber
\end{equation}

So what does it look like, a linear model for logoddsratios (or logits of probabilities)?

In Figure \ref{fig:gen_9} we show a hypothetical example of a linear model for the logit of probabilities of passing an exam. These logits or logoddsratios are predicted by age using a straight, linear regression line:


<<gen_9, fig.height=4, echo=FALSE, fig.align='center', message=F,fig.cap='Example of a linear model for the logit of probabilities of passing an exam.'>>=
age <- seq(0,200, 1)
logoddsratio <- predict.glm(outglm, newdata=data.frame(age))
data <- data.frame(age, logoddsratio)
data %>% ggplot(aes(x=age, y=logoddsratio)) + geom_line() + ylab("logit(p)=logarithm of the odds ratio") + xlab("age in months") +
        geom_label(  aes(x=125, y=0.2), col='black', label='intercept: -3.8, slope=0.05', show.legend = F)
@

When we take all these predicted logoddsratios and convert them back to probabilities, we obtain the plot in Figure \ref{fig:gen_10}. Note the change in the scale of the vertical axis, the rest of the plot is the same.

<<gen_10, fig.height=4, echo=FALSE, fig.align='center', message=F,fig.cap='Example with logoddsratios transformed into probabilties (vertical axis).'>>=
probs <- exp(logoddsratio)/(1+exp(logoddsratio))
data <- data.frame(age, probs)
data %>% ggplot(aes(x=age, y=probs)) + geom_line() + ylab("probability of passing the exam") + xlab("age in months")  
@

Here again we see the S-shape relationship between probabilities and the logoddsratios. Here we see that our model predicts probabilities close to 0 for very young ages, and probabilities close to 1 for very old ages. There is a clear positive effect of age on the probability of passing the exam. But note that the relationship is not linear on the scale of the probabilities: it is linear on the scale of the logit of the probabilities see Figure \ref{gen_9}!

The curvilinear shape we see in Figure \ref{fig:gen_10} is called a \textit{logistic} curve. It is based on the logistic function: here $p$ is a logistic function of age:


\begin{equation}
p = logistic(b_0 + b_1 age) = \frac{exp(b_0 + b_1 age)}{1+exp(b_0+ b_1 age)} \nonumber
\end{equation}

If we go back to our data on the third-grade children that either passed or failed the exam, we see that this curve gives a description of our data, see Figure \ref{fig:gen_11}. The model predicts that around the age of 75 months, the probability of passing the exam is around 0.50. We indeed see that some children pass the exam (score=1) and some don't (score=0). On the basis of this analysis there seems to be a positive relationship between age in third-degree children and the probability of passing the exam, at least in this sample. 

<<gen_11, fig.height=4, echo=FALSE, fig.align='center', message=F, fig.cap='Transformed regression line and raw data points.'>>=
data %>% ggplot( aes(x=age, y=probs)) + geom_line() + ylab("probability of passing the exam") + xlab("age in months")  +
  geom_point(data=data.exam, aes(x=age, y=score)) +xlim(c(0, 200))
@

What we have done here is a \textit{logistic regression} of passing the exam on age. It is called logistic because the curve in Figure \ref{fig:gen_11} has a logistic shape. Logistic regression is one specific form of a \textit{generalized linear model}. Here we have applied a generalized linear model with a so-called \textit{logit link function}: instead of modelling dependent variable $y$ directly, we have modelled \textit{the logit of the probabilities of obtaining a $y$-value of 1}. There are many other link functions possible. One of them we will see in the section on generalized linear models for count data. But first, let's see how logistic regression can be performed in SPSS, and how we should interpret the output.

\section{Logistic regression in SPSS}

Imagine a data set on travellers from Amsterdam to Paris. From 1000 travellers, randomly sampled in 2017, we know whether they took the train to Paris, or whether they used other means of transportation. Of these travellers, we know their age, sex, yearly income, and whether they are travelling for business or not.

Part of the data are displayed in Table \ref{tab:gen_12}. A score of 1 on the variable \textbf{train} means they took the train, a score of 0 means they did not.



<<gen_12, fig.height=4, echo=FALSE, fig.align='center', message=F, results="asis", warning=F>>=
set.seed(123)
age <- runif(1000, 17, 80)
sex_male <- rbinom(1000, 1, 0.6)
income <- exp (rnorm(1000, log(30000), log(4))) %>% round(0) 
business <- rbinom(1000, 1, 0.6)
logodds = 65.5  - 0.006 *income 
probs = exp(logodds) /(1+exp(logodds))
train <- rbinom(1000, 1, probs)
data.train <- data.frame(train, age, sex_male, income, business )
data.train %>% 
        head() %>% 
        xtable(caption="Taking the train to Paris data.", label="tab:gen_12") %>%
        print(include.rownames=F, caption.placement = "top")
# glm(train~ income + sex_male +sex_male*income, data=data.train, family="binomial") %>% summary()
out.train <- glm(train~ income , data=data.train, family="binomial") 
source('/Users/stephanievandenberg/Dropbox/Statistiek_Onderwijs/Data Analysis/writeForeignCode.R')
write.foreign(data.train,
              '/Users/stephanievandenberg/Dropbox/Statistiek_Onderwijs/Data Analysis/spss examples logistic/train.sav',
              '/Users/stephanievandenberg/Dropbox/Statistiek_Onderwijs/Data Analysis/spss examples logistic/train.sps',
              package = c("SPSS"))
@


Suppose we want to know what kind of people are more likely to take the train to Paris. We can use a logistic regression analysis to predict whether people take the train or not, on the basis of their age, sex, income, and main purpose of the trip. 

Let's first see whether income predicts the probability of taking the train. The syntax for such a model is



\begin{verbatim}
GENLIN train (REFERENCE=FIRST) WITH income
  /MODEL income 
 DISTRIBUTION=BINOMIAL LINK=LOGIT
  /PRINT CPS DESCRIPTIVES   SOLUTION.
\end{verbatim}


Note the similary with the GLM and MIXED procedures: start with the dependent variable (\textbf{train} in this case) and then after the WITH word the variables that you'd like to treat quantitatively, here \textbf{income}. Under the MODEL subcommand we specify the model, here only a main effect of \textbf{income}. But further we have to specify that we want to use the Bernoulli distribution and a logit link function. So LINK=LOGIT, but why a binomial distribution? Well, a Bernoulli distribution (one coin flip) is only a special case of the Binomial distribution (the distribution of several coin flips). So here we use a binomial distribution for one coin flip, which is equivalent to a Bernoulli distribution. The last line indicates what type of output we want to see: case processing statistics, descriptives and the solution in terms of parameter estimates.

One very important part of the syntax is the (REFERENCE = FIRST) statement for the dependent variable. The default SPSS syntax uses (REFERENCE = LAST), so that's what you get when you do not specify this part. (REFERENCE = LAST) means that the reference category of the train variable is the last value. Since there are only two values, 0 an 1, the last value is equal to 1. In that case, SPSS will derive a model that predicts the logoddsratios for NOT taking the train, since it estimates the effect of income on the dependent variable \textit{relative to taking the train}. In our case, it makes more sense to derive a model for the logoddsratios of taking the train. We want to predict logodddsratios for taking the train, so we need to specify that our first value, 0, is our reference category: (REFERENCE = FIRST).


In Figure \ref{fig:train1} we see the parameter estimates from this generalized linear model run on the train data. 


\begin{figure}[h]
    \begin{center}
       \includegraphics[scale=0.7, trim={0cm 22cm 0cm 0cm}]{/Users/stephanievandenberg/Dropbox/Statistiek_Onderwijs/Data" "Analysis/spss" "examples" "logistic/train1.pdf}
    \end{center}
     \caption{SPSS output of a generalized linear model for predicting taking the train from income.}
    \label{fig:train1}
\end{figure}


The parameter estimates table looks very much like that of the ordinary linear model and the linear mixed model. The only difference is that we no longer see $t$-statistics, but Wald Chi-Square statistics. This is because with logistic models, the ratio $B/SE$ does not have a $t$-distribution. In ordinary linear models, the ratio $B/SE$ has a $t$-distribution because in linear models, the variance of the residuals, $\sigma^2_e$, has to be estimated. If the residual variance was known, $B/SE$ would have a standard normal distribution. In logistic models, there is no $\sigma^2_e$ that needs to be estimated, so the ratio $B/SE$ has a standard normal distribution\footnote{This is the reason why you see (scale) equal to constant 1 in the SPSS output, right under the parameter for \textbf{income}. In the logistic model, the variance (scale) is fixed (assumed known).}. One could therefore calculate a $Z$-statistic $Z=B/SE$ and see whether that value is smaller than 1.96 or larger than 1.96, if you want to test with a Type I error rate of 0.05. SPSS has chosen to not compute such a $Z$-statisic, but to compute a chi-square statistic $X^2= B^2/SE^2$. This chi-square or $X^2$-statistic has a $\chi^2$ distribution with 1 degree of freedom. Both approaches, computing $Z$ or $X^2$, are equivalent.
\\
\\
The interpretation of the $B$-parameters is very similar to other linear models. Note that we have the following equation for the logistic model:



\begin{eqnarray}
logit(p_{train}) = b_0 + b_1 income \nonumber \\
train \sim Bern(p_{train})
\end{eqnarray}

If we fill in the values from the SPSS output, we get


\begin{eqnarray}
logit(p_{train}) = 90.017 - 0.008 \times income \nonumber \\
train \sim Bern(p_{train})
\end{eqnarray}


We can interpret these results by making some predictions. Imagine a traveller with a yearly income of 11,000 Euros. Then the predicted logoddsratio equals $90.017 - 0.008 \times 11000= \Sexpr{90.017 - 0.008 * 11000}$. When we transform this back to a probability, we get $\frac{exp(\Sexpr{90.017 - 0.008 * 11000}) } {1+ exp(\Sexpr{90.017 - 0.008 * 11000}) }= \Sexpr{round(predict(out.train, newdata=data.frame(income=11000), type="response"), 3) }  $. So this model predicts that for people with a yearly income of 11,000, about 52\% of them take the train (if they travel at all, that is!).

Now imagine a traveller with a yearly income of 100,000. Then the predicted logoddsratio equals $6.752 - 0.001 \times 100000= \Sexpr{90.017 - 0.008 * 100000}$. When we transform this back to a probability, we get $\frac{exp(\Sexpr{90.017 - 0.008 * 100000}) } {1+ exp(\Sexpr{90.017 - 0.008 * 100000}) }= \Sexpr{round(predict(out.train, newdata=data.frame(income=100000), type="response"), 3)}$. So this model predicts that for people with a yearly income of 100,000, close to none of them take the train.
Going from 11,000 to 100,000 is a big difference. But the change in probabilities is also huge: it goes down from 0.52 to 0.

We found a difference in this sample of travellers, but is there also a difference in the entire population of travellers between Amsterdam and Paris? The SPSS table shows us that the effect of income, $- 0.008$, is statistically significant, $X^2(1)=7.541, p<0.01$. We can therefore reject the null-hypothesis that income is not related to whether people take the train or not.

Note that similar to other linear models, the intercept can be interpreted as the predicted logoddsratio for people that have values 0 for all other variables in the model. Therefore, 90.017 means in this case that the predicted logoddsratio for people with zero income equals 90.017. This is equivalent to a probability of very close to 1.



\subsection{Exercises}

Using the train data, we try to predict whether people take the train or not by their purpose of their trip: business or not.


\begin{enumerate}

\item

What does the SPSS syntax look like? Note the data in Table \ref{tab:gen_12}.


\item
Suppose the results look like those in Figure \ref{fig:train2}. What is the predicted probability of taking the train for people that travel for business? Provide the calculations.


\begin{figure}[h]
    \begin{center}
       \includegraphics[scale=0.7, trim={0cm 22cm 0cm 0cm}]{/Users/stephanievandenberg/Dropbox/Statistiek_Onderwijs/Data" "Analysis/spss" "examples" "logistic/train2.pdf}
    \end{center}
     \caption{SPSS output of a generalized linear model for predicting taking the train from purpose of the trip.}
    \label{fig:train2}
\end{figure}


\item Suppose the results look like those in Figure \ref{fig:train2}. What is the predicted probability of taking the train for people that travel NOT for business? Provide the calculations.



\item
Suppose the results look like those in Figure \ref{fig:train3}. What is the predicted probability of taking the train for people that travel for business? Provide the calculations.


\begin{figure}[h]
    \begin{center}
       \includegraphics[scale=0.7, trim={0cm 22cm 0cm 0cm}]{/Users/stephanievandenberg/Dropbox/Statistiek_Onderwijs/Data" "Analysis/spss" "examples" "logistic/train3.pdf}
    \end{center}
     \caption{SPSS output of a generalized linear model for predicting taking the train from purpose of the trip.}
    \label{fig:train3}
\end{figure}


\item Suppose the results look like those in Figure \ref{fig:train3}. What is the predicted probability of taking the train for people that travel NOT for business? Provide the calculations.


\item On the basis of this SPSS output, do business travellers tend to take the train more or less often than non-business travellers? Motivate your answer.


\item
Suppose in SPSS output for logistic regression, you find an intercept value of 0.5 with a standard error of 0.1. There is a corresponding Wald chi-square value of $\Sexpr{0.5^2/0.1/0.1}$. Explain where this Wald chi-square value comes from.

\item

Suppose we have the data on coin flips in following table:

<<nonparmixed_13, fig.height=4, echo=FALSE, fig.align='center'>>=
set.seed(1234)
ID <- seq(1:5)
Heads <- rbinom(5, 1, 0.5)
weight <- rchisq(5, df=3)
type <- c("5cents", "10cents", "1Euro", "10cents","1Euro")
data <- data.frame(ID, Heads, weight, type )
data %>% kable()
@

If we want to predict the outcome of the coin flip, on the basis of the type of coin, should we use a linear model, a linear mixed model, or a generalized linear model? Motivate your answer.
\\
\\
If we want to predict the weight of the coin, on the basis of the type of the coin, should we use a linear model, a linear mixed model, or a generalized linear model? Motivate your answer.


\end{enumerate}


Answers:
\begin{enumerate}


\item
It could look like this (using WITH, treating the independent variable as quantitative):

\begin{verbatim}
GENLIN train (REFERENCE=FIRST) WITH business
  /MODEL business
 DISTRIBUTION=BINOMIAL LINK=LOGIT
  /PRINT CPS DESCRIPTIVES   SOLUTION.
\end{verbatim}


or like this (using BY, treating the independent variable as qualitative)

\begin{verbatim}
GENLIN train (REFERENCE=FIRST) BY business
  /MODEL business
 DISTRIBUTION=BINOMIAL LINK=LOGIT
  /PRINT CPS DESCRIPTIVES   SOLUTION.
\end{verbatim}


\item
People that travel for business score 1 on the business variable. So the predicted logoddsratio for those people is $-1.155 - 0.050 \times 1 = -1.205$. The odds ratio is the $exp(-1.205)=\Sexpr{exp(-1.205)} $. So the odds of going by train are 0.30 to 1. This is equivalent to 3 to 10. So suppose we have 13 trips, 3 are by train and 10 are not by train. So the probability of a trip being by train equals $3/13=\Sexpr{round(3/13,2)}$.   

\item
People that travel NOT for business score 0 on the business variable. So the predicted logoddsratio for those people is $-1.155 - 0.050 \times 0 = -1.155$. The odds ratio is the $exp(-1.155)=\Sexpr{exp(-1.155)} $. So the odds of going by train are 0.32 to 1. This is equivalent to 32 to 100. So suppose we have 132 trips, 32 are by train and 100 are not by train. So the probability of a trip being by train equals $32/132=\Sexpr{round(32/132,2)}$.   

\item


\item

\item


If we want to predict the outcome of the coin flip, on the basis of the type of coin, we should use a generalized linear model, because the dependent variable is dichotomous (has only 2 values), so the residuals can never have a normal distribution. 
\\
\\
If we want to predict the weight of the coin, on the basis of the type of the coin, we should use a linear model, because the dependent variable is continuous.


\end{enumerate}





\section{Linear models for count data}


\section{Linear model as a special case of a generalized linear model}



\end{document}